---
title: 'Basic concepts in Probability and Statistics'
author: "Peter SÃ¸rensen"
date: "`r Sys.Date()`"
bibliography: lbgfs2021.bib
biblio-style: apalike
link-citations: yes
output:
  pdf_document:
    includes:
      in_header: preamble.tex
  html_document:
    includes:
      in_header: mathjax_header.html
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(19)
```

This compendium summarizes important probability probability concepts, formulas, and distributions widely used in statistical genetics. It is based on the following material: https://github.com/wzchen/probability_cheatsheet.

https://en.wikipedia.org/wiki/Algebra_of_random_variables
https://en.wikipedia.org/wiki/Random_variable

## Probability theory

### Independent Events: 
$A$ and $B$ are independent if knowing whether $A$ occurred gives no information about whether $B$ occurred. More formally, $A$ and $B$ (which have nonzero probability) are independent if and only if one of the following equivalent statements holds: 
\begin{align} 
  P({A}\cap { B}) &= P({A})P({B}) \\
  P({ A}|{ B}) &= P({A})\\
  P(B|A) &= P(B)
\end{align}
           
### Conditional Independence of Events: 
${A}$ and ${B}$ are conditionally independent given ${C}$ if $P({A}\cap {B}|{C}) = P({A}|{C})P({B}|{C})$. Conditional independence does not imply independence, and independence does not imply conditional independence.


### Joint, Marginal and Conditional Probabilitiy

* Joint Probability $P({A} \cap {B})$ or $P({A},{B})$ is the joint probability of ${A}$ and ${B}$.
* Marginal (Unconditional) Probability $P({A})$ is the marginal probability of ${A}$.
* Conditional Probability  $P({A}|{B}) = P(A,B)/P(B)$ is the conditional probability of ${A}$, given that ${B}$ occurred.
* Conditional Probability \emph{is} Probability]  $P({A}|{B})$ is a probability function for any fixed $B$. Any theorem that holds for probability also holds for conditional probability.


### Law of Total Probability (LOTP)
Let ${B}_1, {B}_2, { B}_3, ... { B}_n$ be a \emph{partition} of the sample space (i.e., they are disjoint and their union is the entire sample space).
\begin{align} 
P({ A}) &= P({ A} | { B}_1)P({ B}_1) + P({ A} | { B}_2)P({ B}_2) + \dots + P({ A} | { B}_n)P({ B}_n)\\
P({ A}) &= P({ A} \cap { B}_1)+ P({ A} \cap { B}_2)+ \dots + P({ A} \cap { B}_n)
\end{align} 
    
For \textbf{LOTP with extra conditioning}, just add in another event $C$!
\begin{align} 
P({ A}| { C}) &= P({ A} | { B}_1, { C})P({ B}_1 | { C}) + \dots +  P({ A} | { B}_n, { C})P({ B}_n | { C})\\
P({ A}| { C}) &= P({ A} \cap { B}_1 | { C})+ P({ A} \cap { B}_2 | { C})+ \dots +  P({ A} \cap { B}_n | { C})
\end{align} 

### Bayes' Rule
\begin{align} 
  P({A}|{B})  = \frac{P({ B}|{ A})P({ A})}{P({ B})}
\end{align} 

### Bayes' Rule with with extra conditioning
\begin{align} 
P({ A}|{ B}, {C}) = \frac{P({B}|{ A}, {C})P({ A} | { C})}{P({ B} | { C})}
\end{align} 

We can also write
\begin{align} 
P(A|B,C) = \frac{P(A,B,C)}{P(B,C)} = \frac{P(B,C|A)P(A)}{P(B,C)}
\end{align} 

### Odds Form of Bayes' Rule
\begin{align} 
\frac{P({ A}| { B})}{P({ A^c}| { B})} = \frac{P({ B}|{ A})}{P({ B}| { A^c})}\frac{P({ A})}{P({ A^c})}
\end{align} 
The \emph{posterior odds} of $A$ are the \emph{likelihood ratio} times the \emph{prior odds}. 


## Random Variables and their Distributions

### Probability Mass Function (PMF) 
Gives the probability that a \emph{discrete} random variable takes on the value $x$.
\begin{align} 
p_X(x) = P(X=x)
\end{align} 

The PMF satisfies
\begin{align} 
p_X(x) \geq 0 \textrm{ and } \sum_x p_X(x) = 1
\end{align} 


### Cumulative Distribution Function (CDF)
Gives the probability that a random variable is less than or equal to $x$.
\begin{align} 
F_X(x) = P(X \leq x)
\end{align} 

The CDF is an increasing, right-continuous function with
\begin{align} 
F_X(x) \to 0 \textrm{ as $x \to -\infty$ and } F_X(x) \to 1 \textrm{ as $x \to \infty$}
\end{align} 

* Independence: Intuitively, two random variables are independent if knowing the value of one gives  no information about the other. 
Discrete random variables $X$ and $Y$ are independent if for \emph{all} values of $x$ and $y$  
\begin{align} 
P(X=x, Y=y) = P(X = x)P(Y = y)
\end{align} 


### Expected Value and Linearity
The expected Value (a.k.a.~\emph{mean}, \emph{expectation}, or \emph{average}) is a weighted average of the possible outcomes of our random variable. Mathematically, if $x_1, x_2, x_3, \dots$ are all of the distinct possible values that $X$ can take, the expected value of $X$ is
\begin{align} 
E(X) = \sum\limits_{i}x_iP(X=x_i)
\end{align} 

* Linearity: For any random variables $X$ and $Y$, and constants $a,b,c,$ 
\begin{align} 
E(aX + bY + c) = aE(X) + bE(Y) + c
\end{align} 

* Same distribution implies same mean: If $X$ and $Y$ have the same distribution, then $E(X)=E(Y)$ and, more generally, 
\begin{align} 
E(g(X)) = E(g(Y))
\end{align} 

* Conditional Expected Value: is defined like expectation, only conditioned on any event $A$. 
\begin{align} 
\E(X | A) = \sum\limits_{x}xP(X=x|A)
\end{align} 


### Indicator Random Variable
An indicator random Variable is a random variable that takes on the value 1 or 0. It is always an indicator of some event: if the event occurs, the indicator is 1; otherwise it is 0. They are useful for many problems about counting how many events of some kind occur. Write \[
I_A =
 \begin{cases}
   1 & \text{if $A$ occurs,} \\
   0 & \text{if $A$ does not occur.}
  \end{cases}
\]
Note that $I_A^2 = I_A, I_A I_B = I_{A \cap B},$ and $I_{A \cup B} = I_A + I_B - I_A I_B$.

* Distribution $I_A \sim \Bern(p)$ where $p = P(A)$.
* Fundamental Bridge The expectation of the indicator for event $A$ is the probability of event $A$: $E(I_A)=P(A)$.


### Variance and Standard Deviation of a Random Variable
\[\Var(X) = E \left(X - E(X)\right)^2 = E(X^2) - (E(X))^2\]
\[\textrm{SD}(X) = \sqrt{\Var(X)}\]


### Continuous Random Variables
A continuous random variable can take on any possible value within a certain interval (for example, [0, 1]), whereas a discrete random variable can only take on variables in a list of countable values (for example, all the integers, or the values 1, $\frac{1}{2}, \frac{1}{4}, \frac{1}{8}$, etc.)
  * Do Continuous Random Variables have PMFs? No. The probability that a continuous random variable takes on any specific value is 0.
  * What's the probability that a CRV is in an interval? Take the difference in CDF values (or use the PDF as described later).
\[P(a \leq X \leq b) = P(X \leq b) - P(X \leq a) = F_X(b) - F_X(a)\]

For $X \sim \N(\mu,\sigma^2)$, this becomes
\begin{align}
P(a\leq X\leq b)&=\Phi \left(\frac{b-\mu }{\sigma } \right) - \Phi \left( \frac{a-\mu }{\sigma } \right)
\end{align}
  * What is the Probability Density Function (PDF)? The PDF $f$ is the derivative of the CDF $F$.
\[ F'(x) = f(x) \]
A PDF is nonnegative and integrates to $1$. By the fundamental theorem of calculus, to get from PDF back to CDF we can integrate:
\begin{align} 
F(x) &=  \int_{-\infty}^x f(t)dt  
\end{align}

To find the probability that a CRV takes on a value in an interval, integrate the PDF over that interval.
\begin{align} 
    F(b) - F(a)  &=  \int^b_a f(x)dx
\end{align}
   
Two additional properties of a PDF:  it must integrate to 1 (because the probability that a CRV falls in the interval $[-\infty, \infty]$ is 1, and the PDF must always be nonnegative.
  * How do I find the expected value of a CRV? Analogous to the discrete case, where you sum $x$ times the PMF, for CRVs you integrate $x$ times the PDF.
\[E(X) = \int^\infty_{-\infty}xf(x)dx \]

Expected value is \emph{linear}. This means that for \emph{any} random variables $X$ and $Y$ and any constants $a, b, c$, the following is true:
% \[E(aX + bY + c) = aE(X) + bE(Y) + c\]

### Expected value of a function of an random variable
The expected value of $X$ is defined this way:
\[E(X) = \sum_x xP(X=x) \textnormal{ (for discrete $X$)}\]
\[E(X) = \int^\infty_{-\infty}xf(x)dx  \textnormal{ (for continuous $X$)}\]

The \textbf{Law of the Unconscious Statistician (LOTUS)} states that you can find the expected value of a \emph{function of a random variable}, $g(X)$, in a similar way, by replacing the $x$ in front of the PMF/PDF by $g(x)$ but still working with the PMF/PDF of $X$:
\[E(g(X)) = \sum_x g(x)P(X=x) \textnormal{ (for discrete $X$)}\]
\[E(g(X)) = \int^\infty_{-\infty}g(x)f(x)dx \textnormal{ (for continuous $X$)}\]
  
  * What's a function of a random variable? A function of a random variable is also a random variable. For example, if $X$ is the number of bikes you see in an hour, then $g(X) =  2X$ is the number of bike wheels you see in that hour and $h(X) = {X \choose 2} = \frac{X(X-1)}{2}$ is the number of \emph{pairs} of bikes such that you see both of those bikes in that hour.
  
  * What's the point? You don't need to know the PMF/PDF of $g(X)$ to find its expected value. All you need is the PMF/PDF of $X$. 


### Joint Distributions
The \textbf{joint CDF} of $X$ and $Y$ is 
$$F(x,y)=P(X \leq x, Y \leq y)$$
In the discrete case, $X$ and $Y$ have a \textbf{joint PMF} 
$$p_{X,Y}(x,y) = P(X=x,Y=y).$$ In the continuous case, they have a \textbf{joint PDF}
\[f_{X,Y}(x,y) = \frac{\partial^2}{\partial x \partial y} F_{X,Y}(x,y).\]
The joint PMF/PDF must be nonnegative and sum/integrate to 1.

### Conditional Distributions
\textbf{Conditioning and Bayes' rule for discrete r.v.s}
\[P(Y=y|X=x) = \frac{P(X=x, Y=y)}{P(X=x)} = \frac{P(X=x|Y=y)P(Y=y)}{P(X=x)}\]
\textbf{Conditioning and Bayes' rule for continuous r.v.s}
\[f_{Y|X}(y|x) = \frac{f_{X,Y}(x, y)}{f_X(x)} = \frac{f_{X|Y}(x|y)f_Y(y)}{f_X(x)}\]
\textbf{Hybrid Bayes' rule}
\[f_X(x|A) = \frac{P(A | X = x)f_X(x)}{P(A)}\]

### Marginal Distributions
To find the distribution of one (or more) random variables from a joint PMF/PDF, sum/integrate over the unwanted random variables.

\textbf{Marginal PMF from joint PMF}
\[P(X = x) = \sum_y P(X=x, Y=y)\]
\textbf{Marginal PDF from joint PDF}
\[f_X(x) = \int_{-\infty}^\infty f_{X, Y}(x, y) dy\]


### Independence of Random Variables
Random variables $X$ and $Y$ are independent if and only if any of the following conditions holds:
  * Joint CDF is the product of the marginal CDFs
  * Joint PMF/PDF is the product of the marginal PMFs/PDFs
  * Conditional distribution of $Y$ given $X$ is the marginal distribution of $Y$
Write $X \independent Y$ to denote that $X$ and $Y$ are independent.

### Multivariate LOTUS
LOTUS in more than one dimension is analogous to the univariate LOTUS.
For discrete random variables:
\[E(g(X, Y)) = \sum_x\sum_yg(x, y)P(X=x, Y=y)\]
For continuous random variables:
\[E(g(X, Y)) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x, y)f_{X,Y}(x, y)dxdy\]


### Normal Distribution
Let us say that $X$ is distributed $\N(\mu, \sigma^2)$. We know the following:

* Central Limit Theorem: 
The Normal distribution is ubiquitous because of the Central Limit Theorem, which states that the sample mean of i.i.d.~r.v.s will approach a Normal distribution as the sample size grows, regardless of the initial distribution.

* Location-Scale Transformation:
Every time we shift a Normal r.v.~(by adding a constant) or rescale a Normal (by multiplying by a constant), we change it to another Normal r.v. For any Normal $X \sim \N(\mu, \sigma^2)$, we can transform it to the standard $\N(0, 1)$ by the following transformation:
    \[Z= \frac{X - \mu}{\sigma} \sim \N(0, 1) \]

* Standard Normal:
The Standard Normal, $Z \sim \N(0, 1)$, has mean $0$ and variance $1$. Its CDF is denoted by $\Phi$.


### Multivariate Normal (MVN) Distribution
A  vector $\vec{X} = (X_1, X_2, \dots, X_k)$ is  Multivariate Normal if every linear combination is Normally distributed, i.e., $t_1X_1 + t_2X_2 + \dots + t_kX_k$ is Normal for any constants $t_1, t_2, \dots, t_k$. The parameters of the Multivariate Normal are the \textbf{mean vector} $\vec{\mu} = (\mu_1, \mu_2, \dots, \mu_k)$ and the \textbf{covariance matrix} where the $(i, j)$ entry is $\Cov(X_i, X_j)$. 

The Multivariate Normal has the following properties.

* Any subvector is also MVN.
* If any two elements within an MVN are uncorrelated, then they are independent.
* The joint PDF of a Bivariate Normal $(X,Y)$ with $\N(0,1)$ marginal distributions and correlation $\rho \in (-1,1)$ is 
$$  f_{X,Y}(x,y) = \frac{1}{2 \pi \tau} \exp{\left(-\frac{1}{2 \tau^2} (x^2+y^2-2 \rho xy)\right)},$$
with $\tau = \sqrt{1-\rho^2}.$ 



## Covariance and Correlation
* Covariance is the analog of variance for two random variables.
    \[\Cov(X, Y) = E\left((X - E(X))(Y - E(Y))\right) = E(XY) - E(X)E(Y)\]
    Note that 
    \[\Cov(X, X) = E(X^2) - (E(X))^2 =  \Var(X)\]
    
* Correlation is a standardized version of covariance that is always between $-1$ and $1$.
    \[\Corr(X, Y) = \frac{\Cov(X, Y)}{\sqrt{\Var(X)\Var(Y)}} \]
    
* Covariance and Independence: 
If two random variables are independent, then they are uncorrelated. The converse is not necessarily true (e.g., consider $X \sim \N(0,1)$ and $Y=X^2$).
    \begin{align}
    	X \independent Y &\longrightarrow \Cov(X, Y) = 0 \longrightarrow E(XY) = E(X)E(Y)
    \end{align}
    
* Covariance and Variance:  
The variance of a sum can be found by
    \begin{align}
        \Var(X + Y) &= \Var(X) + \Var(Y) + 2\Cov(X, Y) \\
        \Var(X_1 + X_2 + \dots + X_n ) &= \sum_{i = 1}^{n}\Var(X_i) + 2\sum_{i < j} \Cov(X_i, X_j)
    \end{align}
    
    If $X$ and $Y$ are independent then they have covariance $0$, so
    \[X \independent Y \Longrightarrow \Var(X + Y) = \Var(X) + \Var(Y)\]
    If $X_1, X_2, \dots, X_n$ are identically distributed and have the same covariance relationships (often by \textbf{symmetry}), then 
    \[\Var(X_1 + X_2 + \dots + X_n ) = n\Var(X_1) + 2{n \choose 2}\Cov(X_1, X_2)\]

* Covariance Properties:  
For random variables $W, X, Y, Z$ and constants $a, b$:
    \begin{align*}
    	\Cov(X, Y) &= \Cov(Y, X) \\
        \Cov(X + a, Y + b) &= \Cov(X, Y) \\
        \Cov(aX, bY) &= ab\Cov(X, Y) \\
        \Cov(W + X, Y + Z) &= \Cov(W, Y) + \Cov(W, Z) + \Cov(X, Y) + \Cov(X, Z)
    \end{align*}
    
* Correlation:
is location-invariant and scale-invariant] For any constants $a,b,c,d$ with $a$ and $c$ nonzero,
    \begin{align*}
        \Corr(aX + b, cY + d) &= \Corr(X, Y) 
    \end{align*}



### Conditional Expectation

#### Conditioning on an Event. 
We can find $E(Y|A)$, the expected value of $Y$ given that event $A$ occurred. A very important case is when $A$ is the event $X=x$. Note that $E(Y|A)$ is a \emph{number}. For example:
  * The expected value of a fair die roll, given that it is prime, is $\frac{1}{3} \cdot 2 + \frac{1}{3} \cdot 3 + \frac{1}{3} \cdot 5 = \frac{10}{3}$.
  
  * Let $Y$ be the number of successes in $10$ independent Bernoulli trials with probability $p$ of success. Let $A$ be the event that the first $3$ trials are all successes. Then
    $$E(Y|A) = 3 + 7p$$
    since the number of successes among the last $7$ trials is $\Bin(7,p)$. 
  
  * Let $T \sim \Expo(1/10)$ be how long you have to wait until the shuttle comes. Given that you have already waited $t$ minutes, the expected additional waiting time is 10 more minutes, by the memoryless property. That is, $E(T|T>t) = t + 10$.
        \scalebox{0.85}{
            \begin{tabular}{ccc}
                  \textbf{Discrete $Y$} & \textbf{Continuous $Y$} \\
            \toprule
            $E(Y) = \sum_y yP(Y=y)$ & $E(Y) =\int_{-\infty}^\infty yf_Y(y)dy$ \\
           % $E(Y|X=x) = \sum_y yP(Y=y|X=x)$ & $E(Y|X=x) =\int_{-\infty}^\infty yf_{Y|X}(y|x)dy$ \\
            $E(Y|A) = \sum_y yP(Y=y|A)$ & $E(Y|A) = \int_{-\infty}^\infty yf(y|A)dy$ \\ 
            \bottomrule
            \end{tabular}
        }

#### Conditioning on a Random Variable:
We can also find $E(Y|X)$, the expected value of $Y$ given the random variable $X$. This is \emph{a function of the random variable $X$}. It is \emph{not} a number except in certain special cases such as if $X \independent Y$. To find $E(Y|X)$, find $E(Y|X = x)$ and then plug in $X$ for $x$. For example:
* If $E(Y|X=x) = x^3+5x$, then $E(Y|X) = X^3 + 5X$.
* Let $Y$ be the number of successes in $10$ independent Bernoulli trials with probability $p$ of success and $X$ be the number of successes among the first $3$ trials. Then $E(Y|X)=X+7p$.
* Let $X \sim \N(0,1)$ and $Y=X^2$. Then $E(Y|X=x) = x^2$ since if we know $X=x$ then we know $Y=x^2$. And $E(X|Y=y) = 0$ since if we know $Y=y$ then we know $X = \pm \sqrt{y}$, with equal probabilities (by symmetry). So $E(Y|X)=X^2, E(X|Y)=0$.  

#### Properties of Conditional Expectation:
* $E(Y|X) = E(Y)$ if $X \independent Y$
* $E(h(X)W|X) = h(X)E(W|X)$ (\textbf{taking out what's known}) \\
In particular, $E(h(X)|X) = h(X)$.
*$E(E(Y|X)) = E(Y)$ (\textbf{Adam's Law}, a.k.a.~Law of Total Expectation)

#### Adam's Law (a.k.a.~Law of Total Expectation):  
The aaw of total expectation can also be written in a way that looks analogous to LOTP. For any events $A_1, A_2, \dots, A_n$ that partition the sample space, 
\begin{align}
  E(Y) &= E(Y|A_1)P(A_1) + \dots + E(Y|A_n)P(A_n)
\end{align}
For the special case where the partition is $A, A^c$, this says
\begin{align}
  E(Y) &= E(Y|A)P(A) + E(Y|A^c)P(A^c)
\end{align}

#### Eve's Law (a.k.a.~Law of Total Variance)]
\[\Var(Y) = E(\Var(Y|X)) + \Var(E(Y|X))\]


### Law of Large Numbers (LLN)
Let $X_1, X_2, X_3 \dots$ be i.i.d.~with mean $\mu$. The \textbf{sample mean} is $$\bar{X}_n = \frac{X_1 + X_2 + X_3 + \dots + X_n}{n}$$ The \textbf{Law of Large Numbers} states that as $n \to \infty$, $\bar{X}_n \to \mu$ with probability $1$. For example, in flips of a coin with probability $p$ of Heads, let $X_j$ be the indicator of the $j$th flip being Heads.  Then LLN says the proportion of Heads converges to $p$ (with probability $1$).

### Central Limit Theorem (CLT)

#### Approximation using CLT
We use $\dot{\,\sim\,}$ to denote \emph{is approximately distributed}. We can use the \textbf{Central Limit Theorem} to approximate the distribution of a random variable $Y=X_1+X_2+\dots+X_n$ that is a sum of $n$ i.i.d. random variables $X_i$. Let  $E(Y) = \mu_Y$ and $\Var(Y) = \sigma^2_Y$. The CLT says
\[Y \dot{\,\sim\,} \N(\mu_Y, \sigma^2_Y)\]
If the $X_i$ are i.i.d.~with mean $\mu_X$ and variance $\sigma^2_X$, then $\mu_Y = n \mu_X$ and $\sigma^2_Y = n \sigma^2_X$. For the sample mean $\bar{X}_n$, the CLT says
\[ \bar{X}_n = \frac{1}{n}(X_1 + X_2 + \dots + X_n) \dot{\,\sim\,} \N(\mu_X, \sigma^2_X/n) \]


#### Asymptotic Distributions using CLT

We use $\xrightarrow{D}$ to denote \emph{converges in distribution to} as $n  \to \infty$. The CLT says that if we standardize the sum $X_1 + \dots + X_n$  then the distribution of the sum converges to $\N(0,1)$ as $n \to \infty$:
\[\frac{1}{\sigma\sqrt{n}} (X_1 + \dots + X_n - n\mu_X) \xrightarrow{D} \N(0, 1)\]
In other words, the CDF of the left-hand side goes to the standard Normal CDF, $\Phi$. In terms of the sample mean, the CLT says
\[ \frac{\sqrt{n} (\bar{X}_n - \mu_X)}{\sigma_X} \xrightarrow{D} \N(0, 1)\]
