--- 
title: "A Minimal Book Example"
author: "Yihui Xie"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook."
---

# Prerequisites

This is a _sample_ book written in **Markdown**. You can use anything that Pandoc's Markdown supports, e.g., a math equation $a^2 + b^2 = c^2$.

The **bookdown** package can be installed from CRAN or Github:

```{r eval=FALSE}
install.packages("bookdown")
# or the development version
# devtools::install_github("rstudio/bookdown")
```

Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading `#`.

To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): <https://yihui.org/tinytex/>.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

---
title: 'Basic concepts in Probability and Statistics'
author: "Peter Sørensen"
date: "`r Sys.Date()`"
bibliography: lbgfs2021.bib
biblio-style: apalike
link-citations: yes
output:
  pdf_document:
    includes:
      in_header: preamble.tex
  html_document:
    includes:
      in_header: mathjax_header.html
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(19)
```

This compendium summarizes important probability probability concepts, formulas, and distributions widely used in statistical genetics. It is based on the following material: https://github.com/wzchen/probability_cheatsheet.

https://en.wikipedia.org/wiki/Algebra_of_random_variables
https://en.wikipedia.org/wiki/Random_variable

## Probability theory

### Independent Events: 
$A$ and $B$ are independent if knowing whether $A$ occurred gives no information about whether $B$ occurred. More formally, $A$ and $B$ (which have nonzero probability) are independent if and only if one of the following equivalent statements holds: 
\begin{align} 
  P({A}\cap { B}) &= P({A})P({B}) \\
  P({ A}|{ B}) &= P({A})\\
  P(B|A) &= P(B)
\end{align}
           
### Conditional Independence of Events: 
${A}$ and ${B}$ are conditionally independent given ${C}$ if $P({A}\cap {B}|{C}) = P({A}|{C})P({B}|{C})$. Conditional independence does not imply independence, and independence does not imply conditional independence.


### Joint, Marginal and Conditional Probabilitiy

* Joint Probability $P({A} \cap {B})$ or $P({A},{B})$ is the joint probability of ${A}$ and ${B}$.
* Marginal (Unconditional) Probability $P({A})$ is the marginal probability of ${A}$.
* Conditional Probability  $P({A}|{B}) = P(A,B)/P(B)$ is the conditional probability of ${A}$, given that ${B}$ occurred.
* Conditional Probability \emph{is} Probability]  $P({A}|{B})$ is a probability function for any fixed $B$. Any theorem that holds for probability also holds for conditional probability.


### Law of Total Probability (LOTP)
Let ${B}_1, {B}_2, { B}_3, ... { B}_n$ be a \emph{partition} of the sample space (i.e., they are disjoint and their union is the entire sample space).
\begin{align} 
P({ A}) &= P({ A} | { B}_1)P({ B}_1) + P({ A} | { B}_2)P({ B}_2) + \dots + P({ A} | { B}_n)P({ B}_n)\\
P({ A}) &= P({ A} \cap { B}_1)+ P({ A} \cap { B}_2)+ \dots + P({ A} \cap { B}_n)
\end{align} 
    
For \textbf{LOTP with extra conditioning}, just add in another event $C$!
\begin{align} 
P({ A}| { C}) &= P({ A} | { B}_1, { C})P({ B}_1 | { C}) + \dots +  P({ A} | { B}_n, { C})P({ B}_n | { C})\\
P({ A}| { C}) &= P({ A} \cap { B}_1 | { C})+ P({ A} \cap { B}_2 | { C})+ \dots +  P({ A} \cap { B}_n | { C})
\end{align} 

### Bayes' Rule
\begin{align} 
  P({A}|{B})  = \frac{P({ B}|{ A})P({ A})}{P({ B})}
\end{align} 

### Bayes' Rule with with extra conditioning
\begin{align} 
P({ A}|{ B}, {C}) = \frac{P({B}|{ A}, {C})P({ A} | { C})}{P({ B} | { C})}
\end{align} 

We can also write
\begin{align} 
P(A|B,C) = \frac{P(A,B,C)}{P(B,C)} = \frac{P(B,C|A)P(A)}{P(B,C)}
\end{align} 

### Odds Form of Bayes' Rule
\begin{align} 
\frac{P({ A}| { B})}{P({ A^c}| { B})} = \frac{P({ B}|{ A})}{P({ B}| { A^c})}\frac{P({ A})}{P({ A^c})}
\end{align} 
The \emph{posterior odds} of $A$ are the \emph{likelihood ratio} times the \emph{prior odds}. 


## Random Variables and their Distributions

### Probability Mass Function (PMF) 
Gives the probability that a \emph{discrete} random variable takes on the value $x$.
\begin{align} 
p_X(x) = P(X=x)
\end{align} 

The PMF satisfies
\begin{align} 
p_X(x) \geq 0 \textrm{ and } \sum_x p_X(x) = 1
\end{align} 


### Cumulative Distribution Function (CDF)
Gives the probability that a random variable is less than or equal to $x$.
\begin{align} 
F_X(x) = P(X \leq x)
\end{align} 

The CDF is an increasing, right-continuous function with
\begin{align} 
F_X(x) \to 0 \textrm{ as $x \to -\infty$ and } F_X(x) \to 1 \textrm{ as $x \to \infty$}
\end{align} 

* Independence: Intuitively, two random variables are independent if knowing the value of one gives  no information about the other. 
Discrete random variables $X$ and $Y$ are independent if for \emph{all} values of $x$ and $y$  
\begin{align} 
P(X=x, Y=y) = P(X = x)P(Y = y)
\end{align} 


### Expected Value and Linearity
The expected Value (a.k.a.~\emph{mean}, \emph{expectation}, or \emph{average}) is a weighted average of the possible outcomes of our random variable. Mathematically, if $x_1, x_2, x_3, \dots$ are all of the distinct possible values that $X$ can take, the expected value of $X$ is
\begin{align} 
E(X) = \sum\limits_{i}x_iP(X=x_i)
\end{align} 

* Linearity: For any random variables $X$ and $Y$, and constants $a,b,c,$ 
\begin{align} 
E(aX + bY + c) = aE(X) + bE(Y) + c
\end{align} 

* Same distribution implies same mean: If $X$ and $Y$ have the same distribution, then $E(X)=E(Y)$ and, more generally, 
\begin{align} 
E(g(X)) = E(g(Y))
\end{align} 

* Conditional Expected Value: is defined like expectation, only conditioned on any event $A$. 
\begin{align} 
\E(X | A) = \sum\limits_{x}xP(X=x|A)
\end{align} 


### Indicator Random Variable
An indicator random Variable is a random variable that takes on the value 1 or 0. It is always an indicator of some event: if the event occurs, the indicator is 1; otherwise it is 0. They are useful for many problems about counting how many events of some kind occur. Write \[
I_A =
 \begin{cases}
   1 & \text{if $A$ occurs,} \\
   0 & \text{if $A$ does not occur.}
  \end{cases}
\]
Note that $I_A^2 = I_A, I_A I_B = I_{A \cap B},$ and $I_{A \cup B} = I_A + I_B - I_A I_B$.

* Distribution $I_A \sim \Bern(p)$ where $p = P(A)$.
* Fundamental Bridge The expectation of the indicator for event $A$ is the probability of event $A$: $E(I_A)=P(A)$.


### Variance and Standard Deviation of a Random Variable
\[\Var(X) = E \left(X - E(X)\right)^2 = E(X^2) - (E(X))^2\]
\[\textrm{SD}(X) = \sqrt{\Var(X)}\]


### Continuous Random Variables
A continuous random variable can take on any possible value within a certain interval (for example, [0, 1]), whereas a discrete random variable can only take on variables in a list of countable values (for example, all the integers, or the values 1, $\frac{1}{2}, \frac{1}{4}, \frac{1}{8}$, etc.)
  * Do Continuous Random Variables have PMFs? No. The probability that a continuous random variable takes on any specific value is 0.
  * What's the probability that a CRV is in an interval? Take the difference in CDF values (or use the PDF as described later).
\[P(a \leq X \leq b) = P(X \leq b) - P(X \leq a) = F_X(b) - F_X(a)\]

For $X \sim \N(\mu,\sigma^2)$, this becomes
\begin{align}
P(a\leq X\leq b)&=\Phi \left(\frac{b-\mu }{\sigma } \right) - \Phi \left( \frac{a-\mu }{\sigma } \right)
\end{align}
  * What is the Probability Density Function (PDF)? The PDF $f$ is the derivative of the CDF $F$.
\[ F'(x) = f(x) \]
A PDF is nonnegative and integrates to $1$. By the fundamental theorem of calculus, to get from PDF back to CDF we can integrate:
\begin{align} 
F(x) &=  \int_{-\infty}^x f(t)dt  
\end{align}

To find the probability that a CRV takes on a value in an interval, integrate the PDF over that interval.
\begin{align} 
    F(b) - F(a)  &=  \int^b_a f(x)dx
\end{align}
   
Two additional properties of a PDF:  it must integrate to 1 (because the probability that a CRV falls in the interval $[-\infty, \infty]$ is 1, and the PDF must always be nonnegative.
  * How do I find the expected value of a CRV? Analogous to the discrete case, where you sum $x$ times the PMF, for CRVs you integrate $x$ times the PDF.
\[E(X) = \int^\infty_{-\infty}xf(x)dx \]

Expected value is \emph{linear}. This means that for \emph{any} random variables $X$ and $Y$ and any constants $a, b, c$, the following is true:
% \[E(aX + bY + c) = aE(X) + bE(Y) + c\]

### Expected value of a function of an random variable
The expected value of $X$ is defined this way:
\[E(X) = \sum_x xP(X=x) \textnormal{ (for discrete $X$)}\]
\[E(X) = \int^\infty_{-\infty}xf(x)dx  \textnormal{ (for continuous $X$)}\]

The \textbf{Law of the Unconscious Statistician (LOTUS)} states that you can find the expected value of a \emph{function of a random variable}, $g(X)$, in a similar way, by replacing the $x$ in front of the PMF/PDF by $g(x)$ but still working with the PMF/PDF of $X$:
\[E(g(X)) = \sum_x g(x)P(X=x) \textnormal{ (for discrete $X$)}\]
\[E(g(X)) = \int^\infty_{-\infty}g(x)f(x)dx \textnormal{ (for continuous $X$)}\]
  
  * What's a function of a random variable? A function of a random variable is also a random variable. For example, if $X$ is the number of bikes you see in an hour, then $g(X) =  2X$ is the number of bike wheels you see in that hour and $h(X) = {X \choose 2} = \frac{X(X-1)}{2}$ is the number of \emph{pairs} of bikes such that you see both of those bikes in that hour.
  
  * What's the point? You don't need to know the PMF/PDF of $g(X)$ to find its expected value. All you need is the PMF/PDF of $X$. 


### Joint Distributions
The \textbf{joint CDF} of $X$ and $Y$ is 
$$F(x,y)=P(X \leq x, Y \leq y)$$
In the discrete case, $X$ and $Y$ have a \textbf{joint PMF} 
$$p_{X,Y}(x,y) = P(X=x,Y=y).$$ In the continuous case, they have a \textbf{joint PDF}
\[f_{X,Y}(x,y) = \frac{\partial^2}{\partial x \partial y} F_{X,Y}(x,y).\]
The joint PMF/PDF must be nonnegative and sum/integrate to 1.

### Conditional Distributions
\textbf{Conditioning and Bayes' rule for discrete r.v.s}
\[P(Y=y|X=x) = \frac{P(X=x, Y=y)}{P(X=x)} = \frac{P(X=x|Y=y)P(Y=y)}{P(X=x)}\]
\textbf{Conditioning and Bayes' rule for continuous r.v.s}
\[f_{Y|X}(y|x) = \frac{f_{X,Y}(x, y)}{f_X(x)} = \frac{f_{X|Y}(x|y)f_Y(y)}{f_X(x)}\]
\textbf{Hybrid Bayes' rule}
\[f_X(x|A) = \frac{P(A | X = x)f_X(x)}{P(A)}\]

### Marginal Distributions
To find the distribution of one (or more) random variables from a joint PMF/PDF, sum/integrate over the unwanted random variables.

\textbf{Marginal PMF from joint PMF}
\[P(X = x) = \sum_y P(X=x, Y=y)\]
\textbf{Marginal PDF from joint PDF}
\[f_X(x) = \int_{-\infty}^\infty f_{X, Y}(x, y) dy\]


### Independence of Random Variables
Random variables $X$ and $Y$ are independent if and only if any of the following conditions holds:
  * Joint CDF is the product of the marginal CDFs
  * Joint PMF/PDF is the product of the marginal PMFs/PDFs
  * Conditional distribution of $Y$ given $X$ is the marginal distribution of $Y$
Write $X \independent Y$ to denote that $X$ and $Y$ are independent.

### Multivariate LOTUS
LOTUS in more than one dimension is analogous to the univariate LOTUS.
For discrete random variables:
\[E(g(X, Y)) = \sum_x\sum_yg(x, y)P(X=x, Y=y)\]
For continuous random variables:
\[E(g(X, Y)) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x, y)f_{X,Y}(x, y)dxdy\]


### Normal Distribution
Let us say that $X$ is distributed $\N(\mu, \sigma^2)$. We know the following:

* Central Limit Theorem: 
The Normal distribution is ubiquitous because of the Central Limit Theorem, which states that the sample mean of i.i.d.~r.v.s will approach a Normal distribution as the sample size grows, regardless of the initial distribution.

* Location-Scale Transformation:
Every time we shift a Normal r.v.~(by adding a constant) or rescale a Normal (by multiplying by a constant), we change it to another Normal r.v. For any Normal $X \sim \N(\mu, \sigma^2)$, we can transform it to the standard $\N(0, 1)$ by the following transformation:
    \[Z= \frac{X - \mu}{\sigma} \sim \N(0, 1) \]

* Standard Normal:
The Standard Normal, $Z \sim \N(0, 1)$, has mean $0$ and variance $1$. Its CDF is denoted by $\Phi$.


### Multivariate Normal (MVN) Distribution
A  vector $\vec{X} = (X_1, X_2, \dots, X_k)$ is  Multivariate Normal if every linear combination is Normally distributed, i.e., $t_1X_1 + t_2X_2 + \dots + t_kX_k$ is Normal for any constants $t_1, t_2, \dots, t_k$. The parameters of the Multivariate Normal are the \textbf{mean vector} $\vec{\mu} = (\mu_1, \mu_2, \dots, \mu_k)$ and the \textbf{covariance matrix} where the $(i, j)$ entry is $\Cov(X_i, X_j)$. 

The Multivariate Normal has the following properties.

* Any subvector is also MVN.
* If any two elements within an MVN are uncorrelated, then they are independent.
* The joint PDF of a Bivariate Normal $(X,Y)$ with $\N(0,1)$ marginal distributions and correlation $\rho \in (-1,1)$ is 
$$  f_{X,Y}(x,y) = \frac{1}{2 \pi \tau} \exp{\left(-\frac{1}{2 \tau^2} (x^2+y^2-2 \rho xy)\right)},$$
with $\tau = \sqrt{1-\rho^2}.$ 



## Covariance and Correlation
* Covariance is the analog of variance for two random variables.
    \[\Cov(X, Y) = E\left((X - E(X))(Y - E(Y))\right) = E(XY) - E(X)E(Y)\]
    Note that 
    \[\Cov(X, X) = E(X^2) - (E(X))^2 =  \Var(X)\]
    
* Correlation is a standardized version of covariance that is always between $-1$ and $1$.
    \[\Corr(X, Y) = \frac{\Cov(X, Y)}{\sqrt{\Var(X)\Var(Y)}} \]
    
* Covariance and Independence: 
If two random variables are independent, then they are uncorrelated. The converse is not necessarily true (e.g., consider $X \sim \N(0,1)$ and $Y=X^2$).
    \begin{align}
    	X \independent Y &\longrightarrow \Cov(X, Y) = 0 \longrightarrow E(XY) = E(X)E(Y)
    \end{align}
    
* Covariance and Variance:  
The variance of a sum can be found by
    \begin{align}
        \Var(X + Y) &= \Var(X) + \Var(Y) + 2\Cov(X, Y) \\
        \Var(X_1 + X_2 + \dots + X_n ) &= \sum_{i = 1}^{n}\Var(X_i) + 2\sum_{i < j} \Cov(X_i, X_j)
    \end{align}
    
    If $X$ and $Y$ are independent then they have covariance $0$, so
    \[X \independent Y \Longrightarrow \Var(X + Y) = \Var(X) + \Var(Y)\]
    If $X_1, X_2, \dots, X_n$ are identically distributed and have the same covariance relationships (often by \textbf{symmetry}), then 
    \[\Var(X_1 + X_2 + \dots + X_n ) = n\Var(X_1) + 2{n \choose 2}\Cov(X_1, X_2)\]

* Covariance Properties:  
For random variables $W, X, Y, Z$ and constants $a, b$:
    \begin{align*}
    	\Cov(X, Y) &= \Cov(Y, X) \\
        \Cov(X + a, Y + b) &= \Cov(X, Y) \\
        \Cov(aX, bY) &= ab\Cov(X, Y) \\
        \Cov(W + X, Y + Z) &= \Cov(W, Y) + \Cov(W, Z) + \Cov(X, Y) + \Cov(X, Z)
    \end{align*}
    
* Correlation:
is location-invariant and scale-invariant] For any constants $a,b,c,d$ with $a$ and $c$ nonzero,
    \begin{align*}
        \Corr(aX + b, cY + d) &= \Corr(X, Y) 
    \end{align*}



### Conditional Expectation

#### Conditioning on an Event. 
We can find $E(Y|A)$, the expected value of $Y$ given that event $A$ occurred. A very important case is when $A$ is the event $X=x$. Note that $E(Y|A)$ is a \emph{number}. For example:
  * The expected value of a fair die roll, given that it is prime, is $\frac{1}{3} \cdot 2 + \frac{1}{3} \cdot 3 + \frac{1}{3} \cdot 5 = \frac{10}{3}$.
  
  * Let $Y$ be the number of successes in $10$ independent Bernoulli trials with probability $p$ of success. Let $A$ be the event that the first $3$ trials are all successes. Then
    $$E(Y|A) = 3 + 7p$$
    since the number of successes among the last $7$ trials is $\Bin(7,p)$. 
  
  * Let $T \sim \Expo(1/10)$ be how long you have to wait until the shuttle comes. Given that you have already waited $t$ minutes, the expected additional waiting time is 10 more minutes, by the memoryless property. That is, $E(T|T>t) = t + 10$.
        \scalebox{0.85}{
            \begin{tabular}{ccc}
                  \textbf{Discrete $Y$} & \textbf{Continuous $Y$} \\
            \toprule
            $E(Y) = \sum_y yP(Y=y)$ & $E(Y) =\int_{-\infty}^\infty yf_Y(y)dy$ \\
           % $E(Y|X=x) = \sum_y yP(Y=y|X=x)$ & $E(Y|X=x) =\int_{-\infty}^\infty yf_{Y|X}(y|x)dy$ \\
            $E(Y|A) = \sum_y yP(Y=y|A)$ & $E(Y|A) = \int_{-\infty}^\infty yf(y|A)dy$ \\ 
            \bottomrule
            \end{tabular}
        }

#### Conditioning on a Random Variable:
We can also find $E(Y|X)$, the expected value of $Y$ given the random variable $X$. This is \emph{a function of the random variable $X$}. It is \emph{not} a number except in certain special cases such as if $X \independent Y$. To find $E(Y|X)$, find $E(Y|X = x)$ and then plug in $X$ for $x$. For example:
* If $E(Y|X=x) = x^3+5x$, then $E(Y|X) = X^3 + 5X$.
* Let $Y$ be the number of successes in $10$ independent Bernoulli trials with probability $p$ of success and $X$ be the number of successes among the first $3$ trials. Then $E(Y|X)=X+7p$.
* Let $X \sim \N(0,1)$ and $Y=X^2$. Then $E(Y|X=x) = x^2$ since if we know $X=x$ then we know $Y=x^2$. And $E(X|Y=y) = 0$ since if we know $Y=y$ then we know $X = \pm \sqrt{y}$, with equal probabilities (by symmetry). So $E(Y|X)=X^2, E(X|Y)=0$.  

#### Properties of Conditional Expectation:
* $E(Y|X) = E(Y)$ if $X \independent Y$
* $E(h(X)W|X) = h(X)E(W|X)$ (\textbf{taking out what's known}) \\
In particular, $E(h(X)|X) = h(X)$.
*$E(E(Y|X)) = E(Y)$ (\textbf{Adam's Law}, a.k.a.~Law of Total Expectation)

#### Adam's Law (a.k.a.~Law of Total Expectation):  
The aaw of total expectation can also be written in a way that looks analogous to LOTP. For any events $A_1, A_2, \dots, A_n$ that partition the sample space, 
\begin{align}
  E(Y) &= E(Y|A_1)P(A_1) + \dots + E(Y|A_n)P(A_n)
\end{align}
For the special case where the partition is $A, A^c$, this says
\begin{align}
  E(Y) &= E(Y|A)P(A) + E(Y|A^c)P(A^c)
\end{align}

#### Eve's Law (a.k.a.~Law of Total Variance)]
\[\Var(Y) = E(\Var(Y|X)) + \Var(E(Y|X))\]


### Law of Large Numbers (LLN)
Let $X_1, X_2, X_3 \dots$ be i.i.d.~with mean $\mu$. The \textbf{sample mean} is $$\bar{X}_n = \frac{X_1 + X_2 + X_3 + \dots + X_n}{n}$$ The \textbf{Law of Large Numbers} states that as $n \to \infty$, $\bar{X}_n \to \mu$ with probability $1$. For example, in flips of a coin with probability $p$ of Heads, let $X_j$ be the indicator of the $j$th flip being Heads.  Then LLN says the proportion of Heads converges to $p$ (with probability $1$).

### Central Limit Theorem (CLT)

#### Approximation using CLT
We use $\dot{\,\sim\,}$ to denote \emph{is approximately distributed}. We can use the \textbf{Central Limit Theorem} to approximate the distribution of a random variable $Y=X_1+X_2+\dots+X_n$ that is a sum of $n$ i.i.d. random variables $X_i$. Let  $E(Y) = \mu_Y$ and $\Var(Y) = \sigma^2_Y$. The CLT says
\[Y \dot{\,\sim\,} \N(\mu_Y, \sigma^2_Y)\]
If the $X_i$ are i.i.d.~with mean $\mu_X$ and variance $\sigma^2_X$, then $\mu_Y = n \mu_X$ and $\sigma^2_Y = n \sigma^2_X$. For the sample mean $\bar{X}_n$, the CLT says
\[ \bar{X}_n = \frac{1}{n}(X_1 + X_2 + \dots + X_n) \dot{\,\sim\,} \N(\mu_X, \sigma^2_X/n) \]


#### Asymptotic Distributions using CLT

We use $\xrightarrow{D}$ to denote \emph{converges in distribution to} as $n  \to \infty$. The CLT says that if we standardize the sum $X_1 + \dots + X_n$  then the distribution of the sum converges to $\N(0,1)$ as $n \to \infty$:
\[\frac{1}{\sigma\sqrt{n}} (X_1 + \dots + X_n - n\mu_X) \xrightarrow{D} \N(0, 1)\]
In other words, the CDF of the left-hand side goes to the standard Normal CDF, $\Phi$. In terms of the sample mean, the CLT says
\[ \frac{\sqrt{n} (\bar{X}_n - \mu_X)}{\sigma_X} \xrightarrow{D} \N(0, 1)\]

<!--chapter:end:Basic-concepts-in-Probability-and-Statistics.Rmd-->

---
title: "Introduction to Basic Concepts in Quantitative Genetics"
author: "Peter Sørensen, Guillaume Ramstein"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: yes
    includes:
      in_header: preamble.tex
  html_document:
    number_sections: yes
    includes:
      in_header: mathjax_header.html
  word_document: default
biblio-style: apalike
link-citations: yes
bibliography: lbgfs2021.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(19)
```



## Learning objective:  {-}    
This section introduces the basic concepts in quantitative genetics such as:

*	Genetic value and variance for a quantitative trait
*	Genetic parameters (heritability, genetic variance and correlation)
*	Difference between genetic values and breeding values
*	Infinitesimal model

These concepts are relevant for a range of genetic and statistical analyses of complex traits and diseases in animal and plant populations, including:

*	Estimating the effect of single locus (or marker) for gene discovery
*	Estimating the effect of multiple loci (or markers) for genomic prediction
*	Estimating the heritability of a trait (the part of its variability due to genetics)
*	Estimating breeding values by pedigree or genomic information
*	Selection of breeding individuals based on estimated breeding values
*	Prediction of selection response based on estimated heritability (breeder’s equation)

In the appendix, further details about the quantative genetic models are presented, but these may be outside the scope of this BSc course.


# Quantitative Genetics
Quantitative genetics, also referred to as the genetics of complex traits, is the study of quantitative traits. Quantitative genetics is based on models in which many genes influence the trait, and in which non-genetic factors may also be important. Quantitative traits such as size, obesity or longevity vary greatly among individuals. Their phenotypes are continuously distributed phenotypes and do not show simple Mendelian inheritance (i.e., their phenotypes are distributed in discrete categories determined by one or a few genes). The quantitative genetics framework can also be used to analyze discrete traits like litter size (which consist of discrete counts like 0, 1, 2, 3, …) or binary traits like survival to adulthood (which consist of 0 or 1, ‘dead’ or ‘alive’, etc.), provided that they have a polygenic basis (i.e., they are determined by many genes). The quantitative genetics approach has diverse applications: it is fundamental to an understanding of variation and covariation among relatives in natural and managed populations; it is also used as basis for selective breeding methods in animal and plant populations  (https://doi.org/10.1098/rstb.2009.0203).


## Infinitesimal model
The infinitesimal model, also known as the polygenic model, is a widely used genetic model in quantitative genetics. Originally developed in 1918 by Ronald Fisher, it is based on the idea that variation in a quantitative trait is influenced by an infinitely large number of genes, each of which makes an infinitely small (infinitesimal) contribution to the phenotype, as well as by environmental (non-genetic) factors. In the most basic model the phenotype (P) is the sum of genetic effects (G), and environmental effect (E):

\begin{align}
P = G + E
\end{align}

The genetic effect (G) in the model can be split into additive effects (A), dominance effects (D), and epistatic effects (I) such that the expanded infinitesimal model becomes:

\begin{align}
P = A + D + I + E
\end{align}

The genetic effect may also depend on the environment in which they are expressed (e.g., in plants a drought-tolerance gene may have a favorable effect on grain yield under water-limited conditions, but may be useless under irrigation). Therefore we may consider an extended version of the infinitesimal model where the phenotype (P) is the sum of genetic effects (G), environmental effect (E), and genotype-environment interaction effects (G×E):

\begin{align}
P = G + E + GxE
\end{align}

In practice, the genotype-environment interaction effect can be important for the phenotype of individuals, but for the sake of simplicity we will ignore them in the remainder of this section. Therefore, hereafter, we will assume that genotypic effects are not impacted by environmental factors.

### Genetic effects, genetic values and breeding values
The genetic effect (G) in the model can include additive effects (A), dominance effects (D), and epistatic effects (I). Additive effects are the summed effects of individual alleles. Dominance effects are interactions between alleles within loci. Epistatic effect are interactions between alleles in different loci and can therefore only occur if two or more loci affect the trait. 

Consider an individual that is diploid, like most animals and plants like maize, soybean, barley (i.e., they carry two copies of every genes, except in their sexual chromosomes). Assume that one locus in its genome exists under two possible alleles: A1 and A2, with respective allele effects +1 and -1.  How do the individual’s alleles combine into a genotype? They may combine additively, so that the value of a genotype (the combination of two alleles genotype) is simply the sum of allele effects, but this is only a very special case! If genetic effects are entirely additive, then the value of each possible genotype is the sum of their respective allele effects, i.e., -2 if the individual is A2A2, 0 if it is A2A1 (or A1A2), and +2 if it is A1A1. Generally, the value of each genotype will depend on the combination of alleles within one locus (G = A + D) or across multiple loci (G = A + D + I). For example, in presence of dominance, the value of each possible genotype may be -2 if the individual is A2A2, +1 if it is A2A1 (or A1A2), and +2 if it is A1A1.

#### Additive Effects
Additive effects are the summed effects of average allele effects. Quite confusingly, additive effects depend on the population, because average allele effects depend on the frequency of genotypes in the population! For example, assume that genotypes have values -2 (A2A2), +1 (A2A1) and +2 (A1A1). In a population consisting of 25% A2A2, 50% A2A1 and 25% A1A1, you would expect the A1 allele in a A1A2 genotype 2/3 of the time, and you would expect the A1 allele in a A1A1 genotype 1/3 of the time. In another population consisting of 90% A2A2, 18% A2A1 and 1% A1A1, you would expect the A1 allele in a A1A2 genotype about 95% of the time, and in a A1A1 genotype only about 5% of the time. As a result, the effect of the A1 allele, averaged over genotypes, will not be the same, from one population to another.
The concept of additive genetic effects and average allele effects is fundamental to quantitative genetics. However, it is one of is most confusing, precisely because of the dependance of allele effects on genotype frequencies.

#### Dominance Effects
Dominance genetic effects are the interactions among alleles at a given locus. This is an effect that is extra to the sum of the additive allele effects. Each genotype has its own dominance effect, denoted by  $\delta_{ij}$, for the specific combination of alleles i and j, (e.g., $\delta_{A_{1}A_{2}}$), and each of them are non-zero quantities. Using the previous example, the additive and dominance effects would give .....to be completed

#### Epistatic Genetic Effects
Epistatic genetic effects encompass all possible interactions among the loci impacting the trait, whenever there is more than one such loci. This includes all two-way interactions (e.g., interactions between loci A and B, A and C), three-way interactions (e.g., joint interaction among A, B and C), etc. Epistasis can be decomposed, so it includes interactions between additive effects at different loci, interactions between additive effects at one locus with dominance effects at a second locus, and interactions between dominance effects at different loci.

#### Genetic value versus Breeding value       
For selective breeding purposes additive genetic effects are of primary interest. This is because additive effects are generally the largest of the genetic effects, and the allelic effects are passed directly to offspring while the other genetic effects are not transmitted to the progeny, and are generally smaller in magnitude. The sum of the additive effects of all loci on a quantitative trait is known as the true breeding value.

* Breeding value = the value of genes to progeny (additive effects only)

* Genetic value = the value of genes to self (which includes additive, dominance and epistatic effects)

The difference between genetic value and breeding value is largely largely dominance deviation. This is because an individual can express dominance deviation (e.g. an A1A2 heterozygote). However, an individual cannot pass on dominance deviation to its progeny as it only transmits one allele (e.g., an A1A2 heterozygote will either transmit a A1 gamete or an A2 gamete to one of its progeny, but not both!)
With fully inbred lines, offspring have the same genotype as their parent, and hence the entire parental genotypic value G is passed along. Hence, favorable interactions between alleles  are not lost by randomization under random mating but rather passed along.
When offspring are generated by crossing (or random mating), each parent contributes a single allele at each locus to its offspring, and hence only passes along a part of its genotypic value. This part is determined by the average effect of the allele. However, any favorable interaction between alleles is not passed along to their offspring.


### Infinite number of loci each with small effect on the phenotype
Quantitative traits do not behave according to simple Mendelian inheritance laws. More specifically, their inheritance cannot be explained by the genetic segregation of one or a few genes. Even though Mendelian inheritance laws accurately depict the segregation of genotypes in a population, they are not tractable with the large number of genes which typically affect quantitative traits. To better understand the infinitesimal model assume Mendelian inheritance to occur at every locus in the genome. Let’s say there are 30,000 gene loci in the genome. The number of alleles at each locus varies from 2 to 30 or more. If we assume that there are only two alleles (3 possible genotypes) per locus, and gene loci segregate independently, then the number of possible genotypes (considering all loci simultaneously) would be $3^{30000}$ which is large enough to give the illusion of an infinite number of loci. Furthermore each of these loci could contribute additive and dominance effects in addition to interaction effects.


#### Distribution of genetic and phenotypic values in single locus model
First we will consider how to model the genetic basis of a quantitative trait when a single locus affects the trait of interest. We call this a single-locus model. The distribution of the genetic values for a set of individuals will be discrete. The frequency of the genetic values depend on genotype frequencies, which in turn depend on allele frequencies of $A_1$ and $A_2$. The phenotype is however also influenced by the environment. If we assume that the environmental effects are normally distributed (e.g. $\N(0,\sigma^2=1)$) then we can observe that the phenotype distribution is infact normally distributed. 

#### Distribution of genetic and phenotypic values in multiple loci model
Now we will consider a multiple-locus model. When several loci are causal (i.e., they have an effect on a certain trait), then we talk about a __polygenic model__. Letting the number of causal loci tend to infinity, the resulting model is called an __infinitesimal model__. From a statistical point of view, the breeding values in an infinitesimal model are considered random with a known distribution. Due to the central limit theorem, this distribution tends to a normal distribution, because of the infinitely large number of causal loci. The central limit theorem says that the distribution of any sum of a large number of very small effects converges to a normal distribution. In our case where a given trait of interest is thought to be influenced by a large number of genetic loci each having a small effect, the sum of the breeding values of all loci together can be approximated by a normal distribution. The histograms below show a better approximation to the normal distribution for breeding values (summed allele effects at causal loci), as the number of causal loci increases. In practice, 100 independently segregating causal loci may be large enough, so that the infinitesimal model (and the normal approximation is genomic models) is accurate enough for predictions.

```{r, echo=FALSE, fig.cap="Distribution of genetic and phenotypic values for a quantitative trait influenced by a single locus model (top panel) or multiple loci (bottom panel)"}
genotype <- c("A1A1","A1A2","A2A2")
genotype_effects <- c(-1,0,1)
names(genotype_effects) <- genotype
n = 10000     # number of individuals
af1 = 0.5     # allele frequency of allele A1 
af2 = 1- 0.5  # allele frequency of allele A2
genotype_prob <- c(af1*af1, 2*af1*af2,af2*af2)
genotypes <- sample(genotype,size=n,prob=genotype_prob, replace=TRUE)

layout(matrix(1:6,ncol=3, byrow=TRUE))
a <- genotype_effects[genotypes]
e <- rnorm(n)
y <- a+e
hist(y, xlab="phenotypic values",main="P")
hist(a, xlab="genetic values",main="G")
hist(e, xlab="environmental values",main="E")

m <- 100      # number of causal loci
a <- rep(0,n) # initial vector of genetic values
for (locus in 1:m) {
genotypes <- sample(genotype,size=n,prob=genotype_prob, replace=TRUE)
a <- a + genotype_effects[genotypes]/m
}
e <- rnorm(n)
y <- a+e
hist(y, xlab="phenotypic values",main="P")
hist(a, xlab="genetic values",main="G")
hist(e, xlab="environmental values",main="E")
```


### Genetic parameters

Fisher (1918) and Wright (1921) have introduced fundamental statistical methods in quantitative genetics:

* analysis of variance: the partition of phenotypic variation into heritable (A) and non-heritable components (D, I and E).
* resemblance among relatives: the estimations of the proportion of loci shared by relatives under the infinitesimal model.


#### Genetic variance:

In the model proposed by Fisher (1918), Cockerham (1954) and Kempthorne (1954), covariance among relatives is described in terms of the additive genetic variance $V_{A}$ (variance of additive genetic effects, or breeding values), dominance variance $V_{D}$ (variance of interaction effects between alleles in the same locus), and epistatic variance $V_{AA}$, $V_{AD}$, $V_{DD}$, …. (variance of interaction effects – additive and/or dominance effects – among loci) (Falconer & Mackay 1996; Lynch & Walsh 1998). These partitions are not dependent on numbers of genes or how they interact, but in practice the model is manageable only when the effects are independent from each other, requiring many important assumptions. These include random mating, and hence Hardy-Weinberg equilibrium (i.e. no inbred individuals), linkage equilibrium (independent segregation of loci, which requires many generations to achieve for tightly linked genes) and no selection.

\begin{align}
V_{P} &= V_{G} + V_{E} \notag \\
      &= V_{A} + V_{D} + V_{I} + V_{E}
\end{align}

\begin{align}
\sigma^2_{P} &= \sigma^2_{G} + \sigma^2_{E}  \notag \\
             &= \sigma^2_{A} + \sigma^2_{D} + \sigma^2_{I} + \sigma^2_{E}
\end{align}

Many more terms may be included, such as maternal genetic effects, and genotype × environment interaction. The model has unlimited opportunities for complexity. This is a strength, in that it is all-accommodating, and a weakness, in that datasets may allow to partition only a few components. In practice, assumptions must be made to reduce the complexity of the resemblance among relatives. Usually, the resemblance among relatives is assumed to depend only on additive genetic variance $V_{A}$ and dominance variance $V_{D}$, so that the following sources of covariation are neglected:

* Epistatic variance (interaction effects among loci are small compared to additive and dominance effects)
* Environmental variance (effects of __shared environments__ are assumed to be small enough)

#### Heritability:
The models and summary statistics defined by Fisher and Wright have remained at the heart of quantitative genetics, not least because they provide ways to make predictions of important quantities, such as

*	Breeding value ($A$), the expected performance of of an individual’s offspring
* Broad-sense heritability, the ratio of total genetic variance V_G to the overall phenotypic variance $V_{P}$:

\begin{align}
H^2 &= V_{G}/V_P \notag \\
    &= (V_{A} + V_{D} + V_{I})/V_P  \notag \\
H^2 &= \sigma^2_{G}/\sigma^2_P  \notag \\
    &= (\sigma^2_{A} + \sigma^2_{D} + \sigma^2_{I})/\sigma^2_P  \notag 
\end{align}

* Narrow-sense heritability, the ratio of additive genetic variance $V_{A}$ to the overall phenotypic variance $V_{P}$:

\begin{align}
h^2 &= V_{A}/V_P  \notag \\
h^2 &= \sigma^2_{A}/\sigma^2_P
\end{align}

* The response to artificial or natural selection, the increase (or decrease) of genetic values due to selection of individuals, over generations

In view of the assumed complexity of the underlying gene action, involving many loci with unknown effects and interactions, much quantitative genetic analysis has, unashamedly, been at a level of the ‘black box’.

#### Genetic correlation:
In a general quantitative genetic model, in which, for each individual, two traits ($P_1$ and $P_2$) are are each defined as the sum of a genetic value ($G_1$ and $G_2$) and a environmental value ($E_1$ and $E_2$): 
\begin{align}
P_1 = G_1 + E_1 \\
P_2 = G_2 + E_2
\end{align}

The phenotypic correlation ($\rho_{P_{12}}$) between the traits is defined as:

$$\rho_{P_{12}}=\frac{\sigma_{P_{12}}}{\sqrt{\sigma_{P_{1}}^2 \sigma_{P_{2}}^2}}$$

where $\sigma_{P_{12}}$ is the phenotypic covariance and  $\sigma_{P_{1}}^2$ and $\sigma_{P_{2}}^2$  are the variances of the phenotypic values for the two traits in the population.
The genetic correlation ($\rho_{G_{12}}$) of the traits is defined as:

$$\rho_{G_{12}}=\frac{\sigma_{G_{12}}}{\sqrt{\sigma_{G_{1}}^2 \sigma_{G_{2}}^2}}$$

where $\sigma_{G_{12}}$ is the genetic covariance and  $\sigma_{G_{1}}^2$ and $\sigma_{G_{2}}^2$ are the variances of the genetic values for the two traits in the population.



### Basic questions remain    
On the premise that many genes and environmental factors interact to impact the trait, it will be difficult to determine the action of individual causal genes. Many basic questions remain: What do the genes do; how do they interact; on what traits does natural selection act; why is there so much genetic variation; and can we expect continued genetic improvement in selection programmes? Ultimately, we want to know at the molecular level not just which genes are involved, whether structural or regulatory, but what specific mutation (nucleotide substitution, deletious, copy number variant, etc.) is responsible for genetic effects, and how the causal genes are controlled.






\newpage
\mbox{}


## Single locus model for a quantitative trait
In this section we will be introducing the single locus model for a quantitative trait. Quantitative traits do not take discrete levels, instead they show continuous distributions. Although quantitative trait are most likely influenced by many loci, it helps to first consider the case of only one causal locus, in the __single-locus model__. The single-locus model will provide the theoretical basis for more complex models, namely the infinitesimal model and genomic models (statistical models describing the effects of marker loci). In animal and plant breeding, our goal is to improve the population at the genetic level. The term improvement implies the need for a quantitative assessment of our trait of interest. Furthermore, we must associate the genotypes in the population to the quantitative values of our trait. In the following, population mean, values (phenotypic value P, genetic value G, and breeding value A) and associated variance ($V_{P}$, $V_{G}$ and $V_{A}$) will be defined for a single causal locus.


### Genotypic Values {#geno-value}
The values $G_{ij}$ to each genotype $A_iA_j$ are assigned as shown in Figure \@ref(fig:genotypicvalue). 

```{r genotypicvalue, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", fig.cap="Genotypic Values", out.width="100%"}
#rmddochelper::use_odg_graphic(ps_path = "odg/genotypicvalue.odg")
knitr::include_graphics(path = "odg/genotypicvalue.png")
```

The origin (the zero value) for the genotypic values  is placed half-way  between the two homozygous genotypes $A_2A_2$ and $A_1A_1$. Here we are assuming that $A_1$ is the favorable allele. This leads to values of $+a$ for genotype $A_1A_1$ and of $-a$ for genotype $A_2A_2$, where a is called additive gene action. The value of genotype $A_1A_2$ is set to $d$ and d is called dominance gene actions. Table \@ref(tab:tabsumgenvalue) summarizes the values for all genotypes.

```{r tabsumgenvalue, echo=FALSE}
knitr::kable(
  data.frame(Variable = c("$GV_{11}$", "$GV_{12}$", "$GV_{22}$"),
             Genotype = c("$A_1A_1$", "$A_1A_2$", "$A_2A_2$"),
             Values   = c("a", "d", "-a")),
  format   = ifelse(knitr::is_latex_output(), 'latex', 'html'),
  booktabs = TRUE,
  caption = "Values for all Genotypes",
  align = "c",
  escape = FALSE
)
```


### Population Mean {#pop-mean}
For the complete population, we can compute the __population mean__ ($\mu$) of all values at the locus $G$. This mean corresponds to the expected value and is computed as 

For the complete population, we can compute the __population mean__ ($\mu$) of all values at the locus $G$. Under the Hardy-Weinberg equilibrium,  $\mu$ corresponds to the expected value in a panmictic population, and is computed as 

\begin{align}
\mu &= GV_{11} * f(A_1A_1) + GV_{12} * f(A_1A_2) + GV_{22} * f(A_2A_2) \notag \\
    &= a * p^2 + d *2pq + (-a) * q^2 \notag \\
    &= (p-q)a + 2pqd
(\#eq:popmean)
\end{align}
 
Under the simplifying assumptions of Hardy-Weinberg equilibrium, the frequency f of genotypes ($A_1A_1$, $A_1A_2$, $A_2A_2$) depends only on the frequency p of allele $A_1$, and the frequency q=1-p if allele $A_2$. The population mean then depends on the values of $a$ and $d$ and on the allele frequencies $p$ and $q$. The larger the difference between $p$ and $q$ the more influence the value $a$ has on $\mu$ relatively to $d$, because for very different $p$ and $q$ the frequency -- and contribution -- of heterozygotes to $\mu$ is very small (the product 2pq is low). On the other hand, if $p=q=0.5$, then $\mu = 0.5d$. For loci with $d=0$, the population mean $\mu = (p-q)a$ and hence, if in addition we have $p=q$, then $\mu=0$. 



### Breeding Values {#breed-value}
The breeding value of an individual $i$ is defined as two times the difference between the mean value of its offspring and the population mean.



<!-- ```{definition, name = "Breeding Value", label="defbreedingvalue"} -->
<!-- The breeding value of an animal $i$ is defined as two times the difference between the mean value of offspring of animal $i$ and the population mean. -->
<!-- ``` -->

Applying this definition and using the parameters that we have computed so far leads to the following formulas for the breeding value of an individual with a certain genotype. 


#### Breeding value for $A_1A_1$
Assume that we have a given parent $S$ with a genotype $A_1A_1$ and we want to compute its breeding value. Let us further suppose that our single parent $S$ is mated to a potentially infinite number of individuals from the idealized population, then we can deduce the following mean genotypic value for the offspring of parent $S$. 

\vspace{5ex}

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
& \multicolumn{2}{|c|}{Mates of $S$} \\
\hline
& $f(A_1) = p$       &  $f(A_2) = q$   \\
\hline
Parent $S$       &                    &                 \\
\hline
$f(A_1) = 1$ &  $f(A_1A_1) = p$   &  $f(A_1A_2) = q$\\
\hline
\end{tabular}
\end{center}

\vspace{5ex}

Because parent $S$ has genotype $A_1A_1$, the frequency $f(A_1)$ of a $A_1$ allele coming from $S$ is $1$ and the frequency $f(A_2)$ of a $A_2$ allele is 0. The expected genetic value ($\mu_{11}$) of the offspring of individual $S$ can be computed as

\begin{equation}
\mu_{11} = p*a + q*d
(\#eq:MeanOffGen11)
\end{equation}

Applying definition \@ref(def:defbreedingvalue), we can compute the breeding value ($BV_{11}$) for individual $S$ as shown in equation \@ref(eq:BVGen11) while using the results given by equations \@ref(eq:MeanOffGen11) and \@ref(eq:popmean).

\begin{align}
BV_{11} &=  2*(\mu_{11} - \mu)  \notag \\
        &=  2\left(pa + qd - \left[(p - q)a + 2pqd \right] \right) \notag\\
        &=  2\left(pa + qd - (p - q)a - 2pqd \right) \notag\\
        &=  2\left(qd + qa - 2pqd\right) \notag \\
        &=  2\left(qa + qd(1 - 2p)\right) \notag \\
        &=  2q\left(a + d(1 - 2p)\right) \notag \\
        &=  2q\left(a + (q-p)d\right)
(\#eq:BVGen11)
\end{align}


Breeding values for parents with genotypes $A_2A_2$ and $A_1A_2$ are derived analogously.

#### Breeding value for $A_2A_2$
First, we determine the expected genotypic value for the offspring of a parent $S$ with genotype $A_2A_2$

\vspace{5ex}

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
& \multicolumn{2}{|c|}{Mates of parent $S$} \\
\hline
& $f(A_1) = p$       &  $f(A_2) = q$   \\
\hline
Parent $S$       &                    &                 \\
\hline
$f(A_2) = 1$ &  $f(A_1A_2) = p$   &  $f(A_2A_2) = q$\\
\hline
\end{tabular}
\end{center}

\vspace{5ex}

The expected genetic value ($\mu_{22}$) of the offspring of individual $S$ can be computed as

\begin{equation}
\mu_{22} = pd - qa
(\#eq:MeanOffGen22)
\end{equation}

The breeding value $BV_{22}$ corresponds to

\begin{align}
BV_{22} &=   2*(\mu_{22} - \mu)  \notag \\
        &=   2\left(pd - qa - \left[(p - q)a + 2pqd \right] \right) \notag \\
        &=   2\left(pd - qa - (p - q)a - 2pqd \right) \notag \\
        &=   2\left(pd - pa - 2pqd\right) \notag \\
        &=   2\left(-pa + p(1-2q)d\right) \notag \\
        &=  -2p\left(a + (q - p)d\right)
(\#eq:BVGen22)
\end{align}


#### Breeding value for $A_1A_2$
The genotype frequencies of the offspring of a parent $S$ with a genotype $A_1A_2$ is determined in the following table.

\vspace{5ex}

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
& \multicolumn{2}{|c|}{Mates of parent $S$} \\
\hline
& $f(A_1) = p$       &  $f(A_2) = q$   \\
\hline
Parent $S$       &                    &                 \\
\hline
$f(A_1) = 0.5$ &  $f(A_1A_1) = 0.5p$   &  $f(A_1A_2) = 0.5q$\\
\hline
$f(A_2) = 0.5$ &  $f(A_1A_2) = 0.5p$   &  $f(A_2A_2) = 0.5q$\\
\hline
\end{tabular}
\end{center}

\vspace{5ex}

The expected mean genotypic value of the offspring of parent $S$ with genotype $A_1A_2$ is computed as

\begin{equation}
\mu_{12} = 0.5pa + 0.5d - 0.5qa = 0.5\left[(p-q)a + d \right]
(\#eq:MeanOffGen12)
\end{equation}

The breeding value $BV_{12}$ corresponds to 

\begin{align}
BV_{12} &=   2*(\mu_{12} - \mu) \notag \\
        &=   2\left(0.5(p-q)a + 0.5d - \left[(p - q)a + 2pqd \right] \right) \notag \\
        &=   2\left(0.5pa - 0.5qa + 0.5d - pa + qa - 2pqd \right) \notag \\
        &=   2\left(0.5(q-p)a + (0.5 - 2pq)d \right) \notag \\
        &=   (q-p)a + (1-4pq)d  \notag \\
        &=   (q-p)a + (p^2 + 2pq + q^2 -4pq)d  \notag \\
        &=   (q-p)a + (p^2 - 2pq + q^2)d  \notag \\
        &=   (q-p)a + (q - p)^2d   \notag \\
        &=   (q-p)\left[a + (q-p)d \right]
(\#eq:BVGen12)
\end{align}

### Summary of Breeding Values
The term $a + (q-p)d$ appears in all three breeding values. We replace this term by $\alpha$ and summarize the results in the following table.

\vspace{5ex}

\begin{center} 
\begin{tabular}{|c|c|}
  \hline
  Genotype  &  Breeding Value\\
  \hline
  $A_1A_1$  &  $2q\alpha$    \\
  \hline
  $A_1A_2$  &  $(q-p)\alpha$ \\
  \hline
  $A_2A_2$  &  $-2p\alpha$   \\
  \hline
\end{tabular}
\end{center}

\vspace{5ex}

### Allele Substitution {#allele-substitution}
The difference between genotypes $A_2A_2$ and $A_1A_2$ is in the number of $A_1$-alleles. $A_2A_2$ has zero $A_1$-alleles and $A_1A_2$ has one $A_1$-allele. Let us imagine that we take individual $i$ with a $A_2A_2$ genotype and use the CRISPR-CAS genome editing technology to replace one of the $A_2$ alleles in individual $i$ by a $A_1$ allele (see Figure \@ref(fig:genome-editing-allele-substitution)). After applying the gene editing procedure to individual $i$ at locus $G$, individual $i$ would have genotype $A_1A_2$. 

```{r genome-editing-allele-substitution, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", fig.cap="Schematic Depiction of Genome Editing on Individual i", out.width="100%"}
#rmdhelp::use_odg_graphic(ps_path = "odg/genome-editing-allele-substitution.odg")
knitr::include_graphics(path = "odg/genome-editing-allele-substitution.png")
```

Due to the application of genome editing at locus $A$ of individual $i$ the breeding value changed. Before the genome editing procedure it was $BV_{22}$ and after genome editing the breeding value of individual $i$ is $BV_{12}$. So the effect of replacing a $A_2$ allele by a $A_1$ allele on the breeding value corresponds to the difference $BV_{12} - BV_{22}$. The computation of this difference between the breeding value $BV_{12}$ and $BV_{22}$ is:

\begin{align}
    BV{12} - BV_{22} &=   (q-p)\alpha - \left( -2p\alpha \right)  \notag \\
                      &=   (q-p)\alpha + 2p\alpha \notag \\
                      &=   (q-p+2p)\alpha \notag \\
                      &=   (q+p)\alpha \notag \\
                      &=   \alpha
  (\#eq:AdditiveBv1)
\end{align}

The analogous computation can be done by comparing the breeding values $BV_{11}$ and $BV_{12}$.

\begin{align}
    BV_{11} - BV_{12} & = & 2q\alpha - (q-p)\alpha \notag \\
                      & = & \left(2q - (q-p)\right)\alpha \notag\\
                      & = & \alpha 
  (\#eq:AdditiveBv2)
\end{align}

Because the differences between breeding values computed in \@ref(eq:AdditiveBv1) and \@ref(eq:AdditiveBv2) are equal, we can conclude that the breeding values show a linear dependence on the number of $A_1$ alleles. This is the reason why the breeding values are also called additive effects, because adding a further $A_1$ allele instead of a $A_2$ allele has always the same effect on the breeding values, namely just adding the constant allele substitution effect $\alpha$. 


### Dominance Deviation
When looking at the difference between the genotypic value $GV_{ij}$ and the breeding value $BV_{ij}$ for each of the three genotypes, we get the following results.

  \begin{align}
  GV_{11} - BV_{11} &=   a - 2q \alpha \notag \\
                   &=   a - 2q \left[ a + (q-p)d \right] \notag \\
                   &=   a - 2qa -2q(q-p)d \notag \\
                   &=   a(1-2q) - 2q^2d + 2pqd \notag \\
                   &=   \left[(p - q)a + 2pqd\right] - 2q^2d \notag \\
                   &=   \mu + D_{11} 
  \end{align}

  \begin{align}
  GV_{12} - BV{12} &=   d - (q-p)\alpha \notag \\
                   &=   d - (q-p)\left[ a + (q-p)d \right] \notag \\
                   &=   \left[(p-q)a + 2pqd\right] + 2pqd \notag \\
                   &=   \mu + D_{12}
  \end{align}

  \begin{align}
  GV_{22} - BV_{22} &=   -a - (-2p\alpha) \notag \\
                   &=   -a + 2p\left[ a + (q-p)d \right] \notag \\
                   &=   \left[(p-q)a + 2pqd\right] - 2p^2d \notag \\
                   &=   \mu + D_{22} \notag
  \end{align}

The difference all contain the population mean $\mu$ plus a certain deviation. This deviation term is called __dominance deviation__. It corresponds to the part of genotypic values which are not accounted for by additive effects -- and linear allelic subsitution effects. Therefore, it captures the non-linear relationships between genotypic values and the number of $A_1$ alleles (zero in $A_2A_2$, 1 in $A_1A_2$, 2 in $A_2A_2$).


### Summary of Values
The following table summarizes all genotypic values, all breeding values and the dominance deviations. 

\vspace{5ex}

\begin{center} 
\begin{tabular}{|c|c|c|c|}
   \hline
   Genotyp  &  Genotypic value     &  Breeding Value    &  Dominance Deviation \\
   $A_iA_j$ &  $GV_{ij}$            &  $BV_{ij}$         &  $D_{ij}$           \\
   \hline
   $A_1A_1$ &  $a$                 &  $2q\alpha$        &  $-2q^2d$          \\
   \hline
   $A_1A_2$ &  $d$                 &  $(q-p)\alpha$     & $2pqd$             \\
   \hline
   $A_2A_2$ &  $-a$                &  $-2p\alpha$       & $-2p^2d$           \\
   \hline
\end{tabular}
\end{center}    

\vspace{5ex}


The formulas in the above shown table assume that $A_1$ is the favorable allele with frequency $f(A_1) = p$. The allele frequency of $A_2$ is $f(A_2) = q$. Since we have a bi-allelic locus, $p+q=1$.

Based on the definition of dominance deviation, the genotypic values $GV_{ij}$ can be decomposed into the following components: population mean ($\mu$), breeding value ($BV_{ij}$) and dominance deviation ($D_{ij}$) according to equation \@ref(eq:SeparationGenoValue).

\begin{align}
GV_{ij} &=   \mu + BV_{ij} + D_{ij}
(\#eq:SeparationGenoValue)
\end{align}


Taking expected values on both sides of equation \@ref(eq:SeparationGenoValue) and knowing that the population mean $\mu$ was defined as the expected value of the genotypic values in the population, i.e. $E\left[ GV \right] = \mu$, it follows that the expected values of both the breeding values and the dominance deviations must be $0$. More formally, we have 

\begin{align}
E\left[ GV \right] &=  E\left[ \mu + BV + D \right] \notag \\
                  &=  E\left[ \mu \right]  + E\left[ BV \right] + E\left[ D \right] \notag \\
                  &=  \mu
(\#eq:ExpValueGenBvDom)
\end{align}

From the last line in equation \@ref(eq:ExpValueGenBvDom), it follows that $E\left[ BV \right] = E\left[ D \right] = 0$. This also shows that both breeding values and dominance deviations are defined as deviation from the population mean.


### Variances {#variances}
The population mean $\mu$ and the breeding values were defined as expected values ($\mu$: expected value of genotypes in a given generation; breeding value: expected advantage of the offspring of each genotype, relative to $\mu$). Their main purpose is to assess the state of a given population with respect to a certain genetic locus and its effect on a phenotypic trait of interest. One of our primary goals in animal and plant breeding is to improve the populations at the genetic level through the means of selection and mating. Selection of potential parents that produce offspring that are closer to our breeding goals is only possible, if the selection candidates show a certain level of variation in the traits that we are interested in.  

In statistics the measure that is most often used to assess variation in a certain population is called __variance__. For any given discrete random variable $X$ the variance is defined as the second central moment of $X$ which is computed as shown in equation \@ref(eq:VarianceDiscreteRV).

\begin{equation}
Var\left[X\right] = \sum_{x_i \in \mathcal{X}} (x_i - \mu_X)^2 * f(x_i)
(\#eq:VarianceDiscreteRV)
\end{equation}

 \vspace*{1ex}
  \begin{tabular}{p{1cm}p{1cm}p{6cm}}
  where & $\mathcal{X}$: &  set of all possible $x$-values\\
        & $f(x_i)$       &  probability that $x$ assumes the value of $x_i$ \\
        & $\mu_X $       &  expected value $E\left[X\right]$ of $X$
  \end{tabular}
  
  
\vspace*{2ex}
In this section we will be focusing on separating the obtained variances into different components according to their causative sources. Applying the definition of variance given in equation  \@ref(eq:VarianceDiscreteRV) to the genotypic values $GV_{ij}$, we obtain the following expression.

\begin{align}
\sigma_G^2 = Var\left[V\right] &=   (GV_{11} - \mu)^2 * f(A_1A_1) \notag \\
                               &  +\  (GV_{12} - \mu)^2 * f(A_1A_2) \notag \\
                               &  +\  (GV_{22} - \mu)^2 * f(A_2A_2)
(\#eq:VarianceGenotypicValue)
\end{align}

where $\mu = (p - q)a + 2pqd$ the population mean.

Based on the decomposition of the genotypic value $GV_{ij}$ given in \@ref(eq:SeparationGenoValue), the difference between $GV_{ij}$ and $\mu$ can be written as the sum of the breeding value and the dominance deviation. Then $\sigma_G^2$ can be written as

\begin{align}
\sigma_G^2 = Var\left[V\right] &=   (BV_{11} + D_{11})^2 * f(A_1A_1) \notag \\
                               &  +\  (BV_{12} + D_{12})^2 * f(A_1A_2) \notag \\
                               &  +\  (BV_{22} + D_{22})^2 * f(A_2A_2)
(\#eq:GeneticVarianceBVDom)
\end{align}

Inserting the expressions for the breeding values $BV_{ij}$ and for the dominance deviation $D_{ij}$ found earlier and simplifying the equation leads to the result in \@ref(eq:FinalGeneticVariance). A more detailed derivation of $\sigma_G^2$ is given in the appendix (\@ref(appendix-derivations)) of this chapter.

\begin{align}
  \sigma_G^2 &=  2pq\alpha^2 + \left(2pqd \right)^2 \notag\\
             &=  \sigma_A^2 + \sigma_D^2
(\#eq:FinalGeneticVariance)             
\end{align}

The formula in equation \@ref(eq:FinalGeneticVariance) shows that $\sigma_G^2$ consists of two components. The first component $\sigma_A^2$ is called the __genetic additive variance__ and the second component $\sigma_D^2$ is termed __dominance variance__. As shown in equation \@ref(eq:VarBV) $\sigma_A^2$ corresponds to the variance of the breeding values. $\sigma^2_{a}$, the variance of breeding values, is also called the additive genetic variance, because as we have already seen the breeding values are additive in the number of favorable alleles. In populations where there is no additive genetic variance, individuals all have the same breeding value. Therefore, they will produce offspring with the same expected advantage (zero), and selection cannot generate any improvement over generations. Because $\sigma_D^2$ corresponds to the variance of the dominance deviation effects (see equation \@ref(eq:VarDom)) it is called dominance variance.


## Multiple locus model for a quantitative trait {#extension-to-more-loci}
When only a single locus is considered, the genotypic values ($GV_{ij}$) can be decomposed according to equation \@ref(eq:SeparationGenoValue) into population mean, breeding value and dominance deviation. When a genotype refers to more than one locus, the genotypic value may contain an additional deviation caused by non-additive combination effects. 


### Epistatic Interaction {#epistatic-interaction}
Let $GV_A$ be the genotypic value of locus $A$ and $GV_B$ denote the genotypic value of a second locus $B$, then the total genotypic value $GV$ attributed to both loci $A$ and $B$ can be written as 

\begin{align}
V &= V_A + V_B + I_{AB} 
(\#eq:AggregateGenotypicValueTwoLoci)
\end{align}

where $I_{AB}$ is the deviation from additive combination of these genotypic values. When computing the population mean earlier in this chapter, we assumed that $I$ was zero for all combinations of genotypes. If $I$ is not zero for any combination of genes at different loci, those genes are said to __interact__ with each other or to exhibit __epistasis__. The deviation $I$ is called interaction deviation or epistatic deviation. If $I$ is zero, the genes are called to act additively between loci. Hence _additive action_ may mean different things. When referring to one locus, it means absence of dominance. When referring to different loci, it means absence of epistasis.

Interaction between loci may occur between pairs or between higher numbers of different loci. The complex nature of higher order interactions, i.e., interactions between higher number of loci does not need to concern us. Because in the total genotypic value $GV$, interaction deviations of all sorts are treated together in an overall interaction deviation $I$. 

Applying the decomposition of the genotypic values $GV_A$ of locus $A$ and $GV_B$ of locus $B$ as shown in \@ref(eq:SeparationGenoValue) leads to 

\begin{align}
V &= V_A + V_B + I_{AB} \notag \\
  &= \mu_A + BV_A + D_A + \mu_B + BV_B + D_B + I_{AB}
(\#eq:DecomposeGenotypicValueTwoLoci)
\end{align}

Collecting terms in \@ref(eq:DecomposeGenotypicValueTwoLoci) as follows

\begin{align}
\mu &= \mu_A + \mu_B \notag \\
U   &= BV_A + BV_B \notag \\
D   &= D_A + D_B \notag \\
I   &= I{AB}
(\#eq:CollectVariables)
\end{align}

The decomposition shown in \@ref(eq:DecomposeGenotypicValueTwoLoci) and the collection of variables (see \@ref(eq:CollectVariables)) can be generalized to more than two loci. This leads to the following generalized decomposition of the overall total genotypic value $GV$ for the case of multiple loci affecting a certain trait of interest.

\begin{equation}
V = \mu + U + D + I
(\#eq:AggregateGenotypicValueMultipleLoci)
\end{equation}

where $U$ is the sum of the breeding values attributable to the separate loci and $D$ is the sum of all dominance deviations. For our purposes in animal and plant breeding where we want to assess the genetic potential of a selection candidate to be a parent of offspring forming the next generation, the __breeding value__ is the most important quantity. The breeding value is of primary importance because a given parent passes a random sample of its alleles to its offspring. We have seen in section \@ref(allele-substitution) that breeding values are additive in the number of favorable alleles. Hence the more favorable alleles a given parent passes to its offspring the higher the breeding value of this parent. 

On the other hand, the dominance deviation measures the effect of a certain genotype occurring in an individual and the epistatic deviation estimates the effects of combining different genotypes at different loci in the genome. But because parents do not pass complete genotypes nor do they pass stretches of DNA with several unlinked loci, but only a random collection of its alleles, it is really the breeding value that is of primary importance in assessing the genetic potential of a given selection candidate. 


### Interaction Variance {#interaction-variance}
If genotypes at different loci show epistatic interaction effects as described in section \@ref(epistatic-interaction), the interactions give rise to an additional variance component called $V_I$, which is the variance of interaction deviations. This new variance component $V_I$ can be further decomposed into sub-components. The first level of sub-components is according to the number of loci that are considered. Two-way interactions involve two loci, three-way interactions consider three loci and in general $n$-way interactions arise from $n$ d0ifferent loci. Epistatic interaction can be further decomposed according to whether they involve additive effects, dominance deviations or both, across loci. 

In general, interaction effects explain only a very small amount of the overall genetic variation. As already mentioned in section \@ref(#epistatic-interaction) for animal and plant breeding, we are mostly interested in the additive effects (the breeding values). This is also true when looking at the variance components. Hence dominance variance and epistatic deviations are not used very often in practical breeding application. 

<!--chapter:end:Basic-concepts-in-Quantitative-Genetics.Rmd-->

---
title: 'Brief Introduction to Plant and Animal Breeding'
author: "Peter Sørensen, Guillaume Ramstein"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: yes
    includes:
      in_header: preamble.tex
  html_document:
    number_sections: yes
    includes:
      in_header: mathjax_header.html
  word_document: default
biblio-style: apalike
link-citations: yes
bibliography: lbgfs2021.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(19)
```

These lecture notes contributes to "Compendium Plant and Animal Breeding and Genetics" (CPABG).   
The notes support the lectures.

The notes consist of the following chapters:    

Chapter 1: Brief Introduction to Plant and Animal Breeding (L8).    
Chapter 2: Basic concepts in Quantitative Genetics used in Breeding (L10).   
Chapter 3: Introduction to Estimation of Genetic Parameters used in Breeding (L12).   
Chapter 4: Introduction to Estimation of Breeding Values (L14).    
Chapter 5: Introduction to Estimation of Genomic Breeding Values (L16). 

Appendix 1: Basic concepts in Probability and Statistics    
Appendix 2: Basic concepts in Linear Algebra       
Appendix 3: Introduction to R       

The aim of this chapter is to give a brief introduction to breeding in animals and plants.   
We can in addition suggest some further readings.   
The text below is in part from the following sources(CC-BY-SA 4.0, https://creativecommons.org/licenses/by-sa/4.0/):   
https://en.wikipedia.org/wiki/Selective_breeding    
https://charlotte-ngs.github.io/lbgfs2021/cn/01_intro_lbg.pdf   


# Brief Introduction to Breeding in Animals and Plants

Selective breeding (also called artificial selection) is the process by which humans use animal breeding and plant breeding techniques to selectively develop particular phenotypic traits (characteristics) by choosing which parents will sexually reproduce and make offspring. Domesticated animals are known as breeds, normally bred by a professional breeder. Domesticated plants are known as varieties, cultigens, cultivars, or breeds. Two purebred animals of different breeds produce a crossbreed, and crossbred plants are called hybrids. Flowers, vegetables and fruit trees may be bred by amateurs or professional breeders: major crops are usually produced by professional breeders, in academia or industry.

In both animal and plant breeding, crossing techniques such as inbreeding, linebreeding, and outcrossing are utilized.    Charles Darwin (https://en.wikipedia.org/wiki/Charles_Darwin) discussed how selective breeding had been successful in producing change over time in his 1859 book, On the Origin of Species. Its first chapter discusses selective breeding and domestication of animals like pigeons, cats, cattle, and dogs. Darwin used artificial selection as a springboard to introduce and support the theory of natural selection.

The deliberate exploitation of selective breeding to produce desired results has become very common in agriculture and experimental biology.

Selective breeding can be unintentional. In plants, it may result from the process of human cultivation, and may produce unintended – desirable or undesirable – results. For example, in some grains, an increase in seed size may have resulted from certain ploughing practices rather than from the intentional selection of larger seeds. Most likely, there has been an interdependence between natural and artificial factors that have resulted in plant domestication.

## Fundamental Questions in Selective Breeding
Natural selection and selective breeding can both cause changes in animals and plants. The difference between the two is that natural selection happens naturally, but selective breeding only occurs when humans intervene. For this reason, selective breeding is sometimes called artificial selection. Selective breeding takes place over many generations. These are the main steps involved:

* Which traits should we breed for?
* How do we select the _best_ breeding individuals?
* What can breeders do to obtain the _best_ breeding individuals?

The term `best` is relative, because there is no `best` breeding for all situations and all environments. Breeding individuals that show high performance in one environment, may not perform as well in a different environment. For example, Holstein cows in Europe or North America, are able to produce a lot of milk, but they have difficulties surviving in Africa. Knowing that the environment plays an important role for animals and plants, we usually assume that breeding populations are more or less adapted to their environment.
Breeding individuals are usually described or characterized in terms of appearance or performance or a combination of both. In any case, we will be talking about __traits__ where any trait is an observable or measurable characteristic of an breeding individual. Examples of _observable_ traits in animals are 

* coat color
* size 
* muscling
* leg set
* udder conformation and many more.

Observable traits are mostly used to describe the appearance of an animal. In contrast to that, _measurable_ traits are mostly used to describe the performance of an animal. Examples of measurable traits are 

* body weight
* milk production
* protein and fat yield.

Note, it is important to distinguish between the observed or measured values of a trait which might be `red` coat color or $343$ kg of body weight and the traits themselves which are just coat color or body weight. The observed or measured values of a trait are also called __phenotypes__. 


## Genotypes and Phenotypes {#geno-pheno}
In selective breeding we are mainly concerned with changing the populations at the genetic level. The reason why we are interested in changing a population genetically is because parents do not pass their phenotypes per se to their offspring. Instead, they pass a random sample of their genes to their offspring. For each offspring every parent does transmit a different sample of their genes. From a genetic point of view, we want to know not only the most desirable phenotype, but also the most desirable genotypes. From the central dogma of molecular biology (https://en.wikipedia.org/wiki/Central_dogma_of_molecular_biology), it follows that an animal’s genotype provides the genetic basis for phenotypes. The relationship between phenotypes ($P$) and genotypes ($G$) can be summarized by the following equation:
\begin{equation}
P = G + E
\end{equation}

where $E$ represents the __environmental effects__. These may include external effects like climate and soil composition (abiotic factors) and exposure to beneficial or pathogenic microbes (biotic factors). They may also include developmental effects like maternal effects, or simply measurement errors (random fluctuations in measurement of phenotypes). Because we want to change our populations at the genetic level, we are interested in the effect of genotypes (G). In most cases, we are not able to directly observe or measure G. But we will see later on how we can estimate G based on measurements and observations of P and based on estimates of E.  In particular, we will see how __breeding values__ , the part of G that is transmitted over generation, are estimated and used by breeders to perform crosses and improve populations. Those tools are being described in the following section.


## Improvement of Breeding Populations
The purpose of selective breeding is to improve the traits selected for, in a breeding population. Once a breeding individual is conceived, its genotype is fixed and cannot be improved anymore. Breeders can improve populations at the genetic level using the following two tools:

 * __evaluation__  of individuals in the current generation 
 * __selection__ of the 'best' individuals (for phenotypes of interest)
 * __mating__ of selected individuals to generate the next (improved) generation

### Evaluation


### Selection
Selection is the process of choosing individuals in the current generation, as parents for the next generation. The application of selection in a certain population over a certain time changes the breeding individuals in that population at the genetic level. The most familiar form of selection is natural selection which occurs in natural and wildlife populations. Natural selection is one of the great forces of evolution and it also affects domestic animal and plant populations. All animal or plants with lethal genetic defects are naturally selected against, i.e., they do not survive before the breeder gets to evaluate them. 

Although natural selection cannot be ignored for agricultural animal and plant species, for the focus of animal and plant breeders is __artificial selection__. The idea behind artificial selection is simple. For a given trait all individuals in a breeding population are ranked according to their estimated breeding value. From this list, the top-ranking breeding individual are used as parents for the next generation. In most breeding populations, breeders are interested in proving more than one trait. When considering more than one trait, the question is how to come up with the ranking for the individuals to be selected as potential parents. There are several strategies to produce such a ranking based on a number of traits. It has been shown that using a weighted mean of the breeding values of all traits, which is called an __aggregate genotype__ , to rank all animals is an optimal procedure to be used as selection criterion. 

### Mating
The second tool we have available as animal or plant breeders is __mating__. In a mating scheme, we must decide which of the selected males and females are mated with each other. There are a number of different rules that can be followed. The application of a given set of rules is summarized as a mating system. There are three reasons for using a specific mating system.

1. producing offspring with extreme breeding values. When parents with extreme breeding values (high or low) are mated, offspring with extreme phenotypes can be expected. This is mostly used when a given trait is to be changed in one direction
2. making use of complementarity in parental traits. When neither of the parents is optimal, a mix of traits can be desirable. In such a case parental genotypes can be quite different. . When parents of different breeds are mated, then this is called __crossbreeding__. 
3. obtain positive effects due to heterosis. Hybrid vigor or heterosis in crossbreeding occurs when offspring performance exceeds the performance of the pure-breds. 

There might also be other aspects that influence a mating system, e.g. to restrict the level of inbreeding or to consider optimum genetic contribution theory.


## Scientific and Technological Advances is driving Breeding and Genetics 
  

Brief overview of these scientific and technological advances.....

### Quantitative Genetics
### Statistics
### Computing
### Genomic technologies
### Phenotyping technologies


<!--chapter:end:Brief-Introduction-to-Plant-and-Animal-Breeding.Rmd-->

---
title: "Estimation of Breeding Values"
author: "Guillaume Ramstein, Peter Sørensen,....."
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: yes
    includes:
      in_header: preamble.tex
  html_document:
    number_sections: yes
    includes:
      in_header: mathjax_header.html
  word_document: default
biblio-style: apalike
link-citations: yes
bibliography: lbgfs2021.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(19)
```




## Learning objective:  {-}    
This section introduces the basic concepts of estimating breeding values such as: 

   * basic principle behind estimating breeding values
   * accuracy of estimated breeding values
   * use of genetic relationships for estimating breeding values
   * relationship between genetic parameters and estimated breeding values
   * different methods, data sources and experimental designs for estimating breeding values



# Introduction
Estimation of breeding values is a fundamental component of breeding programmes. 
The purpose of breeding value assessment is to calculate the best possible estimate for the individual's breeding value. The breeding value for an individual is the total additive genetic value which is passed on to the offspring. Thus the breeding value is not a measure of how good an individual is in itself, 
but rather of the effect its genes will have in the population. Breeding values are used for:

  * comparing individuals in the breeding population and to select parents for the next generation
  * predicting the consequences of selection decisions 
  * describing genetic differences over time (result of previous selection)

The true breeding value (TBV) for an individual cannot be observed. It is only possible to measure the phenotypic value, which is influenced both by genotype and environment. Therefore, we need a way to infer the breeding value from the phenotypic value and select individuals based on an estimated breeding value (EBV). This is the objective of the breeding value estimation. 


# Basic principles for breeding value estimation
Breeding values are estimated using information on phenotypes and genetic relationships for individuals from the breeding population. As introduced previously the phenotype for a quantitative trait is the sum of both genetic and environmental factors. In general the amount of information provided by the phenotype about the breeding value is determined by the narrow sense heritability. Furthermore phenotypes collected from close relatives provide more information about the breeding value of an individual (as close compared to distant relatives share more DNA in common). In this section we will illustrate these principles using phenotypic sources genetic and relationships used for for estimating breeding values. We will now try to derive a general approach for predicting breeding values for 
any type of situation. Even though the procedure is general we will use a simple 
example to describe it. 
The breeding value is based on an assumption of a specific genetic model. In general the total genetic effect for an individual is the sum of both additive and non-additive effects that affect the trait: 

\begin{align}
	y &=\mu+a+d+i+\epsilon \notag		
\end{align}

where  $\mu$ is the population mean, $a$ is the additive effect, $d$ is the dominance effect, $i$ is the epistasis effect, and $e$ is the environmental deviation (or residual) not explained by the genetic effects in the model. However, only the additive genetic effects are passed on to the offspring and therefore have a breeding value. In contrast non-additive genetic effects (dominance and epistasis) are degraded by recombination and are not inherited but may be important for the individual's phenotype. Therefore we only consider the additive genetic model as the basis for breeding value estimation.

\begin{align}
			y=\mu+a+e	 \notag
\end{align}
			
The true breeding value for an individual is the sum of all additive genetic effects that affect the quantitative trait:
\begin{align}
		a_i=\sum_{i=1}^{q}\alpha_{ij} \notag
\end{align}

where $a_{i}$ is the total additive genetic effect and $\alpha_{ij}$ is the additive genetic effect for loci j in individual i. We therefore assume (based on the central limit theory) that the true breeding values, a, and the residual term, e, are normally distributed which means that the observed phenotype is also normally distributed
\begin{align}
a \sim N(0,\sigma^2_{a}) \notag \\
e \sim N(0,\sigma^2_{e}) \notag \\
y \sim N( \mu,\sigma^2_{y}) \notag
\end{align}


### Expected breeding value ($E(a|y)$) {-}
The breeding value cannot be observed but must be estimated or predicted from phenotypic data and genetic relationships between individuals from the breeding population. Prediction or estimation of an unknown parameter using statistical modelling expresses the predicted or estimated quantity as a mathematical function of the observed data. The question is how this function should look like and what properties the predicted breeding values should fulfill. For breeding purposes one objective for the
predicted breeding values is that the response to selection is maximized.
[Henderson, 1963] found that the improvement of an offspring generation compared to the parent generation can be maximized when parents are selected
based on the conditional expected value ($E(a|y)$) of the true breeding value $a$
given the observed phenotypic values y. Under the assumption of multivariate
normality for $a$ and $y$, the expected value of the breeding value conditional on the observed phenotype y can be written as:
\begin{align}
E(a|y) &= E(a) + Cov(a,y)[Var(y)]^{-1}(y-E(y))
\end{align}
Since the breeding value is defined as deviation from the general mean which means that the expected value $E(a)$ of the true breeding value $a$ is $E(a)=0$ and therefore the expected value of the breeding value can be written as:
\begin{align}
E(a|y) &= Cov(a,y)[Var(y)]^{-1}(y-E(y))
\end{align}
The expression for the estimate of the breeding value consists of two parts;
The term, $y-E(y)$, shows that the observed phenotypic values are corrected for the
fixed non-genetic environmental effects represented by $\mu$.
The term, $b_{a|y}=Cov(a,y)[Var(y)]^{-1}$, often referred to as the regression coefficient is a weighting factor with which the corrected phenotypic values are multiplied. 

To be able to compute an estimate of the breeding value ($E(a|y)$) we need to determine the values for the terms, $E(a)$, $E(y)$, $Var(y)$, and $Cov(a,y)$ in the expression above. It is possible to derive simple formula's for these terms based on: 

   * adjusted phenotypic observations for the quantitative trait of related individuals
   * heritability of the quantitative trait
   * knowledge of inheritance laws and genetic relationship information (eg parents, grandparents, siblings) for individuals with phenotypic observations of the quantitative trait

We will distinguish between true and estimated breeding value using the following notation:
\begin{align}
a &= \text{additive genetic value = breeding value} \notag \\
\hat{a} &= E(a|y) = \text{estimated additive genetic value = estimated breeding value} \notag \\
\end{align}

### Estimated breeding values are unbiased {-}
Below we show that $\hat{a}$ is an unbiased estimator of $a$.The expected value ($E(\hat{a})$) of the predicted breeding value $\hat{a}$ can be computed
as:
\begin{align}
E(\hat{a}) &= E(Cov(a,y)[Var(y)]^{-1}(y-E(y))) \notag \\
           &= Cov(a,y)[Var(y)]^{-1}E((y-E(y))) \notag \\
           &= Cov(a,y)[Var(y)]^{-1}(E(y)-E(y))=0 \notag
\end{align}
Because we have already specified that $E(\hat{a})=0$, it follows that $E(\hat{a})=E(a)=0$. This means that $\hat{a}$ is an unbiased estimator of $a$.

### Variance of estimated breeding value ($\hat{a}$) {-}
The variance $Var(\hat{a})$ is the same as the covariance $Cov(a,\hat{a})$ between the true
and predicted breeding value:
\begin{align}
Var(\hat{a}) &= Var(Cov(a,y)[Var(y)]^{-1}(y-E(y))) \notag \\
           &= Cov(a,y)[Var(y)]^{-1}Var((y-E(y)))[Var(y)]^{-1}Cov(a,y) \notag \\
           &= Cov(a,y)[Var(y)]^{-1}Var(y)[Var(y)]^{-1}Cov(a,y) \notag \\
           &= Cov(a,y)[Var(y)]^{-1}Cov(a,y)\notag \\
           \notag \\ 
Cov(a,\hat{a}) &= Cov(a,Cov(a,y)[Var(y)]^{-1}(y-E(y))) \notag \\
           &= Cov(a,y)[Var(y)]^{-1}Cov(a,(y-E(y))) \notag \\
           &= Cov(a,y)[Var(y)]^{-1}Cov(a,y) \notag \\
           &= Cov(a,y)[Var(y)]^{-1}Cov(a,y)\notag \\
           &= Var(\hat{a})\notag 
\end{align}

### Conditional density of estimated breeding values ($\hat{a}$) {-}

In some cases, e.g., for specifying confidence intervals of true breeding values,
it might be interesting to have a look at the conditional density $f(u|\hat{a})$. This
density is a multivariate normal density with expected value $E(u|\hat{a})$ and variance
$Var(u|\hat{a})$. These values can be computed based on the theory of conditional
multivariate normal densities.
\begin{align}
E(a|\hat{a}) &= E(u) + Cov(a,\hat{a})[Var(\hat{a})]^{-1}(\hat{a}-E(\hat{a})) \notag \\
           &= 0 + Var(\hat{a})[Var(\hat{a})]^{-1}(\hat{a}-0) \notag \\
           &= \hat{a} \notag \\
Var(a|\hat{a}) &= Var(u) - Cov(a,\hat{a})[Var(\hat{a})]^{-1}Cov(a,\hat{a}) \notag \\
Var(a|\hat{a}) &= Var(u) (1- Cov(a,\hat{a})^2Var(a)^{-1}Var(\hat{a})^{-1}) \notag \\
Var(a|\hat{a}) &= Var(u) (1-r_{a,\hat{a}}^2) \notag
\end{align}

### Prediction error variance (PEV) of estimated breeding values {-}
Because every prediction is associated with an error, the same is true for the predicted breeding values $\hat{a}$. The variability of the error for the predicted breeding values are quantified by the prediction error variance (PEV). This is computed as:

\begin{align}
Var(a-\hat{a}) &= Var(a) - 2Cov(a,\hat{a}) + Var(\hat{a})= Var(a-\hat{a})\notag \\
               &= Var(a) (1 - Var(\hat{a})Var(a)^{-1})\notag \\
               &= Var(a) (1 - r_{a,\hat{a}}^2)\notag
\end{align}

The standard error of prediction (SEP) can be a useful quantity. SEP corresponds just to the square root of PEV. Hence
\begin{align}
SEP(\hat{a}) &= \sqrt{Var(a-\hat{a})} \notag \\
               &= \sqrt{Var(a) (1 - r_{a,\hat{a}}^2))} \notag \\
               &= \sigma_a \sqrt{(1 - r_{a,\hat{a}}^2))} \notag
\end{align}

### Accuracy of breeding value estimates ($r_{a,\hat{a}}$) {-}

Estimated breeding values ($\hat{a}$) are estimates of the true breeding values ($a$), which cannot be observed directly. It is important to determine how well we have estimated the breeding value in relation to the true breeding value. This can be done using accuracy or reliability. Accuracy is the correlation between the estimated and the true breeding value: 
\begin{align}
			r_{a,\hat{a}} &= \frac{\Cov(a,\hat{a})}{\sqrt{\Var(a) \Var(\hat{a})}} \notag
\end{align}
Reliability is the squared correlation, $r_{a,\hat{a}}^2$, between the estimated breeding value and the true breeding value. To be able to compute the accuray or reliability of the estimated breeding value ($E(a|y)$) we need to determine the values for the terms, $Cov(a,\hat{a})$, $Var(\hat{a})$, and $Var(a)$ in the expression above.

A high correlation means that the estimated breeding value is very accurate.
Reliability ($r_{a,\hat{a}}^2$) can be interpreted as part of the genetic variation that we have explained by the estimated breeding vlaues whereas the remainder (1-$r_{a,\hat{a}}^2$) is uncertainty. 

The variance of estimated breeding value $\hat{a}$ can be computed as:
\begin{align}
			\sigma^2_{\hat{a}}=r_{a,\hat{a}}^2\sigma^2_{a}
\end{align}
and from this expression it is clear that when the reliability increases, the variation in $\hat{a}$ increases, but the estimation becomes more precise. If the reliability is 0, then we know nothing and the variance of $\hat{a}$ is 0. If the reliability is 1, then we know everything and the variance of $\hat{a}$ is $\sigma^2_{\hat{a}}=\sigma^2_{a}$ and the error variance (uncertainty) is 0. Reliability of the breeding value ($r_{a,\hat{a}}^2$) is important because it determines how well we can predict an individual's genetic value. It can be used to control the risk of a breeding plan: for example, low $r_{a,\hat{a}}^2$ leads to greater "risk" for both lower and higher true breeding value and we might consider more phenotypic records. Lastly reliability is one of the crucial factors that determines the genetic progress (see breeders equation).


## Estimation of breeding value based on own phenotype:
We will illustrate the basic principles of breeding value estimation using a simple example where the trait has been measured on the individuals themselves, i.e. on the individuals to be genetically evaluated. 
An estimate of the breeding value (a) based on own phenotype (y) can be calculated as: 
\begin{align}
E(a|y) &= E(a) + Cov(a,y)[Var(y)]^{-1}(y-E(y)) \notag \\
E(a|y) &= 0 + \sigma^2_{a}[\sigma^2_{a} + \sigma^2_{e}]^{-1}(y- \mu) \notag \\
E(a|y) &= h^2(y-\mu) \notag
\end{align}

The expression for expected value terms ($E(a)$ and $E(y)$ in the equation above are based on rules for expected value of a sum of (normally distributed) random variables:
\begin{align}
\E(a) &=0 \notag \\
\E(e) &=0 \notag \\
\E(y) &=\E(\mu+a+e) \notag \\
      &=\E(\mu)+\E(a)+\E(e)  \notag \\
      &=\mu+0+0  \notag \\
      &=\mu  \notag 
\end{align}

The expression for (co)variance terms ($Var(y)$, and $Cov(a,y)$ ) in the equation above are based on rules for the variance of a sum of (normally distributed) random variables:
\begin{align}
\Var(y) &= Var(a) + Var(e) + 2Cov(a,e)  \notag \\
\Var(a) &=\sigma^2_{a}  \notag \\
\Var(e) &=\sigma^2_{e}  \notag \\
\Cov(a,e) &=0  \notag \\
\Var(y) &= \sigma^2_{a} + \sigma^2_{e} \notag \\ 
Cov(a,y) &= Cov(a,a+e) \notag \\
         &= Cov(a,a) + Cov(a,e) \notag \\
         &= \sigma^2_{a} + 0 \notag \\
         &= \sigma^2_{a}  \notag
\end{align}

Thus estimated breeding value bases on own phenotypic record can be computed based on estimate of the trait heritability ($h^2$) and observed phenotype deviation ($y-\mu$). Use of records on the candidate itself is called performance testing. For performance testing to be efficient, the heritability should be at least moderately high (this can be derived from this equation: $E(a|y) = h^2(y-\mu)$).


## Accuracy of estimated breeding value based on own phenotype:
The accuracy for the breeding based on own phenotype (y) can be calculated as:
\begin{align}
			r_{a,\hat{a}} &= \frac{\Cov(a,\hat{a})}{\sqrt{\Var(a)} \sqrt{\Var(\hat{a})}} \notag \\
			r_{a,\hat{a}} &= \frac{(h^2 )^2 \sigma_y^2}{\sqrt{h^2 \sigma_y^2}\sqrt{(h^2)^2 \sigma_y^2}} \notag \\
			r_{a,\hat{a}} &= \sqrt{h^2} \notag
\end{align}

The variance of the estimated breeding value, $Var(\hat{a})$, can be expressed as:
\begin{align}
		Var(\hat{a}) &=Var(h^2 (y-\mu)) \notag \\
		Var(\hat{a}) &=(h^2)^2 Var(y-\mu)=(h^2)^2 Var(y)=(h^2)^2 \sigma_y^2 \notag
\end{align}

The variance of the true breeding value, $Var(a)$, can be expressed by the heritability and phenotypic variance:
\begin{align}
		\sigma_a^2=(\sigma_a^2)/(\sigma_y^2 ) \sigma_y^2=h^2 \sigma_y^2 \notag
\end{align}

The covariance between the true (a) and estimated breeding value, ($Cov(a,\hat{a})$), can be expressed as:
\begin{align}
	Cov(a,\hat{a}) &= \sigma_{a,\hat{a}} \notag \\					
	Cov(a,\hat{a}) &=Cov(a,h^2 (y-\mu)  )=h^2 Cov(a,(y-\mu)) \notag \\
	Cov(a,\hat{a}) &=h^2 Cov(a,(\mu+a+e-\mu)) \notag \\
	Cov(a,\hat{a}) &=h^2 Cov(a,(a+e),a) \notag \\
	Cov(a,\hat{a}) &=h^2 Cov(a,a)+h^2 Cov(a,e) \notag \\
	Cov(a,\hat{a}) &=h^2 \sigma_a^2+0 \notag \\
	Cov(a,\hat{a}) &=h^2 h^2 \sigma_y^2=(h^2 )^2 \sigma_y^2 \notag
\end{align}



## Estimation of breeding value and accuracy based on phenotypes of close relatives:

In practice we often use phenotypic records from close relatives, such as progenies, half-sibs, fullsibs, parents and grandparents. Information on the individual itself, i.e. the candidate to be evaluated for selection, is commonly used, when the trait in question can be measured on the individual (directly or indirectly). Sometimes this is not possible, e.g., traits that are sex-limited (e.g. milk production, female fertility) cannot be measured in male individuals. Traits like carcass composition and meat quality cannot be measured on live animals, unless an indirect method can be used (e.g. ultra-sonic measurement of carcass composition). 
In this situation it might be possible to use phenotypic information on relatives. Phenotypes collected from close relatives (as compared to distant relatives) provide more information about the breeding value of an individual (as close relatives share more DNA in common). In the following we will provide a general formula for estimating breeding values and their accuracies using different types of phenotypic information. 

#### General formula for estimating breeding values using different sources of information: {-}
\begin{align}
\hat{a}&=b_{a|y}(y-\mu) \notag \\
\end{align}

where the regression coefficient quantifies the weight (or importances) of the source of information is determined:
\begin{align}
b_{a|y}&=\frac{a'nh^2}{(1+(n-1)r} \notag
\end{align}
where $a'$ is the genetic relationship between the breeding individual and individuals with phenotypes, $n$ is the number of phenotypic records, $h^2$ is the trait heritability, and $r$ is correlation between individuals with observations (r = a''h2  + c2, where a'' = genetic relationship between individuals with records and target, c2 = common environmental component).

Thus the importance given to a specific source of information depends on the additive genetic relationship with the breeding candidate, the heritability of the trait, and the amount of information, i.e. the number of progenies or sibs, etc. 

#### General formula for reliability of estimated breeding value using different sources of information: {-}
\begin{align}
			r_{a,\hat{a}}^2=\frac{(a')^2nh^2}{1+(n-1)r}
\end{align}

Thus reliability depends on the same factors as the estimated breeding value except for the phenotypic value. Although the reliability depends on the number of registrations it does not depend on the numerical value of the phenotype. From this formula is it clear that higher reliability (and accuracy) can be achived when:

 * genetic relationship to individuals with information (a') is high
 * many records (n)
 * high heritability ($h^2$)
 * correlation between individuals with observations ($r = a''h^2 + c^2$) is low 


### Genetic relationship used for calculating breeding value: {-}
Related individuals share genes and thus information about the significance of the genes for the phenotype the importance of the information depends on the degree of additive genetic relationship. Consider a simple parent offspring example. The offspring get half of the genes from each parent and therefore the breeding value for the offspring is the average of the parents' breeding values plus the Mendelian deviation:
\begin{align}
		a_{\text{offspring}}=\frac{1}{2}a_{\text{father}}+\frac{1}{2}a_{\text{mother}}+a_{\text{mendelian}} \notag 	
\end{align}
	(a = additive genetic value = breeding value)

The term $a_{mendelian}$ is necessary, because two fullsibs $i$ and $j$ both having parents $father$ and $mother$ receive different random samples of the set of parental alleles. Hence the breeding values $a_i$ and $a_j$ of fullsibs $i$ and $j$ are not going to be the same.  

In this equation the $\frac{1}{2}$ refers to the additive genetic relationship which in this example indicates that the offspring receives half of its genes from its parent. 

In general the weight given to a specific source of information depends on the additive genetic relationship with the candidate. Examples of different types of additive genetic relationships can be found in the table below. 

Table X. The additive genetic relationship ($A_{ij}$) between the various sources (j) and the individual itself, i.e. the candidate to be evaluated, the proband (i).

\begin{center} 
\begin{tabular}{|c|c|}
  \hline
  Relative  &  $A_{ij}$\\
  \hline
  Self  &  1.0    \\
  \hline
  Unrelated  &  0    \\
  \hline
  Mother  &  0.5 \\
  \hline
  Father  &  0.5 \\
  \hline
  Grandparent  &  0.25 \\
  \hline
  Halfsib  &  0.25 \\
  \hline
  Fullsib  &  0.5 \\
  \hline
  Cousin  &  0.0625 \\
  \hline
  Progeny  &  0.5 \\
  \hline
  Twin(MZ/Z)  &  1/0.5 \\
  \hline
\end{tabular}
\end{center}



### Estimation of breeding values using phenotypic information from multiple sources {-}

Several factors influence which sources of information to use when estimating 
breeding values for a trait: what information is available, the heritability of the 
trait, and how and on what individuals the trait can be measured. Therefore in genetic 
evaluation in practice it is common to combine information from several sources. 
As already mentioned, all information available is usually utilized when an animal’s breeding value is predicted. The weight given to a specific source of information depends on the additive genetic relationship with the candidate, the heritability and the amount of information, i.e. the number of progenies or sibs, etc. 


 * Using phenotypic records on progenies is generally the most accurate source 
of information for genetic evaluation (high genetic relationship $a'$ and high $n$). The average phenotypic value of a progeny group gives a good indication of the additive genetic effect (i.e. the 
breeding value) of the candidate. The value of the information increases with 
the size of the progeny group. 
Progeny testing is useful also when the heritability is low, and can be used 
even for traits with a heritability below 0.1, assuming the candidate has a 
large number of progenies (~100-150). The disadvantage is that it takes time 
before results on progenies are available. 

 * Phenotypic records on the candidate’s sibs, half sibs and full sibs, are often 
used in addition to other information, or to give supplementary information, 
for example on traits that cannot be measured on the candidate itself. The accuracy of sib testing depends on the number of sibs that have records. Full 
sibs are usually raised in the same herd, they have a common environmental 
effect. This may cause a bias when they are used for prediction of breeding 
values, unless we are able to adjust for it. 

 * Information on pedigree (parents, grandparents) is generally available even 
before the candidate is born, and can thus give very early information. However, 
the genes from each locus of the parents are transmitted at random, so information 
based on pedigree alone is not very accurate, but can be valuable 
as additional information. The additive genetic relationship, and thus the proportion of common genes between the candidate and the pedigree, is halved for every generation backwards. If the breeding values of the parents are well known there is little to gain in using information on grandparents (actually, if 
the parents true breeding values were known, there would be nothing gained 
in using grandparental information). 

As already mentioned, all information available is usually utilized when an animal’s breeding value is predicted. The weight given to a specific source of information depends on the additive genetic relationship with the candidate, the heritability and the amount of information, i.e. the number of progenies or sibs, etc. In the following sections we will show how breeding values can be calculated when different types of information is available. 



## Selection index combining multiple sources of information for breeding value estimation (skip this section)
In general it is better to use all information available when estimating the breeding value. 
Before the introduction of the BLUP method ((Henderson, 1973b) and
(Henderson, 1975)), breeding values were estimated using selection index
theory ((Hazel, 1943) and (Hazel and Lush, 1942)). Both methods (selection
index and BLUP) are based on the same genetic model and provide a way to combine information from multiple sources. The main difference between the two methods consists in the way how they correct for identifiable
systematic environmental effects. We start with a treatment of selection index
theory. The selection index method combines information from different information sources to calculate a breeding value. 

In practice the selection index method is very seldom used for estimating breeding values as better alternatives exist (i.e. BLUP method presented below). However the selection index theory provides a simple way to calculate the precision (accuracy) of selection before you set up a breeding program. This is very useful for comparing alternative strategies. Here we will present a brief introduction to selection index theory.

A selection index for calculating breeding value is formulated as a linear combination of the individual information sources:
\begin{align}
			\hat{a} &= b_1p_1 + b_2p_2 + ... + b_mp_n = Pb \notag
\end{align}

Where $b_i$ are optimal weights and $p_i$ are sources of information (eg own phenotype, parental phenotype, offspring average phenotype):
\begin{align}
			p_1 &= y_1 - \mu_{1}		\qquad   (\text{e.g., own phenotype}) \notag \\
			p_2 &= y_2 - \mu_{2} 	\qquad   (\text{e.g., mother’s phenotype}) \notag
\end{align}

\begin{align}
		y_1 &= \mu+a_1+e_1		\qquad   (\text{e.g., own phenotype}) \notag \\
		y_2 &= \mu+a_2+e_2		\qquad   (\text{e.g., mother’s phenotype}) \notag
\end{align}

The optimal weights, b, are determined using calculation rules for random variables and matrix algebra:
\begin{align}
		b &= [Var(P)]^{-1}cov(a,P) \notag \\
		b &= V^{-1}G \notag
\end{align}

where $V$ is a matrix of (co)variances between the different phenotype sources 
\begin{align}
		V &= \begin{bmatrix}
  Var(p_1) & cov(p_1,p_2)\\
  cov(p_2,p_1) & Var(p_2)
  \end{bmatrix} \notag \\
\end{align}

and $G$ is a matrix of (co)variances between the different phenotype sources 
\begin{align}
		G &= \begin{bmatrix}
  cov(a,p_1) \\
  cov(a,p_2)
  \end{bmatrix} \notag
\end{align}

the covariance between the source of information and the true breeding value:$cov(a,p_i)$ 
the variance of the individual information sources; $var(p_i)$
the covariance between the different sources of information:$cov(p_i,p_j)$

Two variants of selection index.Index that combines information from different sources about the same trait the weights ($b_i$) are determined by genetic parameters ($h^2$), number of observation (n) and genetic relationship ($a$). Index that combines information from different traits the weights  ($b_i$) may also bring information about different traits in the breeding goal.
Describe accuracy of selection index method

and $G$ is a matrix of (co)variances between the different phenotype sources 
\begin{align}
			r_{a,\hat{a}} &= \frac{\Cov(a,\hat{a})}{\sqrt{\Var(a) \Var(\hat{a})}} \notag
\end{align}

\begin{align}
			\Cov(a,\hat{a}) &= \Cov(a,\hat{a}) \notag \\
			                &= \Cov(a,Pb) \notag \\
			                &= b\Cov(a,P) \notag \\
			                &= bG \notag \\
			\Var(a) &= \Cov(a,\hat{a}) \notag \\
			\Var(\hat{a}) &= \Var(Pb) \notag \\
			              &= b'\Var(P)b \notag \\
\end{align}

As you could see above, the selection index theory does consider a number of 
factors, such as heritabilities, phenotypic variances and economic weights of 
traits, genetic and phenotypic correlations between traits, family size and type, 
and influence of common environment. It is important to remember, though, that 
the selection index theory per se does not take into account any systematic effects 
that may have influenced the phenotypic records. Any adjustment required needs 
to be done in a separate step. As pointed out earlier, the phenotypic records (Xi) 
that are included in the index (I) must be the adjusted values (expressed as deviations). 



# BLUP a general approach for estimation of breeding values

Breeding value estimation in practical animal and plant breeding programs are nowadays based the BLUP (Best Linear Unbiased Prediction) method. The prediction of breeding values requires to correct the information sources for an appropriate comparison value. So far we have referred to that comparison value as the population mean and we have assumed this correction value $\mu$ to be known. Because, we defined the true breeding value $a$ and the non-identifiable environmental effects $e$ as deviations from a common mean, the average effect of all identifiable environmental components is captured by the population mean $\mu$. But this is only true in an idealized population where all selection candidates are kept in the same environment and where they deliver their performances at the same time. In practice the phenotypic records often need to be adjusted for systematic (fixed) effects, such as age, parity, litter size, days open, sex, herd, year, season, management, etc. Several of those effects fluctuate very little over time, so accurate estimates of their effect may be obtained from previous (“historical”) sets of data. Effects of factors like herd, year, season, and management fluctuate more and are therefore best estimated directly from the data to be used in the genetic evaluations.

The solution to this was presented by Charles R. Henderson in several publications (Henderson1973a) and [@Henderson1975]). The key idea behind the solution is to estimate the identifiable environmental factors as fixed effects and to predict the breeding values as random effects simultaneously in a linear mixed effects model. The properties of the methodology developed by Henderson are similar to those of the selection index method. But the main advantage of Henderson's methodologies is that phenotypic records do not need to be corrected before breeding values can be predicted. But the effects of the identifiable environmental factors are also a result which come out of the analysis. The methodology developed by Henderson is called __BLUP__ and the properties of this methodology are directly incorporated into the name where 

* __B__ stands for __best__ which means that the correlation between the true ($a$) and the predicted breeding value ($\hat{a}$) is maximal or the prediction error variance ($var(a - \hat{a})$) is minimal.
* __L__ stands for __linear__ which means the predicted breeding values are linear functions of the observations ($y$)
* __U__ stands for __unbiased__ which means that the expected values of the predicted breeding values are equal to the true breeding values
* __P__ stands for __prediction__ 

BLUP based approaches have found widespread usage in genetic evaluations. They are used for both traditional predictions of breeding values and also for predicting genomic breeding values. The popularity of BLUP is not only due to the theoretical foundations behind BLUP, but Henderson has also developed efficient algorithms to be able to compute predicted breeding values for very large livestock breeding populations. The theoretic foundations, the development of efficient algorithms together with the availability of large computational resources at a very low price have made BLUP to become the de-facto standard methodology for predicting breeding values.


## Linear Mixed Effects Models {#mixedlineareffectsmodel}
A simple linear model contains fixed effects such as _herd_ or _sex_ of an animal and tries to explain the observations as linear functions of such effects. Because the effects considered in a model cannot account for all influences of a given set of observations, every model must have a random residual component. If a linear model contains besides the residuals any additional random effects, then this model is called a __mixed linear effects model__. 


### Numeric Example {#blupnumericexample}
We want to use a concrete numeric example of a small population to explain how breeding values are predicted using the BLUP methodology. The phenotypic observations consist of measurements of the trait __weaning weight__ in beef cattle. Table \@ref(tab:TableBeefExample) gives an overview of the dataset.

```{r TableBeefExample, echo=FALSE, results='asis', warning=FALSE, message=FALSE}
### # fix the numbers parents and offspring
n_nr_sire <- 3
n_nr_dam <- 8
n_nr_parents <- n_nr_sire + n_nr_dam
n_nr_offspring <- 16
n_nr_animals <- n_nr_parents + n_nr_offspring
### # assign parents to offspring and herds to records
vec_sire_id <- c(rep(1,8), rep(2,6), rep(3,2))
vec_dam_id <- rep(4:11,2)
vec_herd_codes <- c(rep(1,4), rep(2,4), rep(1,4), rep(2,4))
### # vector of observations
vec_weaning_weight <-  c(2.61,2.31,2.44,2.41,2.51,2.55,2.14,2.61,2.34,1.99,3.1,2.81,2.14,2.41,2.54,3.16)

### # create a tibble from the data
tbl_beef_data <- tibble::tibble( Animal = (n_nr_parents + 1):n_nr_animals,
                                    Sire   = vec_sire_id,
                                    Dam    = vec_dam_id[order(vec_dam_id)],
                                    Herd   = vec_herd_codes,
                                    `Weaning Weight` = vec_weaning_weight )
### # count number of observations
n_nr_observation <- nrow(tbl_beef_data)

### # parameters
h2 <- .25
n_var_p <- round(var(tbl_beef_data$`Weaning Weight`), digits = 4)
n_var_g <- round(h2 * n_var_p, digits = 4)
n_pop_mean <- round(mean(tbl_beef_data$`Weaning Weight`), digits = 2)

### # show the data frame
knitr::kable( tbl_beef_data, 
              format = "latex",
              booktabs = TRUE, 
              longtable = TRUE,
              caption = "Example Data Set for Weaning Weight in Beef Cattle" )
```

We assume the phenotypic variance ($\sigma_p^2$) to be `r n_var_p` and the heritability $(h^2)$ corresponds to `r h2`. 




#### Fixed Versus Random Effects {#fixedversusrandomeffects}
Unfortunately, there is no unique and generally accepted definition of which effects should be fixed and which should be random. There are generally accepted guidelines of how to classify effects as fixed or as random. Table \@ref(tab:fixedversusrandom) lists a few criteria that might be helpful.

\small

```{r fixedversusrandom, echo=FALSE, results='asis', message=FALSE, warning=FALSE}
tbl_fixed_versus_random <- tibble::data_frame(`fixed effect` = c("classes can be defined exactly",
                                                                 "the value of a class does not have an apriori expected value",
                                                                 "values are exactly estimable",
                                                                 "the expected value of a class effect is of primary interest",
                                                                 "fixed effects can be corrected for"),
                                              `random effects` = c("realized value come from an underlying distribution",
                                                                   "each realization is unique",
                                                                   "observations are influenced by the variance of the random effect",
                                                                   "main interest is on the variance not on the expected value", 
                                                                   ""))
suppressPackageStartupMessages( library(dplyr) ) 
knitr::kable(tbl_fixed_versus_random, 
             format = "latex",
             booktabs = TRUE, 
             longtable = TRUE,
             caption = "Classification Factors of Fixed and Random Effects") %>%
  kableExtra::kable_styling(full_width = F)  %>%
  kableExtra::column_spec(1, width = "22em") %>%
  kableExtra::column_spec(2, width = "22em")
```

\normalsize

Certain factors such as herd, sex, breed or feeding regimes can be classified unambiguously as fixed effects. On the other hand breeding values are always random effects. Because, we know that breeding values have an expected value of $0$ and have a certain variance, they must be modeled as random effects where these properties can be integrated into the model. Furthermore, each animal has a different realization of a breeding value. Exceptions are mono-clonal twins and clones.

From a practical point of view, the software program that is used to analyse the data has also an influence on whether a certain effect is treated as fixed or as random. If a certain effect has very many levels such as herds, then it is sometimes better for the analysis to treat such an effect as random. 


#### Model Specification {#lmemodelspecification}
In a linear mixed effects model a single observation $y_{ijk}$ is decomposed according to equation \@ref(eq:linearmixedeffectmodelsingleobservation)

\begin{equation}
y_{ijk} = \beta_i + u_j + e_{ijk}
(\#eq:linearmixedeffectmodelsingleobservation)
\end{equation}

where $\beta_i$ stands for the $i-^{th}$ level of a fixed effect, $u_j$ is the $j-{th}$ realization of the random effect $u$ and $e_{ijk}$  is the residual effect of the $k-^{th}$ observation}.  Because, we do not want to model just one observation, but we want to include all observations of a complete population, it is helpful to convert the model in \@ref(eq:linearmixedeffectmodelsingleobservation) into matrix-vector notation. This is shown in equation \@ref(eq:linearmixedeffectmodelmatrixvector)

\begin{equation}
y = X\beta + Zu + e
(\#eq:linearmixedeffectmodelmatrixvector)
\end{equation}

\begin{tabular}{llp{10cm}}
where  &  &  \\
       &  $y$      &  vector of length $n$ of all observations \\
       &  $\beta$  &  vector of length $p$ of all fixed effects  \\
       &  $X$      &  $n \times p$ design matrix linking the fixed effects to the observations \\
       &  $u$      &  vector of length $n_u$ of random effects \\
       &  $Z$      &  $n \times n_u$ design matrix linking random effect to the observations \\
       &  $e$      &  vector of length $n$ of random residual effects.  
\end{tabular}

Furthermore, we assume the following relations for the expected values and for the variances. As already mentioned the random effects are defined as deviations and hence their expected value is set to zero. 

\begin{equation}
E(u) = 0 \qquad \text{and} \qquad E(e) = 0
(\#eq:expectedvaluerandomeffects)
\end{equation}

From this it follows that $E(y) = X\beta$. The variance-covariance matrices for the random effects are set to 

\begin{equation}
var(u) = G \qquad \text{and} \qquad var(e) = R
(\#eq:variancerandomeffects)
\end{equation}

Under the assumption that $cov(u,e^T) = 0$, we can compute $var(y) = Z * var(u) * Z^T + var(e) = ZGZ^T + R = V$. 

In model \@ref(eq:linearmixedeffectmodelmatrixvector) the vectors $\beta$ and $u$ are unknown. The solution of the model \@ref(eq:linearmixedeffectmodelmatrixvector) for the unknowns $\beta$ and $u$ leads to estimates $\hat{\beta}$ for the fixed effects $\beta$ and for predicted random effects $\hat{u}$. Unlike with the selection index, with BLUP, we do not have to correct the observations before predicting random effects.


#### The Solution
An outline of how to derive the BLUP solutions for $\hat{\beta}$ and $\hat{u}$ will be given in an Appendix. The details of this derivation are not important. Therefore, we are presenting here directly the result which are

\begin{equation}
\hat{u} = GZ^TV^{-1}(y - X\hat{\beta})
(\#eq:hatublup)
\end{equation}

We call $\hat{u}$ the best linear unbiased prediction of $u$ or shorter $\hat{u} = BLUP(u)$. For $\hat{\beta}$, we insert the generalized least squares estimator (GLS) which corresponds to 

\begin{equation}
\hat{\beta} = (X^T V^{-1} X)^- X^T V^{-1} y
(\#eq:hatbetahatblue)
\end{equation}

The matrix $(X^T V^{-1} X)^-$ denotes the generalized inverse of the matrix $(X^T V^{-1} X)$. The generalized inverse $K^-$ can be replaced with the simple inverse $K^{-1}$, whenever the columns of matrix $K$ are linearly independent^[For our examples that are shown here, we can always use the simple inverse.]. Analogously to $\hat{u}$, $\hat{\beta}$ is called the best linear unbiased estimator of the fixed effects $\beta$. In short, we can state $\hat{\beta} = BLUE(\beta)$. 


### Mixed Model Equations
The solutions shown in \@ref(eq:hatublup) for $\hat{u}$ and in \@ref(eq:hatbetahatblue) for $\hat{\beta}$ are not suitable for practical purposes. Both solutions contain the inverse $V^{-1}$ of matrix $V$. The matrix $V$ corresponds to the variance-covariance matrix of all observations $y$. The inverse matrix $V^{-1}$ is not easy to compute and furthermore procedures to invert general matrices are computationally expensive and are prone to rounding errors. In one of his many papers, Henderson has shown that the results for $\hat{u}$ and $\hat{\beta}$ are the same when solving the following system of equations simultaneously.

\begin{equation}
\left[
  \begin{array}{lr}
  X^T R^{-1} X  &  X^T R^{-1} Z \\
  Z^T R^{-1} X  &  Z^T R^{-1} Z + G^{-1}
  \end{array}
\right]
\left[
  \begin{array}{c}
  \hat{\beta} \\
  \hat{u}
  \end{array}
\right]
=
\left[
  \begin{array}{c}
  X^T R^{-1} y \\
  Z^T R^{-1} y
  \end{array}
\right]
(\#eq:mixedmodeleq)
\end{equation}

The above shown equations are called __mixed model equations__ (MME). They do no longer contain the inverse $V^{1}$ and hence these MME are much simpler to solve. The MME contain the inverses $R^{-1}$ and $G^{-1}$, but we will see with concrete examples that they are much easier to invert. As a consequence, whenever we have to predict breeding values using BLUP, we will use the mixed model equations shown in \@ref(eq:mixedmodeleq).


### Sire Model
The application of the linear mixed effects model from \@ref(eq:linearmixedeffectmodelmatrixvector) to the numerical example in table \@ref(tab:TableBeefExample). As random effects $u$ we are taking the father $s$ of each animal $i$ with an observation. As fixed effects $\beta$ we are using the herd effect. When fathers are modeled as random effects, then we call this model a __sire model__. Setting up a sire model for the data in table \@ref(tab:TableBeefExample) looks as follows

```{r siremodelbeefexample, echo=FALSE, results='asis'}
mat_x_sire <- matrix(data = c(1, 0,
                              1, 0,
                              1, 0,
                              1, 0,
                              0, 1,
                              0, 1,
                              0, 1,
                              0, 1,
                              1, 0,
                              1, 0,
                              1, 0,
                              1, 0,
                              0, 1,
                              0, 1,
                              0, 1,
                              0, 1), ncol = 2, byrow = TRUE)
vec_betahat_sire <- c("\\beta_1", "\\beta_2")
mat_z_sire <- matrix(data = c(1, 0, 0,
                              1, 0, 0,
                              1, 0, 0,
                              1, 0, 0,
                              1, 0, 0,
                              1, 0, 0,
                              1, 0, 0,
                              1, 0, 0,
                              0, 1, 0,
                              0, 1, 0,
                              0, 1, 0,
                              0, 1, 0,
                              0, 1, 0,
                              0, 1, 0,
                              0, 0, 1,
                              0, 0, 1), ncol = 3, byrow = TRUE)
vec_sirehat_sire <- c("s_1", "s_2", "s_3")
vec_res_sire <- c("e_1", "e_2", "e_3", "e_4", "e_5", "e_6", "e_7", "e_8", "e_9", "e_{10}", "e_{11}", "e_{12}", "e_{13}", "e_{14}", "e_{15}", "e_{16}")
cat("\\begin{equation}\n")
cat("= \n")
cat("+ \n")
cat("+ \n")
cat("\\end{equation}\n")
```

Besides the equation for the sire model we also have to specify the expected values and the variances of all random components. To be able to distinguish the sire model from the general linear mixed effects model, we usually call the random sire effect $s$ and no longer $u$. The expected values for the random variables were already stated when discussing the general linear mixed effects model in section \@ref(lmemodelspecification). Hence 

\begin{equation}
E(s) = 0 \qquad \text{and} \qquad E(e) = 0 \qquad \rightarrow \qquad E(y) = X\beta
(\#eq:exvaluerandvarsire)
\end{equation}

For the variances there are a few simplifications that we can use in our sire model. The covariance between the random effects $s$ and $e$ are assumed to be $0$. The covariances among the single residual effects are also assumed to be $0$. Hence, the variance-covariance matrix of the residual effects are $var(e) = I * \sigma_e^2$. The variance of the sire effects $s$ is 

$$
var(s) = A_s * \sigma_s^2 = G
$$

where $A_s$ is the additive genetic relationship matrix between the sires. We will be deriving the matrix $A_s$ in a later chapter. Because our sires are not related, we can say that $A_s = I$ and hence

$$
G = I * {\sigma_u^2 \over 4}
$$

Now we are ready to set up the mixed model equations from \@ref(eq:mixedmodeleq) for the sire model. The computation of the numerical solutions from the mixed model equations will be the topic of an exercise.



### Animal Model {#animalmodel}
The mixed model equations are a universal tool to find BLUPs of random effects and BLUEs of fixed effect simultaneously. On the other hand it is not satisfactory that with the sire model only sires obtain predicted breeding values. All information that is known about the mothers was completely ignored when we specified the sire model. A better approach would be to combine all available information from a given population. This can be done by replacing in the sire model the random sire effects by random animals effects. As a result each animal in the dataset receives a random effect which models its breeding value. This type of model is called an __animal model__. Because the animal model has the breeding values of all animals as random effects, they are often referred to with the variable or the vector $a$^[This is not the same as the genotypic value in a single locus model.] and no longer $s$ as in the sire model. The variance-covariance matrix ($var(a)$) between all animal effects is proportional to the additive genetic relationship matrix $A$ among all animals. We will see in a later chapter how to compute the matrix $A$.



## BLUP breeding values are useful for ranking and selection {-}
BLUP breeding values, especially from the animal model including relationships, 
are useful tools in selection. Selection on BLUP breeding values maximizes the 
probability for correct ranking of breeding animals and selection on them maximizes genetic gain from one generation to another. There are many factors that 
contribute to this: 

 * The animal model makes full use of information from all relatives, which increases accuracy (precision). 
 * The breeding values are adjusted for systematic environmental effects in an optimal way. This means that animals can also be compared across herds, age classes etc, assuming the data is connected. 
 * The procedure is flexible, various practical situations can be handled. 
 * Non-random mating can be accounted for. 
 * Several traits can be included 
 * Bias due to culling (e.g., between 1st and 2nd lactations) and selection (over generations) is accounted for, assuming that also non-selected animals’ data are included in the analysis. 
  
It should, however, be noted that the genetic evaluation is based on phenotypic 
observations, and that regardless of how splendid the BLUP procedure may be, it 
cannot compensate for bad data. So a good recording is necessary for a reliable 
genetic evaluation and subsequent genetic gain. It should also not be forgotten 
that BLUP (as well as selection index) assumes that the genetic parameters used 
are the true ones. In practice that means that they should be close to the true parameters. 
Something that should be noticed is the potential risk for increased inbreeding
when selection is based on breeding values including information on all relatives. 
The probability that several family members are selected jointly is increased, 
which may result in increased inbreeding. To avoid this, and to optimize longterm selection response, selection on BLUP breeding values might be combined 
with some restriction on average relationship of the selected animals. 
A useful side effect of BLUP genetic evaluation is that it gives estimates of the 
realized genetic trend. This is achieved by comparing BLUP breeding values of 
animals born in different years, assuming there are connections between years 
through successive time overlapping or through relationships. 


\begin{comment}

# Linear Models {#intro-predict-breeding-value}
Because, in the more traditional setting^[That means, at this moment, we are ignoring all recent developments made such as genomic selection.] of livestock breeding, we do not have information about allele frequencies and about genotypic values, we have to predict breeding values. For this prediction we can use different sources of information. Currently, we are assuming that this information is all based on records of phenotypic observations. 


### Model Specification {#basic-model}
Although, the phenotypic observation might originate from different sources, we can use one basic model for all of the breeding value predictions. We have already seen a different form of this model in equation \@ref(eq:phengenenv) in section \@ref(geno-pheno).  The original model from equation \@ref(eq:phengenenv) is modified and extended to the model shown below.

\begin{equation}
y_{ij} = \mu_i + g_i + e_{ij}
(\#eq:breedvalpredbasicmodel)
\end{equation}

\begin{tabular}{llp{9cm}}
where  &  &  \\
       &  $y_{ij}$  &  $j^{th}$ record of animal $i$ \\
       &  $\mu_i$   &  identifiable fixed environmental effect \\
       &  $g_i$     &  sum of all additive ($u$), dominance ($d$) and epistatic effects of the genotype of animal $i$ \\
       &  $e_{ij}$  &  random environmental effect associated to observation $j$ of animal $i$
\end{tabular}

<!-- TODO: Improvement on the following paragraph -->
Livestock species are mostly diploid and hence from a given parent only one allele of a given locus is passed to a gamete which can later be found in the parents offspring. Any interaction effects caused by dominance or epistasis are not preserved from parent to offspring. Only the additive effect of a given allele is passed from parent to offspring. The additive genetic part ($u_i$) of $g_i$ in equation \@ref(eq:breedvalpredbasicmodel) represents the average genetic effect that animal $i$ receives from its parents. It is therefore called the __breeding value__. Because the additive genetic effect is a function of the alleles passed from the parents to the progeny, it is the only component that can be selected for and is therefore the main component of interest from a livestock breeding perspective. Due to the major interest in the genetic additive component, the terms in the basic model in \@ref(eq:breedvalpredbasicmodel) are re-arranged as follows.

\begin{equation}
y_{ij} = \mu_i + u_i + e_{ij}^*
(\#eq:breedvalpredmodifiedmodel)
\end{equation}

\begin{tabular}{llp{9cm}}
where  &  &  \\
       &  $y_{ij}$  &  $j^{th}$ record of animal $i$ \\
       &  $\mu_i$   &  identifiable fixed environmental effect \\
       &  $u_i$       &  sum of all additive ($u$) genetic effects of the genotype of animal $i$ \\
       &  $e_{ij}^*$  &  dominance, epistatic and random environmental effects of the $j^{th}$ record of animal $i$
\end{tabular}

The same re-arrangement of terms in the basic model is illustrated by Figure \@ref(fig:basicmodelrearrterm)

```{r basicmodelrearrterm, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", fig.cap="Re-arrangment of Terms Representing Genetic Effects"}
#rmddochelper::use_odg_graphic(ps_path = "odg/basicmodelrearrterm.odg")
knitr::include_graphics(path = "odg/basicmodelrearrterm.png")
```
  
Equation \@ref(eq:breedvalpredmodifiedmodel) constitutes the linear model that forms the basis for most problems of breeding value prediction in livestock breeding. Usually it is assumed that the phenotypic observations $y_{ij}$ follow a multivariate normal distribution. We have already seen in section \@ref(genetic-models) that the additive genetic effect ($u_i$) is thought to be the sum of a large number of unlinked loci that all contribute a very small amount to the total breeding value. Then by the central limit theorem it follows that $u_i$ converges to a normal distribution. By the same reasoning that the environmental effect $e_{ij}^*$ is composed of very many small contributions, also $e_{ij}^*$ converges to a normal distribution. From distribution theory it is known that the sum of two normally distributed random variables (like $u_i$ and $e_{ij}^*$) plus a fixed term (like $\mu$) is again a random variable that follows a normal distribution. We can conclude that the assumption that all the random effects ($y_{ij}$, $u_i$ and $e_{ij}^*$) in model \@ref(eq:breedvalpredmodifiedmodel) is consistent with distribution theory. Furthermore the central limit theorem implies that in principle the number of breeding values from single loci tends to infinity. That means the total breeding value $u_i$ corresponds to a sum of infinitely many contributions. Based on the fact that in theory $u_i$ is composed of an infinite number of infinitely small components, the model in \@ref(eq:breedvalpredmodifiedmodel) is called the __infinitesimal model__. 

Concerning the variances, it is assumed that $var(y_{ij})$, $var(u_i)$ and $var(e_{ij})$ are all known. Covariances ($cov(u_i, e_{ij})$) between genetic and environmental effects and covariances ($cov(e_{ij}^*, e_{kl}^*)$) between environmental effects of mates $i$ and $k$ are assumed to be zero, respectively. 

Also $\mu_i$ which is used to represent the mean performance of animals in the same identifiable environment such as herd or management group or have the same sex or age, is assumed to be known.


### Decomposition of Breeding Value
As already mentioned earlier, the breeding value $u_i$ of an individual $i$ represents the average additive genetic effect that animal $i$ receives from its parents $s$ and $d$. Hence $u_i$ can be decomposed into 

\begin{equation}
u_i = {1\over 2} u_s + {1\over 2} u_d + m_i
(\#eq:breedingvalueanimalparent)
\end{equation}

where $u_s$ and $u_d$ correspond to the breeding values of parents $s$ and $d$, respectively and $m_i$ is the deviation of $u_i$ from the average breeding values of the parents and is called __Mendelian sampling__. The term $m_i$ is necessary, because two fullsibs $i$ and $k$ both having parents $s$ and $d$ receive different random samples of the set of parental alleles. Hence the breeding values $u_i$ and $u_k$ of fullsibs $i$ and $k$ are not going to be the same. The difference between breeding values $u_i$ and $u_k$ is reflected in the different Mendelian sampling terms $m_i$ and $m_k$ for fullsibs $i$ and $k$. 


## Basic Principle of Predicting Breeding Values {#principle-predic-breedingvalue}
The prediction of breeding values mostly follows the same principles. From the point of view of statistics, estimations or predictions are always a function of the observed data. When looking at the model in \@ref(eq:breedvalpredmodifiedmodel), we can probably guess that the observed phenotypic records ($y_{ij}$) must be corrected somehow for the identifiable environmental effects represented by $\mu_i$. The second influence that we want to consider when predicting breeding values is how "closely related" the observed record $y_{ij}$ is to the breeding value. For traits where the influence of the genetic component is not very strong, it is probably a good idea to down-weigh the information from $y_{ij}$. 

The two principles just described can be generalized as follows. Breeding values are predicted according to the following two steps. 

1. Observations are corrected for the mean performance values of animals under the same environmental conditions. The conditions are described by the effects captured in $\mu_i$. 
2. The corrected observations are weighted by a factor that reflects the amount of information that is available for the prediction of an animals breeding value.

In what follows, we have a look at how breeding values are predicted from different sources of information.



## Animal's Own Performance {#own-performance}
### Single Record {#single-record}
When one phenotypic observation per animal is the only information we have available, the predictor $\hat{u_i}$ of the breeding value $u_i$ of animal $i$ can be derived according to the following line of argument. Let us assume for a moment that we know the true breeding value $u_i$ for a population of animals. In addition to that each animal $i$ has one observation $y_i$ available. Then we plot the values of $u_i$ against the values of $y_i$ for the complete population. 

```{r regbreedingvaluesinglerecord, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", fig.cap="Regression of Breeding Values onto Phenotypic Observations"}
#rmddochelper::use_odg_graphic(ps_path = "odg/regbreedingvaluesinglerecord.odg")
knitr::include_graphics(path = "odg/regbreedingvaluesinglerecord.png")
```

The plot in Figure \@ref(fig:regbreedingvaluesinglerecord) suggests that we fit a regression of the breeding values onto the phenotypic records. The fitted regression is represented by the red line. Hence as soon as we can draw the regression line, we can predict breeding values based on the phenotypic observations. The predicted breeding value $\hat{u_i}$ for a given $y_i$ corresponds to the value on the red line corresponding to the value of $y_i$. The slope of the regression line corresponds to the regression coefficient $b$. From regression theory, the coefficient $b$ is computed as 

\begin{align}
b &= \frac{cov(u,y)}{var(y)} \notag \\
  &= \frac{cov(u,\mu + u + e)}{var(y)}  \notag \\
  &= \frac{cov(u,u)}{var(y)}  \notag \\
  &= \frac{var(u)}{var(y)}  \notag \\
  &= h^2
(\#eq:regcoeffaony)
\end{align}

where $h^2$ corresponds to the ratio between the genetic additive and the phenotypic variance and is called __heritability__. We are using the regression coefficient to predict the breeding value for animal $i$ based on a single record $y_i$. 

\begin{align}
\hat{u_i} &= b * (y_i - \mu) \notag \\
          &= h^2 * (y_i - \mu)
(\#eq:predbreedvalueownsinglerecord)
\end{align}

From that it follows that the predicted breeding value for an animal based on a single own performance record corresponds to the observation corrected for the general mean $\mu$ times the heritability. The correlation between the selection criterion, in our case the phenotypic record and the true breeding value is known as the accuracy of the prediction. It provides a means of evaluating the different selection criteria. The higher the correlation between selection criterion and breeding value, the better is the prediction. Sometimes the accuracy of evaluation is reported in terms of reliability ($r^2$) which corresponds to the squared correlation between selection criterion and true breeding value. With a single own performance record per animal, the correlation is 

\begin{align}
r_{u,y} &= \frac{cov(u, y)}{\sigma_u \ \sigma_y} \notag \\
        &= \frac{\sigma_u^2}{\sigma_u \ \sigma_y} \notag \\
        &= \frac{\sigma_u}{\sigma_y} \notag \\
        &= h
(\#eq:corownperftruebreedingvalue)
\end{align}

An alternative way to assess the quality of the breeding value prediction is to compute the variance of the predicted breeding values. 

\begin{align}
var(\hat{u_i}) &= var(by) = var(h^2y) \notag \\
               &= h^4 var(y) \notag \\
               &= r_{u,y}^2 h^2 \sigma_y^2 \notag \\
               &= r_{u,y}^2 \sigma_a^2
(\#eq:varpredictedbreedingvalue)
\end{align}

Hence the variance of the predicted breeding values corresponds to the product of the reliability times the genetic additive variance. The expected response ($R$) to selection on the basis of one record per animal is 

\begin{equation}
R = i * r_{u,y}^2 * \sigma_y = i * h^2 * \sigma_y
(\#eq:selectionresponseownperformance)
\end{equation}

where $i$, the selection intensity refers to the superiority of selected individuals above population mean expressed in phenotypic standard deviation.


### Repeated Records {#repeatedrecords}
When animals get older, it is likely that we can observe multiple measurements for the same trait. An example is milk yield in dairy cows where a cow might have repeated lactation records. The breeding value of an animal may be predicted based on the mean of the repeated records. With repeated records, an additional resemblance between the records of an animal due to permanent environmental factors occurs. The between-animal variance is partly genetic and partly caused by permanent environmental effects. The within-animal variance is attributed to differences between the successive measurements of the animal arising from temporary environmental variations, i.e., from environmental factors that change over time. The variance of observations ($var(y)$) can therefore be partitioned as 

\begin{equation}
var(y) = var(u) + var(pe) + var(te)
(\#eq:varrepeatedrecords)
\end{equation}

where $var(u)$ is the genetic additive variance, $var(pe)$ the variance due to permanent environmental effects and $var(te)$ the variance due to temporary environmental effects. 

The intra-class correlation $t$ is defined as the ratio of the genetic plus the permanent environmental variance divided by the phenotypic variance.

\begin{equation}
t = \frac{var(u) + var(pe)}{var(y)}
(\#eq:intraclasscorrelation)
\end{equation}

The term $t$ is also called __repeatability__ and it measures the correlation between the records of an individual. From \@ref(eq:intraclasscorrelation) it follows that 

\begin{equation}
1-t = \frac{var(te)}{var(y)}
(\#eq:oneminusintraclasscorrelation)
\end{equation}

With this model, it is assumed that the repeated records on the animal are the same trait. Therefore the genetic correlation between all pairs of records is one. We also assume that all records have equal variance and that the environmental correlations between all pairs of records are equal. Let $\tilde{y}$ represent the mean of $n$ records on animal $i$ which means

\begin{align}
\tilde{y_i} &= {1\over n} \sum_{k=1}^n y_{ik} \notag \\
            &= {1\over n} \sum_{k=1}^n ( \mu + u_i + pe_i + te_{ik}) \notag \\
            &= \mu + u_i + pe_i + \sum_{k=1}^n te_{ik}
(\#eq:meanrepeatedrecords)
\end{align}

In this case, we use the mean ($\tilde{y_i}$) to predict the breeding value ($\hat{u_i}$)

\begin{equation}
\hat{u_i} = b(\tilde{y_i} - \mu)
(\#eq:predbreedingvaluerepeatedrecords)
\end{equation}

where 

\begin{equation}
b = \frac{cov(u,\tilde{y})}{var(\tilde{y})}
(\#eq:regcoeffrepeatedrecords)
\end{equation}

The single elements are computed as

\begin{equation}
cov(u,\tilde{y}) = cov(u, \mu + u + pe + {1\over n} \sum_{k=1}^n te_k) = var(u) = \sigma_u^2
(\#eq:covayrepeatedrecords)
\end{equation}

and 

\begin{equation}
var(\tilde{y}) = var(u) + var(pe) + {1\over n} var(te)
(\#eq:varyrepeatedrecords)
\end{equation}

Expressing \@ref(eq:varyrepeatedrecords) in terms of \@ref(eq:intraclasscorrelation) and \@ref(eq:oneminusintraclasscorrelation) leads to 

\begin{align}
var(\tilde{y}) &= t * \sigma_y^2 + {1\over n} (1-t) * \sigma_y^2 \notag \\
               &= {1\over n}\left( n*t + (1-t) \right) \sigma_y^2 \notag \\
               &= \frac{1 + (n-1)t}{n} \sigma_y^2
(\#eq:varyastrepeatedrecords)
\end{align}

Inserting this into \@ref(eq:regcoeffrepeatedrecords) results in 

\begin{align}
b &= \frac{cov(u,\tilde{y})}{var(\tilde{y})} \notag \\
  &= \frac{n \sigma_u^2}{(1 + (n-1)t) \sigma_y^2} \notag \\
  &= \frac{nh^2}{1 + (n-1)t}
(\#eq:regcoeffrepeatedrecordsresult)
\end{align}

When we predict the breeding value $u_i$ of animal $i$ using repeated records, the regression coefficient $b$ depends on 

1. the heritability ($h^2$) 
2. the repeatability ($t$) and 
3. the number ($n$) of repeated records per animal


The difference between repeated records of an animal is assumed to be due to temporary environmental differences between successive performances. However, if successive records are known to be affected by factors which influence performance, these must be corrected for. For instance, differences in age at calving in first and second lactations may influence milk yield in first and second lactation. Such age differences should be adjusted for before using the means of both lactations for breeding value prediction.

The accuracies of the predicted breeding value using repeated records is 

\begin{align}
r_{u,\tilde{y}} &= \frac{cov(u,\tilde{y}) }{\sigma_u \sigma_y} \notag \\
                &= \frac{\sigma_u^2}{\sigma_u \sqrt{(1 + (n-1)t)/n \sigma_y^2}} \notag \\
                &= h \sqrt{n/(1 + (n-1)t)} \notag \\
                &= \sqrt{nh^2/(1 + (n-1)t)} \notag \\
                &= \sqrt{b}
(\#eq:accrepeatedrecords)
\end{align}

The expected response to selection using repeated records will be covered in an exercise.


## Progeny Records {#progenyrecords}
For traits that are recorded only on female animals, the prediction of breeding values for male animals (sires) is usually based on the mean of their female progeny. This is typical in dairy cattle, where bulls are evaluated on the basis of their daughters. Let $\bar{y_i}$ be the mean of single records of $n$ daughters of sire $i$ with the assumption that the daughters are only related through the sire (paternal half-sibs), the predicted breeding value of sire i can then be computed as 

\begin{equation}
\hat{u_i} = b * (\bar{y_i} - \mu)
(\#eq:predictedbreedingvalueprogeny)
\end{equation}

where 

\begin{equation}
b = \frac{cov(u, \bar{y})}{var(\bar{y})}
(\#eq:regressioncoefficientprogeny)
\end{equation}

and 

\begin{align}
\bar{y} &= {1 \over n} \sum_{k=1}^n y_k \notag \\
        &= {1 \over n} \sum_{k=1}^n (\mu + u_k + e_k) \notag \\
        &= \mu + {1 \over n} \sum_{k=1}^n (u_k + e_k) \notag \\
        &= \mu + {1 \over n} \sum_{k=1}^n (1/2 u_s + 1/2 u_{dk} + m_k + e_k) \notag \\
        &= \mu + 1/2 u_s + {1 \over n} \sum_{k=1}^n (1/2 u_{dk} + m_k + e_k) \notag \\
        &= \mu + 1/2 u_s + {1 \over n} \sum_{k=1}^n 1/2 u_{dk} + {1 \over n} \sum_{k=1}^n e_k
\end{align}


In the current case of using progeny records to predict a breeding value, we have 

\begin{align}
cov(u, \bar{y}) &= cov(u, {1\over 2}u_s + {1\over 2}{1\over n}\sum_{k=1}^n u_{d,k} + {1\over n}\sum_{k=1}^n m_k + {1\over n}\sum_{k=1}^n e_k) \notag \\
                &= cov(u, {1\over 2}u_s) \notag \\
                &= {1\over 2} cov(u, u_s) \notag \\
                &= {1\over 2} \sigma_u^2
\end{align}

where $u_s$ and $u_{d,k}$ denote the breeding values of sire $s$ and dam $d$ of offspring $k$, respectively and $m_k$ and $e_k$ stand for the mendelian sampling and the environmental effect of daughter $k$. Using the same principles as in section \@ref(repeatedrecords), we get

\begin{equation}
var(\bar{y}) = (t + (1-t)/n) \sigma_y^2
(\#eq:variancedaughtermean)
\end{equation}

where $\sigma_y^2 = var(u) + var(e) = \sigma_u^2 + \sigma_e^2$.

Assuming there is no environmental covariance between half-sib records and the intra-class correlation $t$ is $\frac{1/4 \sigma_u^2}{\sigma_y^2}$. Then we can compute the regression coefficient as 

<!-- ---------------------------------------------------- --
  -- TODO: Explain how $var(\bar{y})$ is derived          --
  --                                                      --
  -- ---------------------------------------------------- -->

\begin{align}
b &= \frac{1/2 \sigma_u^2}{(t + (1-t)/n) \sigma_y^2} \notag \\
  &= \frac{1/2 h^2 \sigma_y^2}{({1\over 4}h^2 + (1 - {1\over 4}h^2)/n) \sigma_y^2} \notag \\
  &= \frac{2nh^2}{nh^2 + (4-h^2)} \notag \\
  &= \frac{2n}{n + (4-h^2)/h^2} \notag \\
  &= \frac{2n}{n+k}
(\#eq:regcoeffprogeny)
\end{align}

with $k=\frac{4-h^2}{h^2}$. 

The term $k$ is constant for any assumed heritability ($h^2$). The regression coefficient ($b$) depends on the heritability and number of progeny and converges towards a limit of $2$ as the number of daughters increases.

The accuracy of the estimated breeding value is

\begin{align}
r_{u, \bar{y}} &= \frac{cov(u, \bar{y})}{\sqrt{var(u) var(\bar{y})}} \notag \\
               &= \frac{1/2 h^2 \sigma_y^2}{\sqrt{h^2 \sigma_y^2 ({1\over 4}h^2 + (1 - {1\over 4}h^2)/n) \sigma_y^2}} \notag \\
               &= \frac{1/2 h}{\sqrt{{1\over 4}h^2 + (1 - {1\over 4}h^2)/n}} \notag \\
               &= \sqrt{\frac{nh^2}{nh^2 + (4-h^2)}} \notag \\
               &= \sqrt{\frac{n}{n + k}}
(\#eq:accuracyprogeny)
\end{align}

The term for $r_{u, \bar{y}}$ in \@ref(eq:accuracyprogeny) approaches $1$ as the number of progeny increases, assuming $k$ is constant. The reliability ($r_{u, \bar{y}}^2$) of the predicted breeding value is $n/(n+k)$ and corresponds to $1/2 * b$ computed in \@ref(eq:regcoeffprogeny). 


\end{comment}

<!--chapter:end:Estimation-of-Breeding-Values.Rmd-->

---
title: "Estimation of Genetic Parameters"
author: "Peter Sørensen,....."
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: yes
    includes:
      in_header: preamble.tex
  html_document:
    number_sections: yes
    includes:
      in_header: mathjax_header.html
  word_document: default
biblio-style: apalike
link-citations: yes
bibliography: lbgfs2021.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(19)
```



## Learning objective:  {-}    
This section introduces the basic concepts of estimating genetic parameters such as: 

   * basic principles of estimating genetic parameters
   * use of genetic relationships for estimating genetic parameters
   * different methods, data sources and experimental designs for estimating genetic parameters
   * importance of estimation of genetic parameters in breeding
   * knowing when estimation of genetic parameters may be required 
  

# Introduction
The estimation of genetic parameters is an important issue in animal and plant breeding. First of 
all, estimating additive genetic and possible non-additive genetic variances contributes 
to a better understanding of the genetic mechanism. Secondly, estimates of genetic and 
phenotypic variances and covariances are essential for the prediction of breeding values 
and for the prediction of the expected genetic response of 
selection programmes. Parameters that are of interest are heritability, genetic and 
phenotypic correlation and repeatability, and those are computed as functions of the 
variance components.

Genetic parameters are estimated using information on phenotypes and genetic relationships 
for individuals in the breeding population. Heritability is estimated by comparing individual 
phenotypic variation among related individuals in a population. Close (compared to distant) relatives 
share more DNA in common and if the trait is under genetic influence they will therefore share 
phenotypic similarities. In this section we will illustrate how different phenotypic sources 
and genetic relationships are used for estimating genetic parameters.

## Genetic model
As introduced previously the phenotype for a quantitative trait is the sum of both genetic and environmental factors. In general the total genetic effect for an individual is the sum of both additive and non-additive effects. However, only the additive genetic effects are passed on to the offspring and therefore have a breeding value. Therefore we only consider the additive genetic model as the basis for estimation of genetic parameters. The model for the phenotype ($y$) is:
\begin{align}
			y=\mu+a+e	 \notag
\end{align}
where  $\mu$ is the population mean, $a$ is the additive effect, and $e$ is the environmental deviation (or residual) not explained by the genetic effects in the model. 
We assume that the additive genetic effect, a, and the residual term, e, are normally distributed which means that the observed phenotype is also normally distributed
\begin{align}
a \sim N(0,\sigma^2_{a}), \quad
e \sim N(0,\sigma^2_{e}), \quad
y \sim N( \mu,\sigma^2_{y}) \notag
\end{align}
where $\sigma^2_{a}$ is additive genetic variance, $\sigma^2_{e}$ is residual variance, and ($\sigma^2_{y}$) is the total phenotypic variance. 


## Genetic parameters
Heritability and genetic correlation are the key genetic parameters used in animal and plant breeding. They are defined in terms of the variance component ($\sigma^2_{a}$ and $\sigma^2_{e}$) defined in the previous section.    
    
__Heritability__ estimates the degree of variation in a phenotypic trait in a population that is due to genetic variation between individuals in that population. It measures how much of the variation of a trait can be attributed to variation of genetic factors, as opposed to variation of environmental factors. The narrow sense heritability is the ratio of additive genetic variance ($\sigma^2_{a}$) to the overall phenotypic variance ($\sigma^2_{y}=\sigma^2_{a}+\sigma^2_{e}$):
\begin{align}
h^2 &= \sigma^2_{a}/(\sigma^2_a+\sigma^2_e)
\end{align}
A heritability of 0 implies that the no genetic effects influence the trait, while a heritability of 1 implies that all of the variation in the trait is explained by the genetic effects. In general the amount of information provided by the phenotype about the breeding value is determined by the narrow sense heritability. 

__Genetic correlation__ is the proportion of variance that two traits share due to genetic causes. Genetic correlations are not the same as heritability, as it is about the overlap between the two sets of influences and not their absolute magnitude; two traits could be both highly heritable but not be genetically correlated or have small heritabilities and be completely correlated (as long as the heritabilities are non-zero).
Genetic correlation ($\rho_a$) is the genetic covariance between two traits divided by the product of genetic standard deviation for each of the traits:
\begin{align}
\rho_{a_{12}}=\frac{\sigma_{a_{12}}}{\sqrt{\sigma_{a_{1}}^2 \sigma_{a_{2}}^2}}
\end{align}
where $\sigma_{a_{12}}$ is the genetic covariance and  $\sigma_{a_{1}}^2$ and $\sigma_{a_{2}}^2$ are the variances of the additive genetic values for the two traits in the population. 
A genetic correlation of 0 implies that the genetic effects on one trait are independent of the other, while a correlation of 1 implies that all of the genetic influences on the two traits are identical. 
Thus in order to estimate the heritability and genetic correlation we need to estimate the variance component defined above. 


## Data required for estimating genetic parameters
Information on phenotypes and genetic relationships for individuals in the breeding population 
are used to formulate appropriate statistical models for the analysis of the data
and accurate estimation of genetic parameters and breeding values of individuals.

__Phenotypes__ for traits of economic importance need to be recorded accurately and completely. All individuals
within a production unit (herd, flock, ranch, plot) should be recorded. Individuals should not be
selectively recorded. Data includes the dates of events when traits are observed, factors
that could influence an individual’s performance, and an identification of contemporaries that
are raised and observed in the same environment under the same management regime.
Observations should be objectively measured, if at all possible.

__Genetic relationships__ for the individuals in the breeding population is required. Genetic relationships can be inferred from a pedigree or alternative alternative from genetic markers. Individuals and their parents need to be uniquely identified in the data. Information about birth dates, breed composition, and genotypes for various markers or QTLs could also be stored. If individuals are not uniquely identified, then genetic change of the population may not be
possible. In aquaculture species, for example, individual identification may not be feasible,
but family identification (father and mother) may be known.

Furthermore knowledge and understanding of the production system is 
important for designing optimum selection and mating strategies. For dairy cattle key elements are the
gestation length and the age at first breeding. The number of offspring per female per
gestation will influence family structure. The use of artificial insemination and/or embryo
transfer could be important. Other management practices are also useful to know. 

Prior information about the traits is useful. Read the literature. Most likely other researchers or 
breeders have already made analyses of the same species and traits. Their models could be useful 
starting points for further analyses. Their parameter estimates could predict the kinds of 
results that might be found. The idea is to avoid the pitfalls and problems that other 
researchers have already encountered. Be aware of new kinds of analyses of the same data, 
that may not involve statistical models.


## Statistical models and variance components
For estimating genetic parameters we need to specify a model that describes the genetic and non-genetic factors that may affect the trait phenotypes. Often the non-genetic factors are referred to as systematic effect 
such as age, parity, litter size, days open, sex, herd, year, season, management, etc.

\begin{align}
			\text{mean}=\text{mean} + \text{systematic effect} + \text{genetic effect}  + residual	 \notag
\end{align}

In this, we make a distinction between fixed effects, that determine the level (expected means) of observations, and 
random effects that determine variance. A model at least exists of one fixed (mean) 
and one random effect (residual error variance). If observations also are influenced by a 
genetic contribution of the individuals, then a genetic variance component exists as well. In 
that situation, we have two components contributing to the total variance of the 
observations: a genetic and a residual variance component. 

A statistical model is usually specified as a mathematical relationship between one or more random variables and other non-random variables. In quantitative genetics it is often based on the additive genetic model specified above with inclusion of additional factors that may affect the trait of interest: 

\begin{align}
			y=\mu + \text{...systematic effect....} + a + e	 \notag
\end{align}

where  $\mu$ is the population mean, $a$ is the additive effect, and $e$ is the environmental deviation (or residual) not explained by the systematic effects and the genetic effects in the model. We assume that the additive genetic effect, a, and the residual term, e, are normally distributed such that $a \sim N(0,\sigma^2_{a})$ and $e \sim N(0,\sigma^2_{e})$. The goal of the statistical analysis is to derive estimates for the variance components that is the additive genetic variance $\sigma^2_{a}$, and the residual variance $\sigma^2_{e}$ the residual variance. 

The statistical model is a formal representation of our quantitative genetic theory, but it is important to realize that all models are simple approximations to how factors influence a trait. The goal of the statistical analysis is to find the best practical model that explains the most variation in the data. Statistical knowledge is required. The methods used for estimating genetic parameters is based on statistical concepts such as random variables, multivariate normal theory and linear (mixed) models. These concepts and their use will be explained in the following sections. 



# Methods for estimation of genetic parameters
In general estimation of heritability and genetic correlation is based on methods that determine resemblance 
between genetically related individuals. Here we will present three methods for estimating heritability, parent-offspring regression, analysis of variance (ANOVA) for family data (e.g. halfsib/fullsib families) and restricted maximum likelihood (REML) analysis for general pedigree. 

## Estimating heritability using parent - offspring regression
The simplest method for estimation genetic parameters is based on regression analysis. 
Heritability may be estimated by comparing phenotypes for traits recorded in parent and offspring. Parent-offspring regression compares trait values in parents ($y_p$) to trait values in their offspring ($y_o$). Estimation of heritability is based on a linear regression model:
\begin{align}
			y_o &= y_p b_{o|p}+e_o. \notag \\
\end{align}
The slope of the regression line ($b_{o|p}$) approximates the heritability of the trait when offspring values are regressed against the average trait in the parents. If only one parent's value is used then heritability is twice the slope. In other words the expected value of the regression line is $b_{o|p} = 0.5h^2$ (or h2 when 
regression is on mid-parent mean). 

To better understand this relationship consider a situation 
where we have collected phenotypes on a number of father-offspring families. 
From standard regression theory the slope can be determined as: 
\begin{align}
			b_{o|f} &= \frac{Cov(y_f,y_o)}{Var(y_o)} \notag \\
\end{align}
where $Cov(y_f,y_o)$ is the covariance between the phenotypes of the father and the offspring and $Var(y_o)$ is the variance of the offspring phenotypes. 

The phenotypes for the father ($y_f$) and the offspring ($y_o$) can be expressed as:
\begin{align}
			y_f &= \mu+a_f+e_f \notag \\
			y_o &= \mu+0.5a_m+0.5a_f+a_{mendelian}+e_o \notag
\end{align}
where $\mu$ is the population mean, $a_m$ and $a_f$ are the additive genetic effect for the mother and the father, a_{mendelian} is the mendelian deviation in the offspring, and $e_f$ and $e_o$ are the residual effect for the father and the offspring.

The offspring get half of the genes from each parent and therefore the breeding value for the offspring is the average of the parents' breeding values plus the Mendelian deviation:
\begin{align}
		a_{\text{offspring}}=\frac{1}{2}a_{\text{father}}+\frac{1}{2}a_{\text{mother}}+a_{\text{mendelian}} \notag 	
\end{align}
	(a = additive genetic value = breeding value)
The term $a_{mendelian}$ is necessary, because two fullsibs $i$ and $j$ both having parents $father$ and $mother$ receive different random samples of the set of parental alleles. Hence the breeding values $a_i$ and $a_j$ of halfsibs $i$ and $j$ are not going to be the same. 
Furthermore we assume that the breeding values are normally distributed:
\begin{align}
a_{father} \sim N(0,\sigma^2_{a}) \notag \\
a_{mother} \sim N(0,\sigma^2_{a}) \notag \\
a_{mendelian} \sim N(0,0.5\sigma^2_{a}) \notag \\
\end{align}

Therefor an expression for the covariance between the phenotypes of the parent and the offspring can be derived as:  
\begin{align}
			Cov(y_f,y_o) &= Cov(a_f+e_f,0.5a_m+0.5a_f+a_{mendelian}+e_o) \notag \\
			             &= Cov(a_f,0.5a_f) \notag \\
			             &= 0.5Cov(a_f,a_f) \notag \\
			             &= 0.5\sigma_a^2 \notag 
\end{align}
and similar for the variance variance of the offspring phenotypes:  
\begin{align}
			Var(y_o) &= Var(0.5a_m+0.5a_f+0.5a_{mendelian}+e_o) \notag \\
			         &= Var(0.5a_m) + Var(0.5a_f) + Var(a_{mendelian}) + Var(e_o) \notag \\
			         &= 0.25Var(a_m) + 0.25Var(a_f) + Var(a_{mendelian}) + Var(e_o) \notag \\
			         &= 0.25\sigma_a^2 + 0.25\sigma_a^2 + 0.5\sigma_a^2 + \sigma_e^2 \notag \\
			         &= \sigma_a^2 + \sigma_e^2 \notag
\end{align}
Therefore the expected value of the regression coefficient for a father-offspring analysis is:
\begin{align}
			b_{o|f} &= \frac{0.5\sigma_a^2}{\sigma_a^2 + \sigma_e^2} \notag \\
			        &= 0.5\frac{\sigma_a^2}{\sigma_a^2 + \sigma_e^2} \notag \\
			        &= 0.5h^2 \notag \\
			h^2     &= 2b_{o|f}\notag
\end{align}

Similar relationships can be derived for other types of parent-offspring regression analysis (mother-offspring and mid- parent-offspring). The heritability can be estimated from the regression coefficient based on:  
\begin{align}
    h^2 &= 2b_{o|m} \quad \text{(mother-offspring regression)} \notag \\
    h^2 &= 2b_{o|f} \quad \text{(father-offspring regression)} \notag \\
    h^2 &=  b_{o|mf} \quad \text{(mean parent-offspring regression)} \notag
\end{align}

Offspring-parent regression is not often used in practice. 
It requires data on 2 generations, and uses only this data. It is based on the genetic relationship between parent and offspring which equals 0.5 (i.e. offspring get half of the genes form its parent), but it is not possible to 
utilize genetic relationships among parents. However, the method is robust against 
selection of parents. 


## Estimating heritability using ANOVA for family data
Genetic parameters have been estimated for many years using analysis of variance 
(ANOVA). This method require that individuals can be assigned to groups with the 
same degree of genetic relationship for all members. Family structures considered most often 
are paternal half-sib groups or full-sib groups. In the case of paternal half-sib group all 
offspring of one sire are treated as one group and offspring of different sires are 
allocated to different groups.

Estimation of heritability using ANOVA is based on a linear model. 
Consider a situation where we have phenotypic observation for multiple offspring for a number of families (halfsib or fullsib). 
A simple linear model for the phenotypic observation for the jth offspring in the ith famility include the population mean ($\mu$) and a family effect ($f_i$): 
\begin{align}
 y_{ij} &= \mu + f_i + e_{ij}
\end{align}

Assume n observations, with $n_f$ families, with $n_o$ is the number of offspring per family. 
 
The total sum of squares (SST) is the sum of each of the observations squared:
\begin{align}
 SST &= \sum_{i=1}^{n_f}\sum_{j=1}^{n_o} {y}_{ij}^2
\end{align}

where $y_{ij}$ is an observation on the jth offspring in the ith family. 

The mean sum of squares (SSM) is n times the mean squared: 
\begin{align}
 SSM &= n\bar{y}_{..}^2
\end{align}

The model sum of squares (SSA) due to a particular factor (e.g. the family effect) is therefore the sum over all observations of the estimated (family) effect in each observation squared (in balanced data this 
is the difference between the family group mean and the overall mean): 
\begin{align}
 SSA &= \sum_{i=1}^{n_f} (\bar{y}_{i.}-\bar{y}_{-.})^2
\end{align}
Notice that the sum of squares for the main effect (SSA) is the sum of all the squared 
estimates of $f_i$, because in a balanced data set the estimate of $f_i$ is equal to ($y_{i}.-y_{..}$). In a 
balanced data, it is rather simple to determine the expectations for each sum of squares, 
because the number of observations per class of f is constant ($n_o$). 


The residual sum of squares (SSE) due to the residual (error) is the sum over all 
observations of the residual effect in each observation squared (this is the 
difference between the observation and its group mean): 

\begin{align}
 SSE &= \sum_{i=1}^{n_f}\sum_{j=1}^{n_o} {y}_{ij}^2
\end{align}

The total sum of squares can be expressed as a sum of the components described above:  
\begin{align}
 SST &= SSM + SSA + SSE
\end{align}
 
In balanced data, it is rather simple to estimate variance components, by setting 
the "Mean Squares" equal to their expectations. Those expectations are linear functions 
of the variance components and is derived based on statistical theory (Expected value 
of a sum of squares). 
In this simple model we can calculate estimate of the residual variance components ($\hat{\sigma}^2_{e}$) as: 
\begin{align}
 \hat{\sigma}^2_{e} &= SSE/(n-n_f)
\end{align}
and and estimate of the family variance ($\hat{\sigma}^2_{f}$) as:
\begin{align}
 \hat{\sigma}^2_{f} &= (SSA/(n_f-1) - \hat{\sigma}^2_{e})/n_o
\end{align}

Calculating the variance between groups, involves partitioning the sum of squared 
observations (SS) due to different sources of variation in the model of analysis, groups 
of relatives being one of them, and equating the corresponding mean squares. Mean 
squares are derived as the SS divided by the associated degrees of freedom, to their 
expectations. 

Using ANOVA, the covariance among members of a family or group of relatives is 
usually determined as the variance component between groups. For example, in case of 
a half-sib family model, the variance between half-sib families $\sigma_{s}^2$ 
and variance within half-sib families $\sigma_{e}^2$. 
As shown earlier, the half-sib family variance $\sigma_{s}^2=0.25\sigma_{a}^2$
 while the variance within half-sib families is $0.75\sigma_{a}^2+\sigma_{e}^2$


Fullsib families
\begin{align}
			y_m &= \mu+a_m+e_m \notag \\
			y_f &= \mu+a_f+e_f \notag \\
			y_{o1} &= \mu+0.5a_m+0.5a_f+0.5a_{mendelian_1}+e_{o1} \notag \\
			y_{o2} &= \mu+0.5a_m+0.5a_f+0.5a_{mendelian_2}+e_{o2} \notag
\end{align}
\begin{align}
			Cov(y_{o1},y_{o2}) &= Cov(0.5a_m+0.5a_f+a_{mendelian1}+e_{o1},0.5a_m+0.5a_f+a_{mendelian2}+e_{o2}) \notag \\
			             &= Cov(0.5a_f,0.5a_f) + Cov(0.5a_m,0.5a_m) \notag \\
			             &= 0.25Cov(a_f,a_f)+0.25Cov(a_m,a_m) \notag \\
			             &= 0.25\sigma_a^2 +0.25\sigma_a^2 \notag \\
			             &= 0.5\sigma_a^2 \notag
\end{align}
Halfsib families
\begin{align}
			y_{m1} &= \mu+a_{m1}+e_{m1} \notag \\
			y_{m2} &= \mu+a_{m2}+e_{m2} \notag \\
			y_f &= \mu+a_f+e_f \notag \\
			y_{o1} &= \mu+0.5a_{m1}+0.5a_f+0.5a_{mendelian1}+e_{o1} \notag \\
			y_{o2} &= \mu+0.5a_{m2}+0.5a_f+0.5a_{mendelian2}+e_{o2} \notag
\end{align}
\begin{align}
			Cov(y_{o1},y_{o2}) &= Cov(0.5a_{m1}+0.5a_f+a_{mendelian1}+e_{o1},0.5a_{m2}+0.5a_f+a_{mendelian2}+e_{o2}) \notag \\
			             &= Cov(0.5a_f,0.5a_f) + Cov(0.5a_{m1},0.5a_{m2}) \notag \\
			             &= 0.25Cov(a_f,a_f) + 0 \notag \\
			             &= 0.25\sigma_a^2 \notag \\
			             &= 0.25\sigma_a^2 \notag
\end{align}

Therefore the heritability can be estimated from the variance component based on:  
\begin{align}
    \sigma_{hs}^2 &= 0.25\sigma_a^2 \quad \text{(halfsib families)} \notag \\
    \sigma_{fs}^2 &= 0.5\sigma_a^2 \quad \text{(halfsib families)} \notag \\
    h^2 &= \frac{4\sigma_{hs}^2}{4\sigma_{hs}^2+\sigma_e^2} \quad \text{(halfsib families)} \notag \\
    h^2 &= \frac{2\sigma_{fs}^2}{2\sigma_{fs}^2+\sigma_e^2} \quad \text{(fullsib families)} \notag \\
\end{align}

Data arising from experimental designs used for estimating genetic parameters 
are usually not balanced (i.e. number of offspring varies across families). 
Methods analogous to the ANOVA have been developed for unbalanced data.
However REML is nowadays the method of choice for variance component estimation.

The estimation of variance components (within and between family components). If 
the variation within families is large relative to differences between families, the trait 
must be lowly heritable. Variance components are attributed to specific effects. For 
example, the (paternal) half-sib variance is due to differences between sires. The 
variance component represents the sire variance, which is a quarter of the additive 
genetic variance.

Estimation of variance components is easier to generalise, and this method is generally 
used to estimate genetic parameters. The remainder of this chapter will therefore mostly deal with 
variance component estimation. 


## Estimating heritability using Restricted Maximum Likelihood
Genetic parameters are nowadays estimated using restricted maximum likelihood (REML) or Bayesian methods.
This method allow for estimation of genetic parameters using phenotypic information for individuals from a general pedigree.REML is based on linear mixed model methodology and use a likelyhood approach for estimating genetic parameters.

### Linear mixed model:
The linear mixed model contains the observation vector for the trait(s) of interest, the
factors that explain how the observations came to be, and a residual effect that includes
everything not explainable.

A matrix formulation of a general model equation is:
\begin{align}
y &= Xb + Za + e \notag
\end{align}

where
\begin{align}
y &: \text{is the vector of observed values of the trait,} \notag \\
b &: \text{is a vector of factors, collectively known as fixed effects,} \notag \\
a &: \text{is a vector of factors known as random effects,} \notag \\
e &: \text{is a vector of residual terms, also random,} \notag \\
X,Z &: \text{are known design matrices that relate the elements of b and a to their corresponding element in y.} \notag 
\end{align}

The __observation vector__ contains elements resulting from measurements, either subjective
or objective, on the experimental units (usually animals) under study. The elements in
the observation vector are random variables that have a multivariate distribution, and if
the form of the distribution is known, then advantage should be taken of that knowledge.
Usually $y$ is assumed to have a multivariate normal distribution, but that is not always
true. The elements of $y$ should represent random samples of observations from some defined
population. If the elements are not randomly sampled, then bias in the estimates of b and
$a$ can occur, which would lead to errors in ranking individuals.

A __continuous factor__ is one that has an infinite-like range of
possible values. For example, if the observation is the distance a rock can be thrown,
then a continuous factor would be the weight of the rock. If the observation is the
rate of growth, then a continuous factor would be the amount of feed eaten.

__Discrete__ factors usually have classes or levels such as age at calving might have four
levels (e.g. 20 to 24 months, 25 to 28 months, 29 to 32 months, and 33 months or
greater). An analysis of milk yields of cows would depend on the age levels of the
cows.

In the traditional ”frequentist” approach, __fixed__ and __random__ factors need
to be distinguished. 

If the number of levels of a factor is small or limited to a fixed number, then that
factor is usually fixed. 
If inferences about a factor are going to be limited to that set of levels, and to no
others, then that factor is usually fixed.
If a new sample of observations were made (a new experiment), and the same levels
of a factor are in both samples, then the factor is usually fixed.
If the levels of a factor were determined as a result of selection among possible
available levels, then that factor should probably be a fixed factor.
Regressions of a continuous factor are usually a fixed factor (but not always).

If the number of levels of a factor is large, then that factor can be a
random factor.
If the inferences about a factor are going to be made to an entire population of
conceptual levels, then that factor can be a random factor.
If the levels of a factor are a sample from an infinitely large population, then that
factor is usually random.
If a new sample of observations were made (a new experiment), and the levels were
completely different between the two samples, then the factors if usually random.

#### Expectation and variance of variables in the model: {-}
In the statistical model (specified above) the random effect ($a$ and $e$) and the phenotypes ($y$) are considered to be random variables that are assumed to follow a multivariate normal distribution.  
In general terms the expectations of these random variables are:  
\begin{align}
E(y) &= E(Xb) + E(Za) + E(e) \notag \\
     &= Xb + 0 + 0 \notag \\
     &= Xb \notag
\end{align}
and the variance-covariance matrices are:
\begin{align}
 Var(a) &= G \notag \\
 Var(e) &= R \notag \\
 Var(y) &= ZGZ' + R = V \notag \\
\end{align}
 
where $G$, $R$ and $V$ are square matrices of genetic, residual and phenotypic (co)variances among the individuals in the data set. If we assume  

\begin{align}
 Var(a) &= G \notag \\
        &= A\sigma_a^2 \notag \\
 Var(e) &= R \notag \\
        &= I\sigma_e^2 \notag \\
 Var(y) &= ZGZ' + R = V \notag \\
        &= A\sigma_a^2 + I\sigma_e^2 \notag
\end{align}


#### Assumptions and limitations of the model: {-}
The third part of a model includes items that are not apparent in parts 1 and 2. For
example, information about the manner in which data were sampled or collected. Were
the animals randomly selected or did they have to meet some minimum standards? Did
the data arise from many environments, at random, or were the environments specially
chosen? Examples will follow. A linear model is not complete unless all three parts of the model are present. 
Statistical procedures and strategies for data analysis are determined only after a complete
model is in place.


### Likelihood approach for estimating variance components
Restricted Maximum Likelihood is a method that is used to estimate the parameters in the model (i.e. variance components $\sigma_{a}^2$ and $\sigma_{e}^2$) specified in the linear mixed model above. The general principle used in likelihood methods is to find the set of parameters which maximizes the likelihood of the data. 

It is useful to recall that the likelihood ($L(\theta|{y})$) is any function of the parameter ($\theta$) that is proportional to $p({y}|\theta)$. Maximizing $L(\theta|{y})$ leads to obtaining the most likely value of $\theta$ ($\hat{\theta}$) given the data ${y}$. Usually the likelihood is expressed in terms of its logarithm ($l(\theta|\bf{y})$) as it makes the algebra easier. 

The likelihood of the data for a given linear mixed model can be written as a function;

\begin{equation} 
	l(\matr{V}|\vect{y}, \matr{X}, \bfbeta) 
	\propto
	 -\frac{1}{2}\ln|\matr{V}|
	 -\frac{1}{2}\ln|\matr{X'V}^{-1}\matr{X}|
	 -\frac{1}{2}(\vect{y}-\matr{X}{\bfbeta})'\matr{V}^{-1}(\vect{y}-\matr{X}{\bfbeta})
	\label{eq:reml.likelihood}
\end{equation}

From calculus we know that we can find the maximum of a function by taking the first 
derivative and set that equal to zero. Solving that would result in the desired 
parameters (assuming that we did not find the minimum, this can be checked using 
second derivatives). The first and second derivatives of the likelihood function 
are complicated formulas. 

There are no simple one-step solutions for estimating the variance components based on REML \citep{LynchWalsh1998}. Instead, we infer the partial derivatives of the likelihoods with respect to the variance components. The solutions to these involve the inverse of the variance-covariance matrix, which themselves includes the variance components, so the variance components estimates are non-linear functions of the variance components. It is therefore necessary to apply iterative methods to obtain the estimates.

From the estimate of the variance components the heritability can easily computed by 
\begin{align}
\hat{h}^2 &= \hat{\sigma}^2_{a}/(\hat{\sigma}^2_a+\hat{\sigma}^2_e)
\end{align}
where the "$\hat{h}^2$" refers to the heritability is an estimate. 


### Advantages of using REML for estimating genetic parameters
The REML method was developed by \citet{Patterson1971} as an improvement of the standard Maximum Likelihood (ML). The ML method was originally proposed by \citet{Fisher1922} but was introduced to variance components estimation by \citet{Hartley1967}. ML assumes that fixed effects are known without error which is in most cases false and, as consequence, it produces biased estimates of variance components (usually, the residual variance is biased downward). To solve this problem, REML estimators maximize only the part of the likelihood not depending on the fixed effects,
by assuming that the fixed effects have been, so to speak, fixed. This entails that when comparing multiple models by their REML likelihoods, they must contain the same fixed effects,
and that REML, by itself, does not estimate the fixed effects. 

REML does not produce unbiased estimates owing to the inability to return negative values of variance components of many methods to obtain REML estimators, but it is still the method of choice due to the fact that this source of bias is also present in ML estimates \citep{LynchWalsh1998}.

REML requires that y have a multivariate normal distribution 
although various authors have indicated that ML or REML estimators may be an 
appropriate choice even if normality does not hold (Meyer, 1990). 

REML can account for selection when the complete mixed model is used with 
all genetic relationships and all data used for selection included (Sorensen and Kennedy, 
1984; Van der Werf and De Boer, 1990). 

There is obviously an advantage in using (RE)ML methods that are more flexible in 
handling animal and plant breeding data on several (overlapping) generations (and possibly 
several random effects). However, the use of such methods has a danger in the sense 
that we need not to think explicitly anymore about data structure. To estimate, as an 
example, additive genetic variance, we need to have a data set that contains a certain 
family structure that allows us to separate differences between families from differences 
within families. Or in other words, we need to separate genetic and residual variance. 
ANOVA methods require more explicit knowledge about such structure, since the data 
has to be ordered according to family structures (e.g. by half sib groups). 

Developments in variance component estimation specific to animal and plant breeding have been 
closely linked with advances in the genetic evaluation using Best Linear 
Unbiased Prediction (BLUP). Early REML applications were generally limited to models 
largely equivalent to those in corresponding ANOVA type analysis, considering one 
random effect only and estimating genetic variances from paternal half sib covariances 
(so-called sire model). Today, heritability can be estimated from general pedigrees and from genomic relatedness estimated from genetic markers. Linear mixed models is also used genetic evaluation schemes, 
allowing information on all known relationships between individuals to be incorporated in 
the analysis. Linear mixed models can include maternal, permanent environmental, cytoplasmic or 
dominance effects or effects at QTL thereby more accurately describe the observed data. 
These effects are fitted as additional random effects. 


## When to estimate variance components?
In general, the estimation of variances and covariances has to be based on a sufficient 
amount of data. Depending on the data structure and the circumstances during 
measuring, estimations can be based on some hundreds (selection experiments) or more 
than 10,000 observations (field recorded data). It is obvious that we are not interested in 
estimating variance components from every data set. The information in literature is in 
many cases even better than estimations based on a small data set. In general, we have 
to estimate variance if: 
 - we are interested in a new trait, from which no parameters are available; 
- variances and covariances might have changed over time 
- considerable changes have occurred in a population e.g. due to recent 
importations. 
Mostly it is assumed that variances and covariances, and especially the ratio of both of 
them (like heritability, correlation), are based on particular biological rules, which do 
not rapidly change over time. However, it is well known that the genetic variance 
changes as consequence of selection. Changes are especially expected in situations with 
short generation intervals, high selection intensities or high degrees of inbreeding or in a 
situation in which a trait is determined by only a few genes. Secondly, the 
circumstances under which measurements are taken can change. If conditions are 
getting more uniform over time, the environmental variance decreases, and 
consequently the heritability increases. Thirdly, the biological interpretation of a trait 
can change as consequence of a changed environment; feed intake under limited 
feeding is not the same as feed intake under ad-lib feeding. In conclusion, there are 
sufficient reasons for regular estimation of (co-)variance components. 



\begin{comment}

# Variance Components {#variance-components}
The prediction of breeding values using a BLUP animal model required the __variance components__ $\sigma_e^2$ for the residual variance and $\sigma_u^2$ for the genetic additive variance to be known. For the sire model, $\sigma_u^2$ is replaced by the sire variance component $\sigma_s^2$. In real world livestock breeding evaluations, these variance components are not known and hence must be estimated from the data. The data analysis procedure that estimates the variance components from data is called __variance components estimation__. 


## Sire Model
The sire model is used to motivate the introduction of the topic of variance components estimation. The sire model is given by

\begin{equation}
y = X\beta + Z_ss + e
(\#eq:varcompsiremodel)
\end{equation}

with $var(e) = R$, $var(s) = A_s \sigma_s^2$ and $var(y) = Z_sA_sZ_s^T \sigma_s^2 + R$. The matrix $A_s$ is the numerator relationship for sires, the sire variance component $\sigma_s^2$ corresponds to $0.25 * \sigma_u^2$ and $R$ can often be simplified to $R = I * \sigma_e^2$. The interest in this chapter is how to estimate $\sigma_s^2$ and $\sigma_e^2$. 

In the simple case the vector $\beta$ is reduced to just one scalar fixed effects parameter. This reduced $X$ to a matrix with one column with all elements equal to $1$. Assuming that we have $q$ unrelated sires the relationship matrix $A_s$ for the sires corresponds to the identity matrix $I$. 


## Analysis Of Variance (Anova)
As a first approach we can use an analysis of variance by fitting 

1. a model with an overall effect $\beta = \mu$ and 
2. a model with sire effects. 

These two models give an analysis of variance of the following structure

\begin{tabular}{lll}
\hline \\
Source           &  Degrees of Freedom ($df$)          &  Sums of Squares ($SSQ$) \\
\hline \\
Overall ($\mu$)  &  $Rank(X)=1$                        &  $y^TX(X^TX)^{-1}X^Ty = F$  \\
Sires ($s$)      &  $Rank(Z_s) - Rank(X) = q - 1$      &  $y^TZ_s(Z_s^TZ_s)^{-1}Z_s^Ty - y^TX(X^TX)^{-1}X^Ty = S$  \\
Residual ($e$)   &  $n - Rank(Z_s) = n - q$            &  $y^Ty - y^TZ_s(Z_s^TZ_s)^{-1}Z_s^Ty = T$ \\
\hline \\
Total            &  $n$                                &  $y^Ty$ \\
\hline
\end{tabular}

The sums of squares ($SSQ$) can also be expanded into sums of scalar quantities which might be easier to understand. For our sire model we get

$$F = y^TX(X^TX)^{-1}X^Ty = {1\over n} \left[\sum_{i=1}^n y_i \right]^2$$
where $n$ corresponds to the number of observations in the dataset.

$$S= y^TZ_s(Z_s^TZ_s)^{-1}Z_s^Ty - y^TX(X^TX)^{-1}X^Ty = \sum_{i=1}^{q} {1 \over n_i} \left[\sum_{j=1}^{n_i} y_{ij}\right]^2 - F $$
where $n_i$ corresponds to the number of observations for sire $i$. 

$$T = y^Ty - y^TZ_s(Z_s^TZ_s)^{-1}Z_s^Ty = \sum_{i=1}^n y_i^2 - S - F$$

In principle effects $\beta$ and $s$ are treated as fixed effects in the above anova. If estimates of $\sigma_e^2$ and $\sigma_s^2$ are required the observed sums of squares $S$ and $T$ can be equated to their expected values $E(T) = (n-q) \sigma_e^2$ and $E(S) = (q-1) \sigma_e^2 + tr(Z_sMZ_s)\sigma_s^2$ where $M = I - X(X^TX)^{-1}X^T$ and $tr(M)$ stands for the trace of matrix M which corresponds to the sum of the diagonal elements of matrix $M$.



## Numerical Example
We want to show the estimation of variance components with a very small data set. The data that will be used is shown in the table below. The observations consist of pre-weaning weight gains of beef cattle. 


```{r datavcesm, echo=FALSE, results='asis'}
tbl_num_ex_chp12 <- tibble::tibble( Animal = c(4, 5, 6, 7),
                                        Sire   = c(2, 1, 3, 2),
                                        WWG    =  c(2.9, 4.0, 3.5, 3.5) )

knitr::kable(tbl_num_ex_chp12,
             booktabs  = TRUE,
             longtable = TRUE,
             caption   = "Small Example Dataset for Variance Components Estimation Using a Sire Model")
```


The model used is a simplified sire model where all the fixed effect are captured by a common mean $\mu$. Then there is the sire effect $s$ as a random effect and the random residual effect. Hence for any given observation $y_{ij}$ for animal $i$ of sire $j$, we can write

$$y_{ij} = \mu + s_j + e_i$$

with $\mu$ the common mean, $s_j$ the random effect of sire $j$ ($j = 1, 2, 3$) and $e_i$ corresponds to the random residual of observation $i$ ($i = 1, \ldots, 4$). In matrix notation thi s model was already given in \@ref(eq:varcompsiremodel). The design matrix $X$ is a matrix with one column and with elements all equal to $1$. The design matrix $Z_s$ links observations to sire effects. 


```{r,echo=FALSE, results='asis'}
n_nr_obs_p02 <- nrow(tbl_num_ex_chp12)
### # design matrix X
mat_x_p02 <- matrix(1, nrow = n_nr_obs_p02, ncol = 1)
### # design matrix Z
mat_z_p02 <- matrix(c(0, 1, 0,
                      1, 0, 0,
                      0, 0, 1,
                      0, 1, 0), nrow = n_nr_obs_p02, byrow = TRUE)
n_nr_sire <- ncol(mat_z_p02)
### # Observations
mat_obs <- matrix(tbl_num_ex_chp12$WWG, ncol = 1)

cat("$$\n")
cat("X = \\left[\n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = mat_x_p02, pnDigits = 0), collapse = "\n"), "\n")
cat("\\right] \\text{, }")
cat("Z_s = \\left[\n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = mat_z_p02, pnDigits = 0), collapse = "\n"), "\n")
cat("\\right]")
cat("$$\n")
```

```{r anovacomp, echo=FALSE, results='hide'}
### # compute F
ytx <- crossprod(mat_obs,mat_x_p02)
xtx <- crossprod(mat_x_p02)
ssqf <- ytx %*% solve(xtx) %*% t(ytx)
### # compute S
ytz <- crossprod(mat_obs, mat_z_p02)
ztz <- crossprod(mat_z_p02)
ssqs <- ytz %*% solve(ztz) %*% t(ytz) - ssqf
### # compute R
yty <- crossprod(mat_obs)
ssqr <- yty - ssqs - ssqf

```

An analysis of variance can be constructed as

\begin{center}
\begin{tabular}{lll}
\hline \\
Source           &  Degrees of Freedom ($df$)          &  Sums of Squares ($SSQ$) \\
\hline \\
Overall ($\mu$)  &  $Rank(X)=1$                  &  $F = `r ssqf`$  \\
Sires ($s$)      &  $Rank(Z_s) - Rank(X) = q - 1$  &  $S = `r ssqs`$  \\
Residual ($e$)   &  $n - Rank(Z_s) = n - q$        &  $T = `r ssqr`$ \\
\hline \\
\end{tabular}
\end{center}


With 

```{r varcompest, echo=FALSE, results='asis'}
mat_m <- diag(n_nr_obs_p02) - mat_x_p02 %*% solve(xtx) %*% t(mat_x_p02)
ztmz <- t(mat_z_p02) %*% mat_m %*% mat_z_p02
tr_ztmz <- sum(diag(ztmz))
cat("$$\n")
cat("M = \\left[")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = mat_m)))
cat("\\right] \\text{ and } ")
cat("Z_s^TMZ = \\left[")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = ztmz)))
cat("\\right] \n")
cat("$$\n")
```

we get the following estimates

```{r, echo=FALSE}
hat_sigmae2 <- ssqr
hat_sigmas2 <- (ssqs - (n_nr_sire-1) * hat_sigmae2) / tr_ztmz
```

$$\hat{\sigma_e^2} = T = `r hat_sigmae2`$$
$$\hat{\sigma_s^2} = \frac{S - (q-1)\hat{\sigma_e^2}}{tr(Z_s^TMZ_s)} = \frac{`r ssqs` - `r n_nr_sire-1` * `r hat_sigmae2`}{`r tr_ztmz`} = `r hat_sigmas2`$$

The same computations based on an anova can be done in `R` very easily. Assume that our dataset is in a dataframe which is called `tbl_num_ex_chp12_aov`. We are doing the anova using the function `aov()` to get the sums of squares.

```{r, echo=FALSE, results='hide'}
tbl_num_ex_chp12_aov <- tbl_num_ex_chp12
tbl_num_ex_chp12_aov$Sire <- as.factor(tbl_num_ex_chp12_aov$Sire)
```

```{r, echo=TRUE, results='markup'}
aov_num_ex_chp12 <- aov(formula = WWG ~ Sire, data = tbl_num_ex_chp12_aov)
summary(aov_num_ex_chp12)
```


The results from above are obtained for $\hat{\sigma_e^2} = 0.18$ as the value under the column `Mean Sq` in the row `Residuals`. Because in our computations above, we have considered the estimation of the overall effect which is not done in the function `aov()` in R.


## Negative Estimates with Anova
One of the problems that frequently occurs when using anova to estimate variance components is that some estimates might be negative. Negative estimates are outside of the permissible range for the parameter and hence are not valid estimates. As a consequence of that alternative methods have been proposed to estimate variance components. 


## Likelihood-Based Approaches
The maximum likelihood (ML) approach was developed and popularized by R. A. Fisher. ML is a general approach for parameter estimation and is not only used for estimating variance components. Let us assume that our observed traits are continuous and real-valued quantities. In ML we assume that these quantities follow a certain density. This density is a function of the observed values and of unknown parameters that we want to estimate. 


### Density of Observations
Given a vector $y$ of observations. As already mentioned, the vector $y$ follows a certain density. As an example such a density might be a multivariate normal distribution. For a given vector $y$ of length $n$, the underlying $n$-dimensional multivariate normal distribution has the following form

$$
f_Y(y) = \frac{1}{\sqrt{(2\pi)^n det(\Sigma)}} exp \left\{-{1\over 2}(y - \mu)^T \Sigma^{-1}(y - \mu) \right\}
$$

\begin{tabular}{lll}
with  &  $\mu$  &  expected value of $y$ \\
      &  $\Sigma$  &  variance-covariance matrix of $y$ \\
      &  $det()$   &  determinant
\end{tabular}


### Likelihood Function
As already mentioned the density is a function of the observed data $y$ and of some unknown parameters. For the multivariate normal distribution these parameters are $\mu$ and $\Sigma$. Before observing any data, we can interpret the density $f(y | \mu, \Sigma)$ as a function of $y$ for some fixed values of $\mu$ and $\Sigma$. But once the data has been observed, $y$ is fixed and the parameters $\mu$ and $\Sigma$ are unknown and must be estimated from the data. For the task of parameter estimation, it makes more sense to view $f(y | \mu, \Sigma)$ as a function of $\mu$ and $\Sigma$. We can write this function a little different

$$L(\mu, \Sigma) = f(y | \mu, \Sigma)$$

The function $L(\mu, \Sigma)$ is called the __Likelihood__ function. 


### Maximum Likelihood
For a given dataset we choose an appropriate density which is suitable for our observations. As already mentioned, due to the Central Limit Theorem, the normal distribution is often used as a density for observations. Once, we have chosen the density, it contains unknown parameters which we have to estimate from the data. Loosely speaking, our goal is to determine the parameters such that the observed data is modeled as good as possible. This requirement is translated into a mathematical framework by the maximization of the likelihood. Hence for a given dataset our parameter estimates are determined such that the likelihood is maximized. For our multi-variate normal distribution, this can be transformed into the following equations

$$\hat{\mu} = argmax_{\mu} L(\mu, \Sigma)$$

and

$$\hat{\Sigma} = argmax_{\Sigma} L(\mu, \Sigma)$$


## Summary
The topic of variance component estimation is a huge area. We have just covered two possible approaches to get estimates of variance components. There are many more of them. The coverage of these methods is outside of the scope of this course.

\end{comment}

<!--chapter:end:Estimation-of-Genetic-Parameters.Rmd-->

---
title: "Estimation of Genomic Breeding Values"
author: "Peter Sørensen,....."
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: yes
    includes:
      in_header: preamble.tex
  html_document:
    number_sections: yes
    includes:
      in_header: mathjax_header.html
  word_document: default
biblio-style: apalike
link-citations: yes
bibliography: lbgfs2021.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(19)
```



## Learning objective:  {-}    
This section introduces the basic concepts of estimating breeding values such as: 

   * basic principle behind estimating genomic breeding values
   * accuracy of estimated genomic breeding values
   * use of genomic relationships for estimating breeding values
   * different methods, data sources and experimental designs for estimating genomic breeding values


# Introduction
A new technology called __genomic selection__ has revolutionized animal and plant breeding. Genomic selection refers to selection decisions based on genomic breeding values (GEBV). We have previously learned how breeding values can be estimated from pedigree and phenotypic information (estimated breeding value, EBV). Genome-wide DNA markers can replace or supplement pedigree information for this purpose. The first ideas of genomic prediction were presented by (Meuwissen2001a). They showed that information from genotypes of very many loci evenly spread over the complete genome can successfully be used for the purposes of livestock breeding. Because the information of the genotypes is spread over the complete genome it is often referred to as __genomic information__ and from the use of this information for selection purposes the term of __genomic selection__ was invented. The early results on GS were not considered until the paper by (Schaeffer2006) showed that in a cattle breeding program the introduction of GS could lead to savings in about 90% of the total costs, provided that the accuracies computed by (Meuwissen2001a) can really be achieved. After the publication of (Schaeffer2006) many animal and plant breeding organisation started to introduce procedures of GS. 


## Basic principles for estimating genomic breeding values (GEBV)
In recent years much attention has been given to 
genomic information due to the dramatic development in genotyping technologies. 
Today dense 
genetic maps are available for most of the most important livestock species (Table 1). It is, 
however, still lacking for several species, but they circumvent this by using a so-called RAD-sequencing 
(Restriction site associated DNA sequencing) which enable dividing the entire genome into smaller 
segments just like if a genetic map had been available. Ultimately the entire DNA sequence may be 
genotyped. This is possible, but still very expensive, so only a few founder individual have been fully 
sequenced (mostly bulls, but also some horses and dogs). Lower resolution maps are also used and 
especially in cattle to save costs (e.g. females). 

Table 1. Number of markers in currently available SNP chips (March, 2013)
\begin{center} 
\begin{tabular}{|c|c|c|}
  \hline
  Species & No. SNPs (in thousands) & Genome size (x$10^9$) \\
  \hline
  Cattle & 778 & 2,67 \\
  \hline
  Pig & 64 & 2,81  \\
  \hline
  Chicken & 581 & 1,05 \\ 
  \hline
  Horse & 70 & 2,47  \\
  \hline
  Sheep & 54 & 2,62  \\
  \hline
  Dog & 170 & 2,41  \\
  \hline
\end{tabular}
\end{center}

The genetic maps are based on DNA markers in the form of single nucleotide polymorphisms 
(SNP) and they enable us to divide the entire genome into thousands of relatively small 
chromosome segments. 
 
## Genetic markers
The single location in the genome that are considered in GS are called __markers__. When looking at the complete set of markers consisting the genomic information in a population, the so-called __Single Nucleotide Polymorphisms__ (SNP) have been shown to be the most useful types of markers. These SNP correspond to differences of single bases at a given position in the genome. Based on empirical analyses of very many SNP-loci, almost all SNP just take two different states. Furthermore it is important that these SNPs are more or less evenly spread over the complete genome. Some SNPs are in coding regions and some my be placed in regions of unknown functionality. Figure \@ref(fig:snpdistribution) shows the distribution of SNP over the genome. 

```{r snpdistribution, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", fig.cap="Distribution of SNP-Loci Across A Genome"}
#rmddochelper::use_odg_graphic(ps_path = "odg/snpdistribution.odg")
knitr::include_graphics(path = "odg/snpdistribution.png")
```

## Quantitative Trait Loci
The loci that are relevant for a quantitative traits are called __Quantitative Trait Loci__ (QTL). Any given SNP-Marker can only be informative for a given QTL, if a certain __linkage disequilibrium__ between the QTL and the marker locus exists. The idea behind this linkage disequilibrium is that a certain positive QTL-allele evolved in a certain genetic neighborhood of a number of SNP loci. As a result of that the positive QTL-allele is very often inherited with the same SNP-allele. Over the generations, recombination between the QTL and the neighboring SNP-loci can happen and thereby weaken the association between the positive QTL-allele and the given SNP-allele. This recombination effect is smaller when the QTL and the SNP-loci are physically closer together on the chromosome. The non-random association between QTL and SNP-markers is called linkage disequilibrium.

The marker locus is called $M$ and the QTL is called $Q$, then the LD can be measured by 

\begin{align}
D &= p(M_1Q_1) * p(M_2Q_2) - p(M_1Q_2) * p(M_2Q_1)
(\#eq:linkagediseq)
\end{align}

where $p(M_xQ_y)$ corresponds to the frequency of the combination of marker allele $M_x$ and QTL allele $Q_y$. Very often the LD measure shown in \@ref(eq:linkagediseq) is re-scaled to the interval between $0$ and $1$ which leads to 

\begin{align}
r^2 &= \frac{D^2}{p(M_1)*p(M_2) * p(Q_1) * p(Q_2)}
(\#eq:linkagediseqrescaled)
\end{align}

In \@ref(eq:linkagediseqrescaled) $r^2$ describes the proportion of the variance at the QTL which is explained by the marker $M$. Hence the LD must be high such that the marker can explain a large part of the variance at the QTL. For the length of most livestock species, about $50'000$ SNP markers are required to get a sufficient coverage of the complete genome.

## Basic principles for estimating genomic breeding values (GEBV)
The GEBV are calculated as the sum of the effects of dense genetic markers across the entire genome, thereby potentially capturing all the quantitative trait loci (QTL) that contribute to variation in a trait. The QTL effects inferred from individual single nucleotide polymorphism (SNP) markers, are first estimated in a large reference population with phenotypic information. In subsequent generations, only marker information is required to calculate GEBV. 
Breeding values are conceptually simple to calculate from marker information. First, the entire 
genome is divided into small chromosome segments by dense markers. Second, the additive effects 
of each chromosome segment are estimated simultaneously. Finally, the genomic EBV equals the 
sum of all chromosome segment effects. The chromosome segment effects are estimated for a group 
of animals (i.e. a reference population). For any remaining animal, only a blood or tissue sample is 
needed to determine its genome-wide (or genomic) EBV (Figure 1). For breeding purposes, it is 
desirable that the EBV can be estimated accurately early in the animals life. 
The effect of each of these small chromosome segments can be estimated if we have phenotypes 
and genotypes from a number of animals. With sufficiently dense marker maps, the chromosome 
segment effects apply to all animals in the population in which they were estimated, because 
markers are in linkage disequilibrium with the causal gene that they bracket. 


## A linear mixed model for estimating marker effects (SNP-BLUP) {#linear-model-pgbv}
A linear model to estimate SNP-effects based on the data from the reference population in the two-step procedure can be defined as follows

\begin{equation}
y = X \beta +  Mg + e
(\#eq:linearmodelgpbv)
\end{equation}

\begin{tabular}{lll}
where  &  $m$  &  number of SNP markers \\
       &  $y$  &  vector of observations \\
       &  $\beta$  &  vector of fixed effects \\
       &  $X$      &  design matrix linking fixed effects to observations \\
       &  $g$      &  random genetic effect of SNP-genotypes \\
       &  $M$      &  design matrix linking SNP-genotype effects to observations \\
       &  $e$      &  vector of random residuals
\end{tabular}       

The observations $y$ used in \@ref(eq:linearmodelgpbv) are in most evaluations not phenotypes but traditionally predicted breeding values with an accuracy above a certain threshold. As a consequence of that the variance-covariance matrix ($R$) of the residuals $e$ is not just an identity matrix ($I$) times a residual variance component ($\sigma_e^2$) but $R$ is a diagonal matrix with elements $(R)_{ii} = {1\over B_{m}} - 1$ where $B_m$ is the accuracy of the traditionally predicted breeding value from an animal from the reference population, corrected for the parental contributions. In effect, $B_m$ corresponds to the accuracy of the mendelian sampling term.

The mixed-model equations resulting from models given in \@ref(eq:linearmodelgpbv) have the following structure

\begin{equation}
\left[
  \begin{array}{lr}
  X^TR^{-1}X  &  X^TR^{-1}M  \\
  M^TR^{-1}X  &  M^TR^{-1}M + I * \lambda
  \end{array}
\right]
\left[
  \begin{array}{c}
  \hat{\beta}\\
  \hat{g}
  \end{array}
\right]
=
\left[
  \begin{array}{c}
  X^TR^{-1}y \\
  M^TR^{-1}y
  \end{array}
\right]
(\#eq:mmegpbv)
\end{equation}

where 

\begin{equation}
\lambda = \frac{\sigma_e^2}{\sigma_a^2} \sum_{i=1}^m 2*p_i*(1-p_i)
(\#eq:lambdammegpbv)
\end{equation}

In \@ref(eq:lambdammegpbv) $\sigma_a^2$ is the total genetic variance and $p_i$ is the frequency of the SNP-allele that is associated with the positive QTL-allele.

The solutions for $\hat{g}$ from \@ref(eq:mmegpbv) correspond to the SNP-genotype effects. The predicted breeding value $\hat{a}$ for any selection candidate with genomic information is then computed as

\begin{equation}
\hat{a} = \sum_{i=1}^m M_i \hat{g}_i
(\#eq:genomicpbv)
\end{equation}

where $M_i$ corresponds to the vector of SNP-genotypes of the selection candidate.
This model is sometimes referred to the SNP-BLUP model. 


### Matrix $M$
The elements in matrix $M$ can be encoded in different ways. The results from the genotyping laboratory sends a code representing the nucleotide that can be found at a given position. For the use in the linear model we have to use a different encoding. Let us assume that at a given SNP-position, the bases $G$ or $C$ are observed and $G$ corresponds to the allele with the positive effect on our trait of interest. Based on the two observed alleles, the possible genotypes are $GG$, $GC$ or $CC$. One possible code for this SNP in the matrix $M$ might be the number of $G$-Alleles which corresponds to $2$, $1$ and $0$. Alternatively, it is also possible to use the codes $1$, $0$ and $-1$ instead which corresponds to the factors with which $a$ is multiplied to get the genotypic values in the single locus model.



## A linear mixed model for estimating genomic values (GBLUP) {#gblup}
The term `GBLUP` stands for genomic BLUP and is the most widely used single-step procedure. In GBLUP genomic breed values are directly predicted without the prediction of marker effects. This can be done by including the genomic breeding values $u$ which corresponds to the sum of all SNP-allele effects directly as a random effect in the model. 

\begin{equation}
y = X \beta + W u + e
(\#eq:gblupmodel)
\end{equation}

where $W$ is the design matrix linking genomic breeding values to observations. The mixed model equations are the defined as


\begin{equation}
\left[
  \begin{array}{lr}
  X^TR^{-1}X  &  X^TR^{-1}W \\
  W^TR^{-1}X  &  W^TR^{-1}W + G^{-1}* \lambda
  \end{array}
\right]
\left[
  \begin{array}{c}
  \hat{\beta} \\
  \hat{u}
  \end{array}
\right]
=
\left[
  \begin{array}{c}
  X^TR^{-1}y \\
  W^TR^{-1}y
  \end{array}
\right]
(\#eq:gblupmme)
\end{equation}

where $G$ is defined as in \@ref(eq:genomicrelmat) and $\lambda$ is the same as in equation \@ref(eq:lambdammegpbv). Several authors have shown that both procedures (two-step and single step) are equivalent. From \@ref(eq:gblupmodel) we can see that the GBLUP model looks very similar to the animal model, except that the covariances between random effects in the animal model are based on the numerator relationship matrix and in GBLUP they are modeled via the genomic relationship matrix $G$. This means in the animal model the covariance between random breeding values is based on the concept of common ancestry and identity by descent. This is replaced in GBLUP by the concept of sharing the same alleles based on identity by state which is assumed to be the cause of the covariance between random genomic breeding values. 

The predicted genomic breeding values $\hat{u}$ coming out of \@ref(eq:gblupmme) are referred to as `direct genomic breeding values` (DGV).

### Genomic Relationship Matrix $G$
Multiplying the matrix $M$ with its transpose $M^T$ results in a $n\times n$ square matrix $MM^T$. On the diagonal of this matrix we get counts of how many alleles in each individual have a positive effect. The off-diagonal elements count how many individual share the same alleles across all SNP-positions. In contrast to the additive genetic relationship matrix $A$, the counts here are based on identity by state and not on identity by descent.

The problem with matrix $MM^T$ is its dependence on the number SNP-markers. Therefore the matrix $MM^T$ is proportional to the relationship $A$ but it does not correspond to $A$ directly. As a solution to that problem (VanRaden2008) proposed to re-scale such that allele frequencies on a given locus are expressed as to times the deviation from $0.5$. This re-scaling is done with an $n \times m$ matrix $P$ where each of the $m$ columns corresponds to a SNP-Locus. Elements in column $i$ of matrix $P$ have all the same value corresponding to $2p_i - 0.5$ where $p_i$ corresponds to the frequency of the SNP-allele associated to the positive QTL-allele at locus $i$. 

The difference between matrices $M$ and $P$ is assigned to a new matrix $Z$

$$Z = M-P$$

Finally the matrix $ZZ^T$ must be scaled with the sum of $2p_i(1-p_i)$ over all SNP-loci to get to the genomic relationship matrix $G$.

\begin{equation}
G = \frac{ZZ^T}{\sum_{i=1}^m 2p_i(1-p_i)}
(\#eq:genomicrelmat)
\end{equation}

The matrix $G$ has similar properties as the numerator relationship matrix $A$. The genomic inbreeding coefficient $F_j$ is defined as $F_j = (G)_{jj}-1$. The genomic relationship $a_{ij}$ between two individuals $i$ and $j$ corresponds to the element in matrix $G$ divided by the square root of the diagonal elements 

$$a_{ij} = \frac{G_{ij}}{\sqrt{G_{ii}G_{jj}}}$$



### Practical Problems {#practical-problems}
The model equations \@ref(eq:gblupmodel) look very straight-forward, but the practical implementation can be quite complicated. The reason for these problems is the fact that compared to the total size of a population only a small fraction of all animals are genotyped and hence contribute the the genomic evaluation. On the other hand DGV do not contain all information that occur in conventional breeding values.

Because all non-genotyped offsprings of parents are ignored by GBLUP, this loss of information is even more dramatic. For the two step-procedure as long as the reference population has a reasonable size and is not too heterogeneous, this is not a problem, we can still come up with reasonable estimates of SNP-effects. Due to the in-balanced availability of genotypic information, a procedure to combine DGV with traditional predicted breeding values was adopted. This procedure starts with predicting DGV and combining them with traditionally predicted breeding values from parents which are termed as parent averages (PA). This procedure of combining predicted breeding values from different sources is called __blending__. The problem with blending one has to be aware of is that there is a covariance between DGV and PA which must be accounted for.

A further problem is that there are different techniques to generate genotyping results. The different results also have different densities which means that they give different numbers of SNP-loci per genome. The different techniques also vary in price which is the reason that genotyping results from different technologies must be combined. Combining genotyping results with different densities of SNP-markers per genome is done with a process that is called __imputing__. This basically comes done to inferring missing SNP-genotypes on marker panels with less density based on results from denser marker panels.


### How Does GBLUP Work {#asm-gblup-howdoesgblupwork}
The genomic relationship matrix $G$ allows to predict genomic breeding values for animals with SNP-Genotypes without any observation in the dataset. This fact is the basis of the large benefit of genomic selection. As soon as a young animal is born, its SNP genotypes can be determined and a genomic breeding value can be predicted. This genomic breeding value is much more accurate then the traditional breeding value based only on ancestral information. 

The BVM model given in \@ref(eq:asmgblupbvm) is a mixed linear effects model. The solution for the unknown parameters can be obtained by solving the mixed model equations shown in \@ref(eq:asm-gblup-gblupmme). In this form the Inverse $G^{-1}$ of $G$ and the vector $\hat{g}$ of predicted genotypic breeding values are split into one part corresponding to the animals with observations and a second part for the animals without phenotypic information.

```{r GblupMme, echo=FALSE, results='asis'}
matCoeff <- matrix(c("X^TX","X^TZ","0",
                     "Z^TX","Z^TZ + G^{(11)}","G^{(12)}",
                     "0","G^{(21)}","G^{(22)}"), ncol = 3, byrow = TRUE)
vecSol <- c("\\hat{b}","\\hat{g}_1","\\hat{g}_2")
vecRhs <- c("X^Ty","Z^Ty","0")
### # show mme
cat("\\begin{equation}\n")
cat(paste(rmdhelp::bmatrix(pmat = matCoeff), collapse = '\n'), '\n')
cat(paste(rmdhelp::bcolumn_vector(pvec = vecSol), collapse = '\n'), '\n')
cat(" = \n")
cat(paste(rmdhelp::bcolumn_vector(pvec = vecRhs), collapse = '\n'), '\n')
cat("(\\#eq:asm-gblup-gblupmme)")
cat("\\end{equation}\n")
```

The matrix $G^{(11)}$ denotes the part of $G^{-1}$ corresponding to the animals with phenotypic observations. Similarly, $G^{(22)}$ stands for the part of the animals without genotypic observations. The matrices $G^{(12)}$ and $G^{(21)}$ are the parts of $G^{-1}$ which link the two groups of animals. The same partitioning holds for the vector of predicted breeding values. The vector $\hat{g}_1$ contains the predicted breeding values for the animals with observations and the vector $\hat{g}_2$ contains the predicted breeding values of all animals without phenotypic observations. 

Based on the last line of \@ref(eq:asm-gblup-gblupmme) the predicted breeding values $\hat{g}_2$ of all animals without phenotypic observations can be computed from the predicted breeding values $\hat{g}_1$ from the animals with observations.

\begin{equation}
\hat{g}_2 = -\left( G^{22}\right)^{-1}G^{21}\hat{g}_1
(\#eq:GenomicBvAnimalNoPhen)
\end{equation}

Equation \@ref(eq:GenomicBvAnimalNoPhen) is referred to as genomic regression of predicted breeding values of animals without observation on the predicted genomic breeding values of animals with observations.


## Accuracy of genome-wide EBV {} 
In the example above, EBV were not estimated very accurately because only 6 phenotypes and 
genotypes were available for only two markers. The accuracy of genomic EBV increases when the 
size of the reference population increases, when the reference population represents as much of the 
relevant genetic variation in the population as possible, when selection candidates are closely 
related to the reference population, when genetic diversity in the population is low (i.e. low 
effective population size) and with better statistical models. More informative marker maps also 
increase accuracy, although the increase here is marginal when the marker density is already high 
(i.e. 50,000 markers for within breed selection; advantageous with more markers for very 
heterogeneous populations, across-breed analyses). 

The accuracy is trait dependent and depends on heritability. Figure 2 shows how the accuracy of 
EBV depends on heritability and number of phenotypic records.

Figure 2. Accuracy of EBV (rIA) as a function of number of phenotypic records (n) used to estimate 
marker effects for traits with heritability of 0.4 (solid curve), 0.15 (dotted curve) and 
0.05 (broken curve); Assumes normal distribution of marker effects and effective population size equal to 100. 


A common finding is that a straightforward BLUP method for estimating the marker effects gave reliabilities of GEBV almost as high as more complex methods. The BLUP method is attractive because the only prior information required is the additive genetic variance of the trait. 



## Impact of genome-wide information on breeding programmes {-}
The impact of using genomic information depends on the efficiency of traditional breeding that 
does not use genomic information. If all selection candidates already have accurate EBV at the time 
of selection then genomic information will not add much, if anything. Hence, genome-wide marker 
information is most useful when phenotypic recording is restricted – for instance when phenotypes 
are expressed late in the animal’s life (e.g. meat quality, longevity), are expressed only in one sex 
(e.g. milk yield, egg production) or are expensive to measure (e.g. feed efficiency, bacteriological 
samples, progesterone profiles or other physiological measures). Furthermore, genome-wide marker 
information is useful for traits with low heritability provided a sufficient amount of phenotypes can 
be recorded. It should therefore be clear that the extra benefits of genome-wide information vary 
across traits and across species although it can in principle be used for all species and traits.

Dairy cattle breeding is characterised by the main traits only being measurable in females while 
very intense selection is only possible in males. Thus genome-wide markers are very useful in dairy 
cattle breeding. 
In pig breeding, most traits are measured on all selection candidates before sexual 
maturity. Therefore genomic information gives less extra value for pig breeding compared with 
dairy cattle. However, exceptions for pigs include litter size (only measurable in females and after 
sexual maturity), feed efficiency (only measured on few animals because it is expensive), longevity 
and carcass traits (expressed late). Another potential benefit for pigs is that traits can be recorded on 
crossbreed production animals which may be housed in different production environments and have 
different effects of single genes compared with purebred animals. 

Genomic breeding values can be used to enhance the screening of potential breeding animals for 
testing (pre-selection) which is especially attractive in situations when the costs of genotyping are 
relatively inexpensive compared to the costs of recording phenotypes. They are also useful in 
intensifying selection for young animals thereby facilitating a reduced overall generation interval. 
For instance, with the availability of accurate genome-wide breeding values for young bulls it is 
more attractive to use the best young bulls widely rather than having to wait for the results of 
progeny group testing. Here the substantial reduction in generation interval offset the slightly lower
accuracy of genome-wide breeding values compared with breeding values based on progeny results. 
If, for example, the generation interval (L) is reduced from 5 to 2 years then a reduction in accuracy
(rIA) from 90% to 70% is acceptable as the genetic progress is almost doubled (i.e. increases by a 
factor 0.7×5 / 2×0.9 = 1.94). 

Another benefit of using genome-wide EBV rather than traditional EBV is that it results in less 
inbreeding if the same selection intensities are maintained. This happens because breeding based on 
traditional EBV puts more emphasis than genome-wide EBV on parent information, especially for 
traits with low heritability. 

A potential danger with genome-wide EBV is that it does not capture the effect of new non-recurrent mutations in selection candidates without phenotypic information (relating to self or progeny). Thus if selection and mating decisions are made before there is phenotypic information available from progeny, or the animal itself, it becomes impossible to estimate the effect of a new mutation. This means that new unfavourable mutations may be easier spread in the population and that favourable mutations may be missed if selection is based entirely on genome-wide EBV with 
negative consequences for long term genetic progress. 

Systems genetics approaches using prior knowledge from same or other species/breeds may be used to incorporate information about some of the rare alleles with low frequency ignored in current genomic selection. Such prior information could also sharpen inferences of genes with intermediate effects that are not ignored in current 
genomic selection. 

Also, often some traits that should be in the breeding goal are not systematically recorded (e.g. 
many diseases). But such traits are still influenced by selection on other traits and frequently the 
combined correlated effect on such non-recorded traits is negative. With selection based on 
genome-wide EBV there is a risk that the negative pressure on non-recorded traits increases. So, 
although genome-wide breeding values offers exiting opportunities for enhancing genetic progress 
by allowing for accurate selection of young animals then they should be used with appropriate care. 

For instance, the use of young bulls could be limited (e.g. to max ~10.000 inseminations and max 
~5% of the cow population) until progeny information is available. Such limits would also have a 
large positive effect in controlling inbreeding and a smaller effect on short term genetic gain. How 
to optimally design breeding plans based on genome-wide information is an important topic where 
more research is needed. 

\begin{comment}

Summary 

 * The following 3 steps are performed to calculate genome-wide breeding values: 1) genotype 
and phenotype reference animals, 2) estimate additive effects of small chromosome 
segments by regressing genotypes of reference animals on their phenotypes, 3) the breeding 
value of an animal is estimated as the sum of the effects of all chromosome segments carried 
by the animal

 * Genome-wide breeding values can be calculated both for animals with and without 
phenotypes as long as a DNA sample of the animal can be taken (potentially at embryo 
stage)

 * Genome-wide information allows accurate selection of young animals for all traits that can 
be recorded

 * Genome-wide breeding values are especially beneficial when traditional selection is difficult 
such as when phenotypic recording is restricted by sex and age (e.g. very beneficial for dairy 
cattle)

 * Conservative use of young animals without phenotypic information (relating to self or 
progeny) is advised to avoid potential negative side effects related to unfavourable 
mutations, unfavourable selection pressure on non-recorded breeding goal traits and high 
rates of inbreeding

Nowadays the term `genomic selection` is often used ambiguously. What most people mean when they are talking about GS should better be called __genomic prediction__ of breeding values. This prediction can be done in different ways which are listed below

* Two-step procedure: Effects of SNPs are predicted using single locus models in a reference population which corresponds of mainly male breeding animals with transformed predicted traditional BLUP-breeding values with an accuracy above a certain threshold. Alternatively, it is also possible to use statistics of daughter yields as observations for the prediction of marker effects. Predictions of genomic breeding values for all animals in the population with genomic information are computed by summing up all previously estimated SNP-effects. This procedure is currently applied in the Swiss dairy cattle populations.
* Single-step procedures try to predict genomic breeding values and traditional breeding values in a single evaluation.



## Genome-wide EBV {-}
Breeding values are conceptually simple to calculate from marker information. First, the entire 
genome is divided into small chromosome segments by dense markers. Second, the additive effects 
of each chromosome segment are estimated simultaneously. Finally, the genomic EBV equals the 
sum of all chromosome segment effects. The chromosome segment effects are estimated for a group 
of animals (i.e. a reference population). For any remaining animal, only a blood or tissue sample is 
needed to determine its genome-wide (or genomic) EBV (Figure 1). For breeding purposes, it is 
desirable that the EBV can be estimated accurately early in the animals life. 
The effect of each of these small chromosome segments can be estimated if we have phenotypes 
and genotypes from a number of animals. With sufficiently dense marker maps, the chromosome 
segment effects apply to all animals in the population in which they were estimated, because 
markers are in linkage disequilibrium with the causal gene that they bracket. 

Simple example: estimation of genome-wide breeding values 
Consider the genotype results of 6 animals that have each been genotyped for 2 SNP markers (Table 
2): 

Simple example: estimation of genome-wide breeding values 
Consider the genotype results of 6 animals that have each been genotyped for 2 SNP markers (Table 
2): 
Table 2. Pedigree, nucleotides at two positions, and phenotype for 6 example animals 
\begin{center} 
\begin{tabular}{|c|c|c|c|c|c|}
  \hline
Animal & Sire & Dam & Locus 1 & Locus 2 & Gain (g/day) \\
  \hline
1 & ? & ? & AA & CT & 1943 \\ 
  \hline
2 & ? & ? & GG & CC & 1908 \\ 
  \hline
3 & 1 & 2 & AG & CT & 1853 \\ 
  \hline
4 & 1 & 2 & AG & CT & 1845 \\ 
  \hline
5 & 3 & 4 & AG & CC & 2006 \\
  \hline
6 & 3 & 4 & AA & CT & 1941 \\
  \hline
\end{tabular}
\end{center}

First we check if there are inconsistencies between the pedigree and genotypes. For instance, if a 
parent is homozygous A at one marker locus then its entire offspring must have at least one A-allele 
at the same position. There are no inconsistencies in this example data, but in large real data sets 
errors can be present. 
Now we want to estimate the effect of substituting A with G at marker locus 1 and the effect of 
substituting T with C at locus 2. Here we illustrate the estimation of ‘additive allele substitution 
effects’ by simple linear regression for the purpose of simplicity, although better methods are 
available. The following linear regression model is used: 

\begin{align}
y_i &= \mu + b_1X_{1i} + b_2X_{2i} + e_i
\end{align}

where 

\begin{align}
 y_i &= \quad \text{is the daily gain of animal i (known)} \notag \\
 \mu &= \quad \text{is the mean daily gain (unknown parameter)}  \notag \\
b_1 &= \quad \text{is the additive effect of substituting A with G at locus 1 (unknown parameter)}  \notag \\
b_2 &= \quad \text{is the additive effect of substituting T with C at locus 2 (unknown parameter)} \notag \\
X_1 &= \quad \text{equals the number of A-alleles the i’th animal carries at locus 1 (known)} \notag \\
X_2 &= \quad \text{equals the number of T-alleles the i’th animal carries at locus 2 (known)}\notag \\ 
e_i &= \quad \text{is a random residual effect; residuals are assumed independent and N(0,$\sigma^2_{e}$)}\notag  
\end{align}


Using standard regression theory, the solutions to the unknown parameters are: 
$\mu$=1909.7; $b_1$=94.7; $b_2$ = -156.2; $\sigma^2_{e}$ = 199.1.


The breeding values, estimated using marker information, are: 

\begin{align}
EBV = \mu + b_1X_{1i} + b_2X_{2i}  
\end{align}

where the estimated values of $\mu$, $b_1$ and $b_2$ are used (hence estimated and not true BV). For animals 1-6, the following EBVs are obtained: 1942.8, 1909.7, 1848.2, 1848.2, 2004.3, and 1942.83 g/day. Animals 
1 and 6 have the same EBV because they have identical marker genotypes. Animal 5 has the largest 
EBV because it has two of the very favourable C-alleles at locus 2 and one of the favourable A alleles at locus 1. 
We assumed that the two markers were able to explain all genetic variation with respect to daily 
gain (i.e. all causative genes are in linkage disequilibrium with at least one of the markers). 
Normally we need thousands of markers covering the entire genome to ensure this. To accurately 
estimate the effect of many (small) chromosome segments we need many phenotypic observations 
(much more than the 6 available ones in this example; see also Fig. 2). 
Now imagine that animals 5 and 6 are mated with each other and produces a new offspring with 
genotype AA (locus 1) and CC (locus 2). 
The EBV of this new offspring is:

\begin{align}
EBV &= \mu + b_1X_{1i} + b_2X_{2i} = 2411.3 g/day. 
\end{align}

Note that it was possible to calculate an EBV for this new offspring without having 
phenotypic information for this animal. This feature makes genomic selection very useful because it 
facilitates early selection.


Statistical model and assumptions:
\begin{align}
y &= \mu + Wb + e \notag \\
b &\sim N(0,I\sigma^2_{b}) \notag \\
e &\sim N(0,I\sigma^2_{e}) \notag \\
Var(y) &= WW'\sigma^2_{b}+ I\sigma^2_{e} \notag
\end{align}

Estimating marker effects using the BLUP method:
\begin{align}
\hat{b} &= [WW'+I\frac{\sigma^2_{b}}{\sigma^2_{e}}]^{-1}W'(y-\hat{\mu})
\end{align}

Estimating genomic breeding value:
\begin{align}
\hat{a} &= W\hat{b}
\end{align}


Statistical model and assumptions:
\begin{align}
y &= \mu + a + e \notag \\
a &\sim N(0,G\sigma^2_{a}) \notag \\
e &\sim N(0,I\sigma^2_{e}) \notag \\
Var(y) &= G\sigma^2_{a}+ I\sigma^2_{e} \notag \\
G &= \frac{1}{m}WW' \qquad \text{(G is the genomic relationship matrix)} \notag
\end{align}


Estimating genomic breeding value using the BLUP method:
\begin{align}
\hat{a} &= G[G\sigma^2_{a}+I\sigma^2_{e}]^{-1}(y-\hat{\mu})
\end{align}




```{r,echo=FALSE, results='asis'}
nobs <- 6
m <- 2
muhat <- 1909.7
sigma2_e <- 1
sigma2_a <- 0.5
sigma2_b <- sigma2_a/m
Ie <- diag(1,nobs)
Ib <- diag(1,m)
### # design matrix X
X <- matrix(1, nrow = nobs, ncol = 1)
P <- matrix(2*0.5-0.5, nrow = nobs, ncol = 2)
### # design matrix Z
M <- c(2, 0, 1, 1, 1, 2, 1, 0, 1, 1, 0, 1)
M <- matrix(M, nrow = nobs, byrow = FALSE)
y <- c(1943,1908,1853,1845,2006,1941)
y <- matrix(y, nrow = nobs, ncol = 1)
sc <- 2*0.5*(1-0.5) +2*0.5*(1-0.5)
W <- (M-P)/(2*0.5*(1-0.5))
G <- (M-P)%*%t((M-P))/sc
bhat1 <- solve(t(M)%*%M)%*%t(M)%*%(y-muhat)
ghat1 <- M%*%bhat1
bhat2 <- solve(t(W)%*%W+Ib*sigma2_e/sigma2_b)%*%t(W)%*%(y-muhat)
ghat2 <- W%*%bhat2
ghat3 <- G%*%solve(G*sigma2_a+Ie*sigma2_a)%*%(y-muhat)
cat("$$\n")
cat("y = \\left[\n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = y, pnDigits = 0), collapse = "\n"), "\n")
cat("\\right] \\text{, }")
cat("X = \\left[\n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = X, pnDigits = 0), collapse = "\n"), "\n")
cat("\\right] \\text{, }")
cat("M = \\left[\n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = M, pnDigits = 0), collapse = "\n"), "\n")
cat("\\right]")
cat("$$\n")

cat("$$\n")
cat("P = \\left[\n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = P, pnDigits = 2), collapse = "\n"), "\n")
cat("\\right] \\text{, }")
cat("M-P = \\left[\n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = M-P, pnDigits = 2), collapse = "\n"), "\n")
cat("\\right]")
cat("$$\n")

cat("$$\n")
cat("MM' = \\left[\n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = M%*%t(M), pnDigits = 2), collapse = "\n"), "\n")
cat("\\right] \\text{, }")
cat("G = \\left[\n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = G, pnDigits = 2), collapse = "\n"), "\n")
cat("\\right]")
cat("$$\n")


cat("$$\n")
cat("g1 = \\left[\n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = ghat1, pnDigits = 2), collapse = "\n"), "\n")
cat("\\right] \\text{, }")
cat("g2 = \\left[\n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = ghat2, pnDigits = 2), collapse = "\n"), "\n")
cat("\\right] \\text{, }")
cat("g3 = \\left[\n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = ghat3, pnDigits = 2), collapse = "\n"), "\n")
cat("\\right]")
cat("$$\n")

```

\end{comment}

<!--chapter:end:Estimation-of-Genomic-Breeding-Values.Rmd-->

---
title: "Pedigree Based Relationship Matrix"
author: "Peter Sørensen,....."
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: yes
    includes:
      in_header: preamble.tex
  html_document:
    number_sections: yes
    includes:
      in_header: mathjax_header.html
  word_document: default
biblio-style: apalike
link-citations: yes
bibliography: lbgfs2021.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(19)
```



# Genetic Covariances Between Individuals {#geneticcovariancesbetweenanimals}
The prediction of breeding values using BLUP as shown in chapter \@ref(blup) uses linear mixed effects models where the breeding value of each individual is included as a random effect. Linear mixed effect models in general and specifically Henderson's mixed model equations require us to be able to specify the variance-covariance matrix of all random effects. When using the linear mixed model, the breeding value of each individual is included as a random effect in the linear mixed effects model. As a consequence of that we need to determine the covariance between the true breeding values of all individuals. Figure \@ref(fig:animalcov) tries to display the structure of the required variance-covariance diagrammatically.

```{r animalcov, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", fig.cap="Genetic Covariance Between Individuals"}
#rmddochelper::use_odg_graphic(ps_path = "odg/animalcov.odg")
knitr::include_graphics(path = "odg/animalcov.png")
```
 
The variance-covariance matrix shown at the bottom of Figure \@ref(fig:animalcov) has the variances of the true breeding values on the diagonal and all the covariances between the true breeding values of all individuals as offdiagonal elements. From the formulation of the linear mixed effect model in \@ref(eq:linearmixedeffectmodelmatrixvector), we defined the variance-covariance matrix of the random effects $u$ to be $G$ (see equation \@ref(eq:variancerandomeffects)). When predicting breeding values with the linear mixed model, the variance-covariance matrix of all components in the vector $a$ is defined as 

\begin{equation}
var(a) = G = A * \sigma_a^2
(\#eq:genvarcovarmatrixanimalmodel)
\end{equation}

where the matrix $A$ is called __numerator relationship matrix__.


## Similarity Between Individuals {#similaritybetweenindividuals}
At the genetic level there are two different kinds of similarity

1. Identity by descent (IBD)
2. Identity by state

```{r ibdibs, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", fig.cap="Identity by State Versus Identity by Descent", out.width="100%"}
#rmddochelper::use_odg_graphic(ps_path = "odg/ibdibs.odg")
knitr::include_graphics(path = "odg/ibdibs.png")
```

Figure \@ref(fig:ibdibs) illustrates the difference between the two type of identities. The type of graph shown in Figure  \@ref(fig:ibdibs) is called a __pedigree__ which is commonly used to display the relationship between individuals in a population. The rectangle symbols denote male individuals and the round symbols stand for female individuals. The horizontal connections between female and male individuals denote a mating. All individuals connected to a vertical line and follow below are progeny of the connected parents. 

The notations inside of the symbols stand for the different genotypes of the individuals on a given locus. The red arrows denote the path of two $A_1$-alleles which are copies of the same ancestral allele. These two copies are called __identical by descent__ (IBD). The green arrows show the path of two alleles which are identical by state which do not originate from the same copy of any given ancestral alleles. 


## Numerator Relationship Matrix {#numeratorrelationshipmatrix} 
The probability of identical genes by descent (IBD) occurring in two individuals is termed the co-ancestry or the coefficient of kinship (Falconer1996). The additive genetic relationship between two individuals is twice their co-ancestry. The matrix that expresses the additive genetic relationship among individuals in a population is called the __numerator relationship matrix__ $A$. The matrix $A$ is symmetric and its diagonal elements $(A)_{ii}$ are equal to $1 + F_i$ where $F_i$ is the __coefficient of inbreeding __ of individual $i$. The coefficient of inbreeding $F_i$ indicates whether an individual $i$ is inbred or not. $F_i$ is defined to be half the additive genetic relationship between the parents of $i$. Hence the diagonal element $(A)_{ii}$ of matrix $A$ corresponds to twice the probability that two gametes taken at random from an individual $i$ will carry IBD-alleles.

The off-diagonal elements $(A)_{ij}$ equals the numerator of the coefficient of relationship between individuals $i$ and $j$. Multiplying the matrix $A$ by the additive genetic variance $\sigma_a^2$ leads to the covariance among breeding values. Thus if $a_i$ is the breeding value of individual $i$ then 

\begin{equation}
var(a_i) = (A)_{ii} \sigma_a^2 = (1 + F_i) \sigma_a^2
(\#eq:vartruebreedingvalue)
\end{equation}


### Algorithm To Compute $A$ {#algorithmtocomputea}
The matrix $A$ can be computed using either the 

1. path coefficient method or 
2. recursive method.

The second method is especially suitable for an implementation by a software program. In what follows the recursive method to compute the components of $A$ is described now. Initially, individuals in a pedigree are numbered from $1$ to $n$ and ordered such that parents precede their progeny. The following rules are then  used to compute the components of $A$. 

* If both parents $s$ and $d$ of individual $i$ are known then 
    + the diagonal element $(A)_{ii}$ corresponds to: $(A)_{ii} = 1 + F_i = 1 + {1\over 2} (A)_{sd}$ and
    + the off-diagonal element $(A)_{ji}$ is computed as:  $(A)_{ji} = {1\over 2} ((A)_{js} + (A)_{jd})$
    + because $A$ is symmetric $(A)_{ji} = (A)_{ij}$
    
* If only one parent $s$ is known and assumed unrelated to the mate
    + $(A)_{ii} = 1$
    + $(A)_{ij} = (A)_{ji} = {1\over 2} ((A)_{js}$
    
* If both parents are unknown    
    + $(A)_{ii} = 1$
    + $(A)_{ij} = (A)_{ji} = 0$
    

### Numeric Example
```{r pedexamplesetup, echo=FALSE, results='hide'}
suppressPackageStartupMessages( library(pedigreemm) )
n_nr_ani_ped <- 6
n_nr_parent <- 2
tbl_ped <- tibble::tibble(Calf = c((n_nr_parent+1):n_nr_ani_ped),
                             Sire = c(1, 1, 4, 5),
                             Dam  = c(2, NA, 3, 2))
ped <- pedigree(sire = c(rep(NA, n_nr_parent), tbl_ped$Sire), dam = c(rep(NA, n_nr_parent), tbl_ped$Dam), label = as.character(1:n_nr_ani_ped))
matA <- as.matrix(getA(ped = ped))
matAinv <- as.matrix(getAInv(ped = ped))
```

We are given the following pedigree and we want to compute the matrix $A$ using the recursive method described in \@ref(algorithmtocomputea). 

```{r tabpedexample, echo=FALSE, results='asis'}
knitr::kable(tbl_ped,
             format = 'latex',
             booktabs = TRUE,
             longtable = TRUE,
             caption = "Example Pedigree To Compute Additive Genetic Relationship Matrix")
```

The first step of the computations of $A$ are the numbering and the ordering of all the individuals. This is already done in the pedigree shown in Table \@ref(tab:tabpedexample). The components of $A$ are computed row-by-row starting with $(A)_{11}$. 

\begin{align}
(A)_{11} &= 1 + F_1 = 1 + 0 = 1 \notag \\
(A)_{12} &= 0 = (A)_{21} \notag \\
(A)_{13} &= {1\over 2} ((A)_{11} + (A)_{12}) = 0.5 = (A)_{31} \notag \\
(A)_{14} &= {1\over 2} (A)_{11} = 0.5 = (A)_{14}  \notag \\
(A)_{15} &= {1\over 2} (A)_{14} + (A)_{13}) = 0.5 = (A)_{51} \notag \\
(A)_{16} &= {1\over 2} (A)_{15} + (A)_{12}) = 0.25 \notag
\end{align}

The same computations are also done for all the other components of the matrix $A$. The final result for the matrix looks as follows

```{r displaymatrixa, echo=FALSE, results='asis'}
cat("$$\n")
# cat("A = \\left[")
# cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = matA, pnDigits = 4), sep = "\n"), "\n")
# cat("\\right]\n")
cat(paste(rmdhelp::bmatrix(pmat = matA, ps_name = 'A'), sep = '\n'), '\n')
cat("$$\n")
```

As a result, we can see from the components of the above shown matrix $A$ that individuals $1$ and $2$ are not related to each other. Furthermore from the diagonal elements of $A$, it follows that individuals $5$ and $6$ are inbred while individuals $1$ to $4$ are not inbred.

\begin{comment}


## The Inverse Numerator Relationship Matrix {#inversenumeratorrelationshipmatrix}
The general form of Henderson's mixed model equations as shown in \@ref(eq:mixedmodeleq) depend on four matrices

1. Design matrix $X$ for the fixed effects
2. Design matrix $Z$ for the random effects
3. The inverse variance-covariance matrix $R^{-1}$ for the residuals $e$ and
4. The inverse variance-covariance matrix $G^{-1}$ for the random breeding values $a$. 

When using the linear mixed model as described in section \@ref(#animalmodel), we have seen in \@ref(eq:genvarcovarmatrixanimalmodel) that $G$ corresponds to the product of the numerator relationship matrix $A$ and the genetic additive variance $\sigma_a^2$. But the mixed model equations depend on the inverse $G^{-1}$ of $G$ which means

\begin{equation}
G^{-1} = (A * \sigma_a^2)^{-1} = A^{-1} * {1\over \sigma_a^2}
(\#eq:ginvformme)
\end{equation}

From \@ref(eq:ginvformme) we can see that in order to be able to set up the mixed model equations we need the inverse numerator relationship matrix $A^{-1}$. Because in practical routine predictions of breeding values, we have something in the order of 10 million individuals that we predict breeding values for. Hence the matrix $A$ has 10 million rows and 10 million columns. A matrix of that size cannot be inverted explicitly with commonly known methods such as described under https://en.wikipedia.org/wiki/Invertible_matrix. Henderson published in (Henderson1976) a set of simple rules to directly construct the matrix $A^{-1}$ without setting up the numerator relationship matrix.


## Structure of $A^{-1}$
When looking at a concrete example of an inverse of a numerator relationship matrix as shown below, we can discover some important facts. Let us assume the following pedigree.

```{r simplepedexamplesetup, echo=FALSE, results='hide'}
### # first six animals from Goetz p. 83
suppressPackageStartupMessages( library(pedigreemm) )
n_nr_ani_ped <- 5
n_nr_parent <- 3
tbl_ped_simple <- tibble::tibble(Calf = c(1:n_nr_ani_ped),
                             Sire = c(NA, NA, NA, 1, 3),
                             Dam  = c(NA, NA, NA, 2, 2))
ped_simple <- pedigree(sire = tbl_ped_simple$Sire, dam = tbl_ped_simple$Dam, label = as.character(1:n_nr_ani_ped))
matu_simple <- as.matrix(getA(ped = ped_simple))
matAinv_simple <- as.matrix(getAInv(ped = ped_simple))
```

```{r tabpedsimpleexample, echo=FALSE}
knitr::kable(tbl_ped_simple,
             format = 'latex',
             booktabs = TRUE,
             longtable = TRUE,
             caption = "Pedigree Used To Compute Inverse Numerator Relationship Matrix")
```

The numerator relationship matrix $A$ for the pedigree shown in Table \@ref(tab:simplepedexamplesetup) is shown in \@ref(eq:numeratorrelationshipmatrix).

```{r displaymatrixasimple, echo=FALSE, results='asis'}
cat("\\begin{equation}\n")
# cat("A = \\left[")
# cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = matu_simple, pnDigits = 4), sep = "\n"), "\n")
# cat("\\right]\n")
cat(paste(rmdhelp::bmatrix(pmat = matu_simple, ps_name = 'A'), sep = '\n'), '\n')
cat("\\label{eq:numeratorrelationshipmatrix}\n")
cat("\\end{equation}\n")
```

The matrix $A^{-1}$ shown below corresponds to the inverse of the matrix $A$ from \@ref(eq:numeratorrelationshipmatrix).

```{r displaymatrixainv, echo=FALSE, results='asis'}
cat("$$\n")
# cat("A^{-1} = \\left[")
# cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = matAinv_simple, pnDigits = 4), sep = "\n"), "\n")
# cat("\\right]")
cat(paste(rmdhelp::bmatrix(pmat = matAinv_simple, ps_name = 'A^{-1}'), sep = '\n'), '\n')
cat("$$")
```

From the above shown inverse $A^{-1}$, we recognize that $A^{-1}$ has a simpler structure than $A$ itself. The reason for this is that non-zero elements occur only at matrix elements of $A^{-1}$ corresponding to parents and progeny or to mates. Furthermore off-diagonal elements corresponding to mates are positive and elements corresponding to parents and progeny are negative. These observations were used by Henderson in [@Henderson1976] to come up with the rules described below. 


## Henderson's Rule To Set Up $A^{-1}$
We denote the row or column index corresponding to an individual of interest as $i$ and the row or column index belonging to the individuals father as $s$ and the row or column index corresponding to individual $i$'s mother as $d$. The rules differentiate the following three cases 

1. both parents of individual $i$ are known
2. only one parent of individual $i$ is known
3. both parents of individual $i$ are unknown


### Both Parents Known

* add $2$ to the diagonal-element $(i,i)$
* add $-1$ to off-diagonal elements $(s,i)$, $(i,s)$, $(d,i)$ and $(i,d)$
* add ${1\over 2}$ to elements $(s,s)$, $(d,d)$, $(s,d)$, $(d,s)$


### Only One Parent Known
We assume that sire $s$ is known

* add ${4\over 3}$ to diagonal-element $(i,i)$
* add $-{2\over 3}$ to off-diagonal elements $(s,i)$, $(i,s)$
* add ${1\over 3}$ to element $(s,s)$


### Both Parents Unknown

* add $1$ to diagonal-element $(i,i)$

The application of Henderson's rules to construct $A^{-1}$ directly will be a problem in one of the coming exercises. When applying the above described rules, it has to be noted well that this simple version of the rules are only valid for a pedigree without inbreeding. We will see in section \@ref(#derivationofhendersonsrules) how to extend the simple rules such that they can be used for a general pedigree with inbreeding.


## Derivation of Henderson's Rules {#derivationofhendersonsrules}
Henderson's rules can be related to the so-called `LDL`-decomposition of the numerator relationship matrix $A$. The result of this decomposition consists of the equivalence between matrix $A$ and the product of three matrices $L$, $D$ and $L^T$. The matrix $L$ is a lower triangular matrix and the matrix $D$ is a diagonal matrix. The reason for why we are doing this decomposition of $A$ is that the matrices $L$ and $D$ can much easier be inverted than the matrix $A$. The `LDL`-decomposition is a general procedure in linear algebra and it can be applied to any symmetric and positive-definite matrix not only to numerator relationship matrices. But when the `LDL`-decomposition is applied to a numerator relationship matrix, the matrices $L$ and $D$ do also have a special genetic meaning. This meaning is revealed in the following derivation.


### Decomposition of True Breeding Value and its Variance {#decompositionoftruebreedingvalueanditsvariance}
The true breeding value ($a_i$) of individual $i$ can be decomposed into the true breeding values of individual $i$'s parents $s$ and $d$ and the mendelian sampling term $m_i$

\begin{equation}
a_i = {1\over 2} a_s + {1\over 2} a_d + m_i
(\#eq:decomposeanimaltruebreedingvalue)
\end{equation}

Applying the decomposition shown in \@ref(eq:decomposeanimaltruebreedingvalue) for all individuals in the pedigree and combining the decompositions into a matrix-vector notation, we get 

\begin{equation}
a = P \cdot a + m
(\#eq:decomposetruebreedingvaluematvec)
\end{equation}

\begin{tabular}{lll}
where  &       &  \\
       &  $a$  &  vector of true breeding values for all individuals  \\
       &  $P$  &  matrix linking individuals to their parents         \\
       &  $m$  &  vector of mendelian sampling terms
\end{tabular}

Equation \@ref(eq:decomposetruebreedingvaluematvec) can be interpreted as regression of the true breeding values onto parental breeding values. In such a regression the random term $m$ is like a residual term. The genetic meaning of $m$ corresponds to the deviation of $a_i$ from the full-sib average of the true breeding values within the nuclear family with parents $s$ and $d$. The term $m$ is called __mendelian sampling__ term. The source of $m$ is the fact that during meiosis, parental alleles are randomly assigned to each progeny. Bulmer (Bulmer1971) has shown that $m_i$ are independent from true breeding values of parents $s$ and $d$. Using this result, we take the variance on both sides of \@ref(eq:decomposeanimaltruebreedingvalue) leading to

\begin{align}
var(a_i)  &=  var({1\over 2} a_s + {1\over 2} a_d + m_i) \notag \\
          &=  {1\over 4} var(a_s) + {1\over 4} var(a_d) + {1\over 2} cov(a_s, a_d) + var(m_i)
(\#eq:vartruebreedingvalue)          
\end{align}

From \@ref(eq:genvarcovarmatrixanimalmodel) together with the definition of the numerator relationship matrix $A$, we know that 

\begin{align}
var(a_i)  &=  (1 + F_i) \sigma_a^2 \notag \\
var(a_s)  &=  (1 + F_s) \sigma_a^2 \notag \\
var(a_d)  &=  (1 + F_d) \sigma_a^2 \notag \\
cov(a_s, a_d)  &=  (A)_{sd} \sigma_a^2 = 2F_i \sigma_a^2
(\#eq:usedefnumeratorrelationshipmatrix)          
\end{align}


### Variance of Mendelian Sampling Terms {#variancemendeliansamplingterm}
Inserting the relations from \@ref(eq:usedefnumeratorrelationshipmatrix) into \@ref(eq:vartruebreedingvalue) and solving for $var(m_i)$ gives the following result

\begin{align}
var(m_i)  &=  var(a_i) - {1\over 4} var(a_s) - {1\over 4} var(a_d) - {1\over 2} cov(a_s, a_d) \notag \\
          &=  (1 + F_i) \sigma_a^2 - {1\over 4}(1 + F_s) \sigma_a^2 - {1\over 4} (1 + F_d) \sigma_a^2 - {1\over 2} * 2 * F_i * \sigma_a^2 \notag \\
          &=  \left({1\over 2} - {1\over 4}(F_s + F_d)\right) \sigma_a^2
(\#eq:mendeliansamplingvariance)          
\end{align}

In case where only parent $s$ of individual $i$ is known the terms in \@ref(eq:decomposeanimaltruebreedingvalue) and \@ref(eq:mendeliansamplingvariance) change to

\begin{equation}
a_i = {1\over 2} a_s + {1\over 2} a_d + m_i
(\#eq:decomposeanimaltruebreedingvalue)
\end{equation}


\begin{align}
a_i       &=  {1\over 2} a_s + m_i \notag \\
var(m_i)  &=  \left(1 - {1\over 4}(1+ F_s)\right) \sigma_a^2 \notag \\
          &=  \left({3\over 4} - {1\over 4}F_s\right) \sigma_a^2
(\#eq:mendeliansamplingvarianceoneparentknown)          
\end{align}

When both parents are unknown, we get

\begin{align}
a_i       &=  m_i \notag \\
var(m_i)  &=  \sigma_a^2
(\#eq:mendeliansamplingvariancebothparentunknown)          
\end{align}


### Decomposition of $A$ {#decompositionofa}
The true breeding values $a_s$ of sire $s$ and $a_d$ of dam $d$ can be decomposed in a similar way as shown for the true breeding value $a_i$ in \@ref(eq:decomposeanimaltruebreedingvalue). 

\begin{align}
a_s  &=  {1\over 2} a_{ss} + {1\over 2} a_{ds} + m_s \notag \\
a_d  &=  {1\over 2} a_{sd} + {1\over 2} a_{dd} + m_d
(\#eq:decomposeparenttruebreedingvalue)
\end{align}

\begin{tabular}{lll}
where  &  &  \\
       &  $ss$  &  sire of $s$  \\
       &  $ds$  &  dam of $s$   \\
       &  $sd$  &  sire of $d$  \\
       &  $dd$  &  dam of $d$  
\end{tabular}

Using \@ref(eq:decomposeparenttruebreedingvalue) together with \@ref(eq:decomposeanimaltruebreedingvalue) leads to the following expression for $a_i$

\begin{align}
a_i  &=  {1\over 2} a_s + {1\over 2} a_d + m_i \notag \\
     &=  {1\over 2} ({1\over 2} a_{ss} + {1\over 2} a_{ds} + m_s)+ {1\over 2} ({1\over 2} a_{sd} + {1\over 2} a_{dd} + m_d) + m_i \notag \\
     &=  {1\over 4} a_{ss} + {1\over 4} a_{ds} + {1\over 4} a_{sd} + {1\over 4} a_{dd} + {1\over 2} m_s + {1\over 2} m_d + m_i  \notag
(\#eq:recursivedecomposeanimaltruebreedingvalue)
\end{align}

This type of decomposition can also be done for the grand-parents of individual $i$ and further back until we get to true breeding values of individuals with unknown parents. Individuals of ancestor generations with unknown parents are called __founder population__. The process of decomposing true breeding values back through all generations of a pedigree is called __recursive decomposition__ of individual $i$'s true breeding value. Because we know from \@ref(eq:mendeliansamplingvariancebothparentunknown) that the decomposition of true breeding values $u_k$ for an individual $k$ with unknown parents is $u_k = m_k$, the result of the recursive decomposition of $a_i$ is a sum of mendelian sampling terms linking the ancestors of $i$ back to the founder population. 

Ordering all individuals in a pedigree according to their age and writing the result of the recursive decomposition of all true breeding values in matrix-vector notation leads to

\begin{equation}
a = L \cdot m
(\#eq:resultrecursivedecomposition)
\end{equation}

The matrix $L$ is a lower triangular matrix. The row corresponding to individual $i$ is the average of the rows in $L$ corresponding to parents $s$ and $d$ of $i$. The matrix $L$ contains the path of the gene flow from the base population to all individuals in the population. From equation \@ref(eq:resultrecursivedecomposition), we are computing the variance of all true breeding values which leads to

\begin{align}
var(a)  &=  var(L \cdot m) \notag \\
        &=  L \cdot var(m) \cdot L^T \notag \\
        &=  L \cdot (D \sigma_a^2) \cdot L^T \notag \\
        &=  L \cdot D \cdot L^T  \sigma_a^2 \notag
        &=  A \sigma_a^2
(\#eq:vararecursivedecomposition)
\end{align}

From \@ref(eq:vararecursivedecomposition), the `LDL`-decomposition of $A$ follows directly as $A = LDL^T$. The single components $m_i$ are independent of each other. This also means that $cov(m_i, m_j) = 0$ for $i \ne j$. Hence the matrix $D$ is a diagonal matrix. In section \@ref(#variancemendeliansamplingterm) we have seen that $var(m_i)$ always contain $\sigma_a^2$ as a factor. Hence it is reasonable to express $var(m)$ as a product of a diagonal matrix $D$ times the genetic additive variance $\sigma_a^2$. The diagonal elements of matrix $D$ are computed as shown in section \@ref(#variancemendeliansamplingterm).

The mixed model equations require the inverse numerator relationship matrix $A^{-1}$. Thanks to the `LDL`-decomposition of $A$, we can compute $A^{-1}$ as

\begin{equation}
A^{-1} = (L \cdot D \cdot L^T)^{-1} = (L^T)^{-1} \cdot D^{-1} \cdot L^{-1}
(\#eq:ainvldldecomp)
\end{equation}

The inverse $D^{-1}$ is easy to compute, because $D$ is a diagonal matrix. As a consequence of that $D^{-1}$ is also a diagonal matrix where the elements $(D^{-1})_{ii}$ correspond to the inverse of the elements of the original matrix $D$, i.e. $(D^{-1})_{ii} = 1/(D)_{ii}$. The matrix $L^{-1}$ is also a lower triangular matrix and can easily computed based on the two relations for the vector $m$. Based on \@ref(eq:decomposetruebreedingvaluematvec), we know

\begin{equation}
m = a - Pa = (I-P)a
(\#eq:mendeliansamplingiminuspa)
\end{equation}

and from \@ref(eq:resultrecursivedecomposition), we get

\begin{equation}
m = L^{-1}a
(\#eq:mendeliansamplinglinversea)
\end{equation}

Setting both expressions for $m$ in \@ref(eq:mendeliansamplingiminuspa) and \@ref(eq:mendeliansamplinglinversea) equal can be used to obtain $L^{-1}$

\begin{equation}
m  = (I-P)a = L^{-1}a
(\#eq:linverseequation)
\end{equation}

Therefore, 

\begin{equation}
L^{-1} = I - P
(\#eq:linverseresult)
\end{equation}


### General Version of Henderson's Rule
Based on the decomposition of $A^{-1}$ shown in \@ref(eq:ainvldldecomp), the general version of Henderson's rule where inbreeding of individuals can be accounted for can be summarized as 

* Start with a matrix $A^{-1}$ where all elements are set to $0$.
* Let $d^i$ be the $i$-th diagonal element of $D^{-1}$ for individual $i$, assuming $i$ has parents $s$ and $d$.
* Then add the following contributions to $A^{-1}$
    + $d^i$ to the element $(i,i)$
    + $-d^i/2$ to the elements $(s,i)$, $(i,s)$, $(d,i)$, $(i,d)$
    + $d^i/4$ to the elements $(s,s)$, $(s,d)$, $(d,s)$, $(d,d)$

The contributions to rows and columns corresponding to parents $s$ and $d$ are only added, if they are known. Because the elements $d^i$ are dependent on the inbreeding coefficients $F_s$ and $F_d$ of parents $s$ and $d$, we have to find an efficient way to compute inbreeding coefficients for all individuals in a population.


## Computing Inbreeding Coefficients {#computinginbreedingcoefficients}
Inbreeding coefficients can be computed using different methods. From all these methods, we are just showing the one method which was described in (Quaas1976). This method is based on the properties of a second decomposition of the numerator relationship matrix $A$ which is called the __cholesky decomposition__. The cholesky decomposition of a matrix $A$ corresponds to 

\begin{equation}
A = RR^T
(\#eq:choleskymata)
\end{equation}

where the matrix $R$ is a lower triangular matrix. At this point we have to be clear about this. In practical routine evaluations, we will not explicitly compute this decomposition, because we do not want to construct $A$ explicitly. We are just using the properties of \@ref(eq:choleskymata) to find an efficient way to compute the diagonal elements $(A)_{ii}$ of $A$ without constructing the complete matrix $A$. The diagonal elements $(A)_{ii}$ are important, because they contain the inbreeding coefficients ($F_i$), as $(A)_{ii} = 1 + F_i$. Based on \@ref(eq:choleskymata), $(A)_{ii}$ can be computed from the components of $R$ as

\begin{equation}
(A)_{ii} = \sum_{j=1}^i (R)_{ij}^2
(\#eq:DiagElemMatA)
\end{equation}

This can be shown with a small $3\times 3$ matrix $A$

```{r SmallExCholA, eval=TRUE, echo=FALSE, results='asis'}
nAnzAni <- 3
matA <- rmdhelp::matGetMatElem(psBaseElement = "(A)", pnNrRow = nAnzAni, pnNrCol = nAnzAni)
matR <- rmdhelp::matLowerTri(psBaseElement = "(R)", pnNrRow = nAnzAni, pnNrCol = nAnzAni)
cat("$$")
cat(paste(rmdhelp::bmatrix(pmat = matA), collapse = "\n"))
cat(" \n")
cat(" = ")
cat(paste(rmdhelp::bmatrix(pmat = matR), collapse = "\n"))
cat(" \n")
cat(" * ")
cat(paste(rmdhelp::bmatrix(pmat = t(matR)), collapse = "\n"))
cat("\n")
cat("$$\n")
```

For the above shown example, the diagonal elements $(A)_{ii}$ are computed as

\begin{align}
  (A)_{11} &= (R)_{11}^2 \notag \\
  (A)_{22} &= (R)_{21}^2 + (R)_{22}^2 \notag \\
  (A)_{33} &= (R)_{31}^2 + (R)_{32}^2 + (R)_{33}^2 \notag
(\#eq:MatADiag)
\end{align}

This shows that diagonal elements $(A)_{ii}$ can be computed using just all the components of row $i$ in $R$ up to the diagonal. Next, we have to show how to compute the components of $R$.


### Recursive Computation of $R$
We are setting the two decompositions of $A$ equal which leads to

\begin{equation}
A = R * R^T = L * D * L^T
(\#eq:CholEqLdl)
\end{equation}

Let us write the matrix $R$ as a product of two matrices $L$ and $S$ where $L$ corresponds to the matrix from the `LDL`-decomposition and insert that product into \@ref(eq:CholEqLdl)

\begin{equation}
A = R * R^T = L * S * (L * S)^T = L * S * S^T * L^T= L * D * L^T
(\#eq:CholRLSEqLdl)
\end{equation}

From \@ref(eq:CholRLSEqLdl), we conclude that $D = S \cdot S^T$ where $S$ is also a diagonal matrix with elements $(S)_{ii} = \sqrt{(D)_{ii}}$. For our small example we get

\begin{equation}
```{r RlsDecompEx, eval=TRUE, echo=FALSE, results='asis'}
matL <- rmdhelp::matLowerTri(psBaseElement = "(L)", pnNrRow = nAnzAni, pnNrCol = nAnzAni, pvecDiag = 1)
matS <- rmdhelp::matDiag(psBaseElement = "(S)", pnNrRow = nAnzAni, pnNrCol = nAnzAni)
cat(paste(rmdhelp::bmatrix(pmat = matR), collapse = "\n"))
cat("  = \n")
cat(paste(rmdhelp::bmatrix(pmat = matL), collapse = "\n"))
cat("  * \n")
cat(paste(rmdhelp::bmatrix(pmat = matS), collapse = "\n"))
cat("\n")
```
(\#eq:SmallExRLS)
\end{equation}

From \@ref(eq:SmallExRLS), we can see that the diagonal elements $(R)_{ii}$ are equal to $(S)_{ii}$. Therefore

\begin{equation}
(R)_{ii} = (S)_{ii} = \sqrt{(D)_{ii}}
\label{eq:ComputeRii}
\end{equation}

Earlier, we have seen that diagonal elements $(D)_{ii}$ of $D$ correspond to 

\begin{equation}
(D)_{ii} = {1\over 2}\ - {1\over 4}\ (F_s + F_d) = 1 - 0.25((A)_{ss} + (A)_{dd})
\label{eq:ComputeDii}
\end{equation}

and hence

\begin{equation}
(R)_{ii} = \sqrt{1 - 0.25((A)_{ss} + (A)_{dd})}
\label{eq:ComputeRiiResult}
\end{equation}

The components $(A)_{ss}$ and $(A)_{dd}$ correspond to diagonal elements of $A$ of parents of $s$ and $d$ which were computed earlier.

The off-diagonal elements of $R$ are computed as

\begin{equation}
(R)_{ij} = (L)_{ij} * (S)_{jj}
\label{eq:ComputeRij}
\end{equation}

One property of the matrix $L$ is that any element $(L)_{ij}$ is equal to the average of elements $(L)_{sj}$ and $(L)_{dj}$, if $s$ and $d$ are parents of individual $i$. Using this we get

\begin{align}
(R)_{ij} &= (L)_{ij} * (S)_{jj} \nonumber\\
       &= {1\over 2}((L)_{sj} + (L)_{dj}) * (S)_{jj} \nonumber\\
       &= {1\over 2}((R)_{sj} + (R)_{dj})
(\#eq:ComputeRijRec)
\end{align}

With that we have shown how to compute all elements of $R$ recursively. This requires an ordering of all individuals according to their age.


<!--
## Appendix: Derivation of Diagonalelements of Numerator Relationship Matrix $A$
The material shown in this section is based on chapters 14 and 15 of [@Falconer1996]. (TBC)
-->


\end{comment}

<!--chapter:end:Pedigree-Based-Relationship-Matrix.Rmd-->

